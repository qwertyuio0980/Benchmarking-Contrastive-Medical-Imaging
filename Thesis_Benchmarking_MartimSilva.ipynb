{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Contrastive Learning for Multimodal Medical Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import glob\n",
    "import copy\n",
    "import re\n",
    "import plistlib\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pydicom\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw\n",
    "from bs4 import BeautifulSoup\n",
    "from skimage.draw import polygon\n",
    "from sklearn.metrics import (\n",
    "    f1_score, matthews_corrcoef, accuracy_score, balanced_accuracy_score,\n",
    "    jaccard_score, precision_recall_curve, average_precision_score,\n",
    "    roc_auc_score, roc_curve, precision_score, recall_score, confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from lightly.loss import NTXentLoss\n",
    "from lightly.models.modules import SimCLRProjectionHead\n",
    "from lightly.transforms.simclr_transform import SimCLRTransform\n",
    "import lightly.data as data\n",
    "\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducible results\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Use GPU\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Number of workers:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    1.2. Directory setting for data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directories setup\n",
    "current_dir = os.getcwd()\n",
    "root_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "notebooks_dir = os.path.join(root_dir, \"notebooks\")\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "data_used_dir = os.path.join(data_dir, \"dataUsed\")\n",
    "results_dir = os.path.join(root_dir, \"results\")\n",
    "\n",
    "# Creating directories\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Subfolder structures\n",
    "contrastive_methods = [\"simclr\", \"moco\", \"byol\", \"supervised\"]\n",
    "modalities = [\"ultrasound\", \"mammography\", \"multimodal\"]\n",
    "task_types = [\"classification\", \"segmentation\"]\n",
    "finetune_splits = [\"train\", \"validation\", \"test\"]\n",
    "classes = [\"benign\", \"malignant\", \"normal\"]\n",
    "subfolders = [\"images\", \"masks\"]  # For segmentation task\n",
    "dataset_folders = [\"UltrasoundDataset\", \"MammographyDataset\", \"MultimodalDataset\"]\n",
    "\n",
    "# Creating directories for cleaned data\n",
    "def create_data_folders(base_data_dir):\n",
    "    \n",
    "    for dataset in dataset_folders:\n",
    "        dataset_dir = os.path.join(base_data_dir, dataset)\n",
    "        \n",
    "        # Pretrain and Finetune directories inside each dataset\n",
    "        pretrain_dir = os.path.join(dataset_dir, \"pretrainData\")\n",
    "        finetune_dir = os.path.join(dataset_dir, \"finetuneData\")\n",
    "        \n",
    "        # Create the pretrain directory with train/validation splits\n",
    "        if dataset == \"MultimodalDataset\":\n",
    "            for split in [\"train\", \"validation\"]:\n",
    "                split_dir = os.path.join(pretrain_dir, split)\n",
    "                create_directory(split_dir)\n",
    "                for modality in [\"ultrasoundImages\", \"mammographyImages\"]:\n",
    "                    create_directory(os.path.join(split_dir, modality))\n",
    "\n",
    "        for split in [\"train\", \"validation\"]:\n",
    "            split_dir = os.path.join(pretrain_dir, split)\n",
    "            create_directory(split_dir)\n",
    "\n",
    "        # Create the finetune directory with train/validation/test splits and class structure\n",
    "        for split in finetune_splits:\n",
    "            split_dir = os.path.join(finetune_dir, split)\n",
    "            for cls in classes:\n",
    "                class_dir = os.path.join(split_dir, cls)\n",
    "                create_directory(class_dir)\n",
    "                for subfolder in subfolders:\n",
    "                    subfolder_path = os.path.join(class_dir, subfolder)\n",
    "                    create_directory(subfolder_path)\n",
    "                    if dataset == \"MultimodalDataset\" and subfolder == \"images\":\n",
    "                        for modality in [\"ultrasoundImages\", \"mammographyImages\"]:\n",
    "                            create_directory(os.path.join(subfolder_path, modality))\n",
    "                    elif dataset == \"MultimodalDataset\" and subfolder == \"masks\":\n",
    "                        for mask_type in [\"UltrasoundMasks\", \"MammographyMasks\"]:\n",
    "                            create_directory(os.path.join(subfolder_path, mask_type))\n",
    "\n",
    "# Results directory\n",
    "def create_results_structure(base_results_dir):\n",
    "    # Create the main pretraining directory\n",
    "    pretrain_dir = os.path.join(base_results_dir, \"pretrainPhase\")\n",
    "    create_directory(pretrain_dir)\n",
    "    \n",
    "    for method in contrastive_methods:\n",
    "        method_dir = os.path.join(pretrain_dir, method)\n",
    "        create_directory(method_dir)\n",
    "        \n",
    "        for modality in modalities:\n",
    "            modality_dir = os.path.join(method_dir, modality)\n",
    "            create_directory(modality_dir)\n",
    "            \n",
    "    for task_type in task_types:\n",
    "        task_dir = os.path.join(base_results_dir, task_type)\n",
    "        create_directory(task_dir)\n",
    "\n",
    "        for method in contrastive_methods:\n",
    "            method_dir = os.path.join(task_dir, method)\n",
    "            create_directory(method_dir)\n",
    "\n",
    "            for modality in modalities:\n",
    "                modality_dir = os.path.join(method_dir, modality)\n",
    "                create_directory(modality_dir)\n",
    "\n",
    "create_data_folders(data_used_dir) \n",
    "create_results_structure(results_dir)\n",
    "\n",
    "# Verification\n",
    "print(f\"Folder structure created under {data_used_dir} for data and {results_dir} for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating datasets (run once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultrasound\n",
    "\n",
    "- Pretrain: QAMEBI’s BUSI dataset + Breast Ultrasound Image (Mendeley) dataset + BrEaST dataset + Thammasat dataset\n",
    "\n",
    "- Finetuning: BUSI dataset\n",
    "\n",
    "Mammography\n",
    "  \n",
    "- Pretrain: CBIS-DDSM dataset\n",
    "\n",
    "- Finetuning: INbreast dataset\n",
    "\n",
    "Both types\n",
    "- Pretrain: 50% of data points considered in the ultrasound and mammography datasets, taken randomly per class and per training phase (pretrain or finetune)\n",
    "\n",
    "- Finetuning: portion of BUSI and CBIS-DDSM (50%) + portion of INbreast (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Handling used datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. INBreast \n",
    "\n",
    "Mammography image dataset\n",
    "\n",
    "https://doi.org/10.1016/j.acra.2011.09.014\n",
    "\n",
    "-----\n",
    "\n",
    "Label adaptations based on:\n",
    "\n",
    "https://doi.org/10.48550/arXiv.1705.08550\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally follows BI-RADS system, now following:\n",
    "# 1          ->  normal\n",
    "# 2/3        ->  benign\n",
    "# 4a,b,c/5/6 ->  malignant\n",
    "\n",
    "#INbreast—has a total of 115 cases from which 90 cases are from women with both breasts affected (four images per case) and 25 cases are from mastectomy patients (two images per case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists: ../data/INbreast Release 1.0\\cleanmergedInbreast\n",
      "Folder already exists: ../data/INbreast Release 1.0\\cleanmergedInbreast\\masks\n",
      "Folder already exists: ../data/INbreast Release 1.0\\cleanmergedInbreast\\images\n"
     ]
    }
   ],
   "source": [
    "# Define directories\n",
    "data_dir = '../data/INbreast Release 1.0'\n",
    "dicom_dir = os.path.join(data_dir, 'AllDICOMs')\n",
    "xml_dir = os.path.join(data_dir, 'AllXML')\n",
    "roi_dir = os.path.join(data_dir, 'AllROI')\n",
    "reports_dir = os.path.join(data_dir, 'MedicalReports')\n",
    "clean_inbreast_dir = os.path.join(data_dir, 'cleanmergedInbreast')\n",
    "clean_inbreast_masks_dir = os.path.join(clean_inbreast_dir, 'masks')\n",
    "clean_inbreast_images_dir = os.path.join(clean_inbreast_dir, 'images')\n",
    "csv_file_path = '../data/INbreast Release 1.0/INBreast.csv'\n",
    "\n",
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f'Created folder: {directory}')\n",
    "    else:\n",
    "        print(f'Folder already exists: {directory}')\n",
    "\n",
    "create_directory(clean_inbreast_dir)\n",
    "create_directory(clean_inbreast_masks_dir)\n",
    "create_directory(clean_inbreast_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract BIRADS number from medical report\n",
    "def extract_birads_from_report(report_path):\n",
    "    try:\n",
    "        with open(report_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            match = re.search(r'bi-?rads\\s*-\\s*(\\d+)', content, re.IGNORECASE)\n",
    "            if match:\n",
    "                birads_number = int(match.group(1))\n",
    "                return birads_number\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading report {report_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Re-classify based on BIRADS number\n",
    "def classify_birads(birads_number):\n",
    "    # Handle special cases like \"4a\", \"4b\", \"4c\" by converting them to \"4\"\n",
    "    if isinstance(birads_number, str) and '4' in birads_number:\n",
    "        birads_number = 4\n",
    "    else:\n",
    "        try:\n",
    "            birads_number = int(birads_number)  # Ensure BIRADS as int\n",
    "        except ValueError:\n",
    "            return None\n",
    "    if birads_number == 1:\n",
    "        return \"normal\"\n",
    "    elif birads_number in [2, 3]:\n",
    "        return \"benign\"\n",
    "    elif birads_number in [4, 5, 6]:\n",
    "        return \"malignant\"\n",
    "    return None\n",
    "\n",
    "def dicom_to_png(dicom_path, output_image_path):\n",
    "    try:\n",
    "        dicom_data = pydicom.dcmread(dicom_path)\n",
    "        img_array = dicom_data.pixel_array\n",
    "        \n",
    "        # Normalize\n",
    "        img_array = (img_array - np.min(img_array)) / (np.max(img_array) - np.min(img_array)) * 255\n",
    "        img_array = img_array.astype(np.uint8)\n",
    "\n",
    "        # Save image\n",
    "        img = Image.fromarray(img_array)\n",
    "        img.save(output_image_path)\n",
    "        return img_array.shape  # Return image size\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting DICOM {dicom_path} to PNG: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Convert DICOM to PNG\n",
    "dicom_sizes = {}\n",
    "for dicom_file in os.listdir(dicom_dir):\n",
    "    if dicom_file.endswith('.dcm'):\n",
    "        dicom_path = os.path.join(dicom_dir, dicom_file)\n",
    "        png_filename = f\"{os.path.splitext(dicom_file)[0]}.png\"\n",
    "        output_image_path = os.path.join(clean_inbreast_images_dir, png_filename)\n",
    "        \n",
    "        img_size = dicom_to_png(dicom_path, output_image_path)\n",
    "        if img_size is not None:\n",
    "            dicom_sizes[png_filename] = img_size\n",
    "\n",
    "print(\"Conversion of DICOMs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load INbreast CSV data\n",
    "inbreast_df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# List image files in the images folder\n",
    "image_files = [f for f in os.listdir(clean_inbreast_images_dir) if f.endswith('.png')]\n",
    "\n",
    "# Counters for different matching scenarios\n",
    "csv_match_count = 0\n",
    "unmatched_images = []\n",
    "\n",
    "# Store BIRADS classification for each image\n",
    "birads_class_dict = {}\n",
    "\n",
    "# Step 1: Check Images Against CSV File and Assign Class Based on BIRADS\n",
    "for image_file in image_files:\n",
    "    # Extract the image ID for CSV matching (assumed as the first part of the filename)\n",
    "    image_id = image_file.split('_')[0]\n",
    "\n",
    "    # Check for a match in the CSV file using \"File Name\" column\n",
    "    csv_match = inbreast_df[inbreast_df['File Name'] == int(image_id)]\n",
    "\n",
    "    if not csv_match.empty:\n",
    "        csv_birads = csv_match.iloc[0]['Bi-Rads']\n",
    "        print(f\"Image {image_file} matched in CSV with BIRADS: {csv_birads}\")\n",
    "\n",
    "        # Classify the BIRADS into a class (normal, benign, malignant)\n",
    "        image_class = classify_birads(csv_birads)\n",
    "        if image_class:\n",
    "            # Store the classification\n",
    "            birads_class_dict[image_file] = image_class\n",
    "            csv_match_count += 1\n",
    "        else:\n",
    "            print(f\"Invalid BIRADS value for {image_file}: {csv_birads}\")\n",
    "    else:\n",
    "        # If no match, add to unmatched images list\n",
    "        unmatched_images.append(image_file)\n",
    "\n",
    "# Step 2: Final Count and Unmatched Images\n",
    "total_images = len(image_files)\n",
    "successful_matches = csv_match_count\n",
    "print(f\"\\nFinal count out of {total_images} images:\")\n",
    "print(f\" - Matched with CSV: {csv_match_count}\")\n",
    "print(f\" - Unmatched images: {len(unmatched_images)}\")\n",
    "\n",
    "if unmatched_images:\n",
    "    print(\"\\nUnmatched image files:\")\n",
    "    for unmatched_file in unmatched_images:\n",
    "        print(unmatched_file)\n",
    "\n",
    "# Step 3: Rename Files Based on Class\n",
    "for image_file, image_class in birads_class_dict.items():\n",
    "    # Full path of the original image\n",
    "    old_image_path = os.path.join(clean_inbreast_images_dir, image_file)\n",
    "\n",
    "    if os.path.exists(old_image_path):\n",
    "        # Construct the new filename by appending the class before the extension\n",
    "        name_part, ext = os.path.splitext(image_file)  # Split filename and extension\n",
    "        new_image_name = f\"{name_part}_{image_class}.png\"\n",
    "        new_image_path = os.path.join(clean_inbreast_images_dir, new_image_name)\n",
    "\n",
    "        # Rename the file\n",
    "        os.rename(old_image_path, new_image_path)\n",
    "        print(f\"Renamed {image_file} to {new_image_name}\")\n",
    "    else:\n",
    "        print(f\"File not found: {old_image_path}\")\n",
    "\n",
    "print(\"\\nFile renaming process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function adapted from: https://github.com/pablogiaccaglia/Breast-Cancer-Segmentation-Datasets/blob/master/INbreast/refactorINbreast.py\n",
    "# Original Author: Pablo Giaccaglia\n",
    "def loadInbreastMask(mask_path, imshape=(4084, 3328), filter=False):\n",
    "    \n",
    "    def load_point(point_string):\n",
    "        # Converts point string \"(x, y)\" into a tuple (y, x)\n",
    "        x, y = tuple([float(num) for num in point_string.strip('()').split(',')])\n",
    "        return y, x  # Remember (row, col) in numpy\n",
    "\n",
    "    mask = np.zeros(imshape)\n",
    "    \n",
    "    # Open the plist XML file\n",
    "    with open(mask_path, 'rb') as mask_file:\n",
    "        plist_dict = plistlib.load(mask_file, fmt=plistlib.FMT_XML)['Images'][0]\n",
    "        \n",
    "        num_rois = plist_dict['NumberOfROIs']\n",
    "        rois = plist_dict['ROIs']\n",
    "        \n",
    "        assert len(rois) == num_rois\n",
    "        \n",
    "        for roi in rois:\n",
    "            num_points = roi['NumberOfPoints']\n",
    "            points = roi['Point_px']\n",
    "            \n",
    "            assert num_points == len(points)\n",
    "            \n",
    "            points = [load_point(point) for point in points]\n",
    "\n",
    "            # Apply filter to skip ROIs with fewer points\n",
    "            if filter and len(points) < 18:\n",
    "                continue\n",
    "            \n",
    "            if len(points) <= 2:  # Handling single points or lines\n",
    "                for point in points:\n",
    "                    # Check bounds before assigning\n",
    "                    if 0 <= int(point[0]) < imshape[0] and 0 <= int(point[1]) < imshape[1]:\n",
    "                        mask[int(point[0]), int(point[1])] = 1\n",
    "            else:\n",
    "                # Draw polygons for regions with more points\n",
    "                y, x = zip(*points)  # Points are in (row, col) order\n",
    "                y, x = np.array(y), np.array(x)\n",
    "                \n",
    "                # Ensure points are valid within the image bounds\n",
    "                poly_y, poly_x = polygon(y, x, shape=imshape)\n",
    "                \n",
    "                # Assign mask regions\n",
    "                mask[poly_y, poly_x] = 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Function adapted from: https://github.com/pablogiaccaglia/Breast-Cancer-Segmentation-Datasets/blob/master/INbreast/refactorINbreast.py\n",
    "# Original Author: Pablo Giaccaglia\n",
    "def generate_masks(xml_dir, images_dir, output_mask_dir):\n",
    "    \n",
    "    os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "    for xml_filename in os.listdir(xml_dir):\n",
    "        if xml_filename.endswith('.xml'):\n",
    "            \n",
    "            xml_file_path = os.path.join(xml_dir, xml_filename)\n",
    "            \n",
    "            # Remove file type extension .xml\n",
    "            base_id = xml_filename[:-4]  # Base ID from XML filename\n",
    "            \n",
    "            matched_image_found = False\n",
    "            \n",
    "            for image_filename in os.listdir(images_dir):\n",
    "                if image_filename.startswith(base_id) and image_filename.endswith('.png'):\n",
    "                    \n",
    "                    image_path = os.path.join(images_dir, image_filename)\n",
    "                    matched_image_found = True\n",
    "                    \n",
    "                    image = cv2.imread(image_path)\n",
    "                    if image is not None:\n",
    "                        image_shape = image.shape[:2]  # Get height and width\n",
    "                        \n",
    "                        # Generate the mask\n",
    "                        mask = loadInbreastMask(xml_file_path, imshape=image_shape)\n",
    "                        \n",
    "                        # Save the mask as a binary image\n",
    "                        mask_output_path = os.path.join(output_mask_dir, f\"{base_id}_mask.png\")\n",
    "                        plt.imsave(mask_output_path, mask, cmap='gray', format='png')\n",
    "                    else:\n",
    "                        print(f\"Could not load image: {image_path}\")\n",
    "                    break\n",
    "            \n",
    "            if not matched_image_found:\n",
    "                print(f\"No matching image found for XML file: {xml_filename}\")\n",
    "\n",
    "generate_masks(xml_dir, clean_inbreast_images_dir, clean_inbreast_masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_masks_by_class(images_dir, masks_dir):\n",
    "    grouped_masks = defaultdict(list)\n",
    "\n",
    "    for image_filename in os.listdir(images_dir):\n",
    "        if image_filename.endswith('.png'):\n",
    "            print(f\"Processing image file: {image_filename}\")\n",
    "            \n",
    "            # Determine class based on suffix\n",
    "            if '_benign.png' in image_filename:\n",
    "                image_class = 'benign'\n",
    "            elif '_malignant.png' in image_filename:\n",
    "                image_class = 'malignant'\n",
    "            elif '_normal.png' in image_filename:\n",
    "                image_class = 'normal'\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Extract base ID by splitting at the first underscore\n",
    "            base_id = image_filename.split('_')[0]\n",
    "\n",
    "            # Look for corresponding mask\n",
    "            mask_filename = f\"{base_id}_mask.png\"\n",
    "            mask_path = os.path.join(masks_dir, mask_filename)\n",
    "\n",
    "            if os.path.exists(mask_path):\n",
    "                grouped_masks[image_class].append(mask_filename)\n",
    "            else:\n",
    "                print(f\"Mask file not found for {mask_filename}\")\n",
    "\n",
    "    return grouped_masks\n",
    "\n",
    "grouped_masks = group_masks_by_class(clean_inbreast_images_dir, clean_inbreast_masks_dir)\n",
    "benign_count = len(grouped_masks.get('benign', []))\n",
    "malignant_count = len(grouped_masks.get('malignant', []))\n",
    "\n",
    "print(f\"Number of benign masks: {benign_count}\")\n",
    "print(f\"Number of malignant masks: {malignant_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_images_count = 0\n",
    "malignant_images_count = 0\n",
    "normal_images_count = 0\n",
    "unmatched_images_count = 0\n",
    "\n",
    "for image_filename in os.listdir(clean_inbreast_images_dir):\n",
    "    if image_filename.endswith('.png'):\n",
    "        if '_benign.png' in image_filename:\n",
    "            benign_images_count += 1\n",
    "        elif '_malignant.png' in image_filename:\n",
    "            malignant_images_count += 1\n",
    "        elif '_normal.png' in image_filename:\n",
    "            normal_images_count += 1\n",
    "        else:\n",
    "            unmatched_images_count += 1\n",
    "\n",
    "print(f\"Number of benign images: {benign_images_count}\")\n",
    "print(f\"Number of malignant images: {malignant_images_count}\")\n",
    "print(f\"Number of normal images: {normal_images_count}\")\n",
    "print(f\"Number of unmatched images: {unmatched_images_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_masks_with_class(image_folder, mask_folder):\n",
    "    \n",
    "    image_files = os.listdir(image_folder)\n",
    "    \n",
    "    # Map ID to class name\n",
    "    id_to_class = {}\n",
    "    \n",
    "    for filename in image_files:\n",
    "        \n",
    "        match = re.match(r\"(\\d+)_.*_(benign|malignant)\", filename)\n",
    "        if match:\n",
    "            id_number = match.group(1)\n",
    "            class_name = match.group(2)\n",
    "            id_to_class[id_number] = class_name\n",
    "\n",
    "    for mask_filename in os.listdir(mask_folder):\n",
    "        \n",
    "        mask_match = re.match(r\"(\\d+)_mask\\.png\", mask_filename)\n",
    "        if mask_match:\n",
    "            mask_id = mask_match.group(1)\n",
    "            # Check corresponding class for ID\n",
    "            if mask_id in id_to_class:\n",
    "                new_mask_filename = f\"{mask_id}_{id_to_class[mask_id]}_mask.png\"\n",
    "                old_mask_path = os.path.join(mask_folder, mask_filename)\n",
    "                new_mask_path = os.path.join(mask_folder, new_mask_filename)\n",
    "                os.rename(old_mask_path, new_mask_path)\n",
    "\n",
    "rename_masks_with_class(clean_inbreast_images_dir, clean_inbreast_masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic masks (empty arrays) serving as \"normal\" case masks in segmentation\n",
    "\n",
    "# Mask dimensions\n",
    "height, width = 3328, 2560\n",
    "\n",
    "# Create the all-black mask (zeroes array)\n",
    "black_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "c = 1\n",
    "\n",
    "for filename in os.listdir(clean_inbreast_images_dir):\n",
    "    if filename.endswith(\"_normal.png\"):\n",
    "        # Unique ID (part before '_normal')\n",
    "        base_id = filename.split('_')[0]\n",
    "        \n",
    "        mask_filename = f\"{base_id}_normal_syntheticMask.png\"\n",
    "        \n",
    "        # Save new mask\n",
    "        mask_path = os.path.join(clean_inbreast_masks_dir, mask_filename)\n",
    "        Image.fromarray(black_mask).save(mask_path)\n",
    "\n",
    "        print(f\"{c} - Created mask: {mask_filename}\")\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id(filename):\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "image_files = [f for f in os.listdir(clean_inbreast_images_dir) if os.path.isfile(os.path.join(clean_inbreast_images_dir, f))]\n",
    "mask_files = [f for f in os.listdir(clean_inbreast_masks_dir) if os.path.isfile(os.path.join(clean_inbreast_masks_dir, f))]\n",
    "\n",
    "image_dict = {extract_id(f): f for f in image_files}\n",
    "mask_dict = {extract_id(f): f for f in mask_files}\n",
    "\n",
    "missing_masks = []\n",
    "duplicate_masks = []\n",
    "\n",
    "# Check correspondence\n",
    "for image_id, image_file in image_dict.items():\n",
    "    if image_id not in mask_dict:\n",
    "        missing_masks.append(image_file)  # No corresponding mask\n",
    "    elif list(mask_dict.values()).count(mask_dict[image_id]) > 1:\n",
    "        duplicate_masks.append(mask_dict[image_id])  # Mask used for more than one image\n",
    "\n",
    "print(f\"Total images: {len(image_files)}\")\n",
    "print(f\"Total masks: {len(mask_files)}\")\n",
    "print(f\"Images without corresponding masks: {len(missing_masks)}\")\n",
    "print(f\"Masks associated with more than one image: {len(duplicate_masks)}\")\n",
    "\n",
    "# Missing or duplicate details\n",
    "if missing_masks:\n",
    "    print(\"Images missing masks:\")\n",
    "    for img in missing_masks:\n",
    "        print(img)\n",
    "\n",
    "if duplicate_masks:\n",
    "    print(\"Duplicate masks:\")\n",
    "    for mask in duplicate_masks:\n",
    "        print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. CBIS-DDSM\n",
    "\n",
    "Mammography image dataset\n",
    "\n",
    "https://doi.org/10.1038/sdata.2017.177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "jpeg_dir = '../data/CBISDDSMdataset/jpeg'\n",
    "clean_cbisddsm_dir = '../data/CBISDDSMdataset/cleanmergedCbisddsm'\n",
    "clean_cbisddsm_images_dir = os.path.join(clean_cbisddsm_dir, 'images')\n",
    "clean_cbisddsm_masks_dir = os.path.join(clean_cbisddsm_dir, 'masks')\n",
    "\n",
    "os.makedirs(clean_cbisddsm_images_dir, exist_ok=True)\n",
    "os.makedirs(clean_cbisddsm_masks_dir, exist_ok=True)\n",
    "\n",
    "# Load DICOM and filter for \"full mammogram images\" and \"cropped images\"\n",
    "dicom_data = pd.read_csv('../data/CBISDDSMdataset/csv/dicom_info.csv')\n",
    "dicom_data = dicom_data[[\"image_path\", \"PatientID\", \"SeriesDescription\"]]\n",
    "\n",
    "dicom_data['clean_image_path'] = dicom_data['image_path'].str.replace(\"CBIS-DDSM/jpeg/\", \"\")\n",
    "\n",
    "# Load pathology data and clean paths\n",
    "columns_to_keep = [\"pathology\", \"image file path\", \"cropped image file path\", \"ROI mask file path\"]\n",
    "calc_test_data = pd.read_csv('../data/CBISDDSMdataset/csv/calc_case_description_test_set.csv')[columns_to_keep]\n",
    "calc_train_data = pd.read_csv('../data/CBISDDSMdataset/csv/calc_case_description_train_set.csv')[columns_to_keep]\n",
    "mass_test_data = pd.read_csv('../data/CBISDDSMdataset/csv/mass_case_description_test_set.csv')[columns_to_keep]\n",
    "mass_train_data = pd.read_csv('../data/CBISDDSMdataset/csv/mass_case_description_train_set.csv')[columns_to_keep]\n",
    "pathology_data = pd.concat([calc_train_data, calc_test_data, mass_train_data, mass_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude cropped images immediately\n",
    "dicom_filtered = dicom_data[dicom_data['SeriesDescription'] != 'cropped images']\n",
    "\n",
    "# Separate full mammogram and ROI images\n",
    "full_mammo_data = dicom_filtered[dicom_filtered['SeriesDescription'] == 'full mammogram images']\n",
    "roi_data = dicom_filtered[dicom_filtered['SeriesDescription'] == 'ROI mask images']\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for _, mammo_row in full_mammo_data.iterrows():\n",
    "    \n",
    "    mammo_patient_id = mammo_row['PatientID']\n",
    "    full_mammo_image_path = mammo_row['clean_image_path']\n",
    "\n",
    "    # Check for corresponding pathology data\n",
    "    matching_pathology = pathology_data[pathology_data['image file path'].str.startswith(mammo_patient_id)]\n",
    "    if matching_pathology.empty:\n",
    "        continue  # Skip if no pathology data is found\n",
    "\n",
    "    # Determine the pathology class\n",
    "    pathology_class = matching_pathology['pathology'].values[0].lower()\n",
    "    if pathology_class == 'benign_without_callback':\n",
    "        pathology_class = 'normal'\n",
    "\n",
    "    # Malignant cases: Process both full mammogram and ROI as in previous logic\n",
    "    if pathology_class == 'malignant':\n",
    "        # Check for matching ROI in roi_data\n",
    "        matching_roi = roi_data[roi_data['PatientID'].str.startswith(mammo_patient_id)]\n",
    "        \n",
    "        # Skip if more or less than one matching ROI\n",
    "        if len(matching_roi) != 1:\n",
    "            continue\n",
    "\n",
    "        # ROI row\n",
    "        roi_row = matching_roi.iloc[0]\n",
    "        roi_image_path = roi_row['clean_image_path']\n",
    "        \n",
    "        # Destination filenames\n",
    "        mammo_dest_filename = f\"{os.path.splitext(os.path.basename(full_mammo_image_path))[0]}_id{counter}_malignant.png\"\n",
    "        roi_dest_filename = f\"{os.path.splitext(os.path.basename(roi_image_path))[0]}_id{counter}_malignant.png\"\n",
    "        mammo_dest_path = os.path.join(clean_cbisddsm_images_dir, mammo_dest_filename)\n",
    "        roi_dest_path = os.path.join(clean_cbisddsm_masks_dir, roi_dest_filename)\n",
    "\n",
    "        # Copy full mammogram and ROI mask images\n",
    "        shutil.copy(os.path.join(jpeg_dir, full_mammo_image_path), mammo_dest_path)\n",
    "        shutil.copy(os.path.join(jpeg_dir, roi_image_path), roi_dest_path)\n",
    "        print(f\"Copied malignant full mammogram: {mammo_dest_path}\")\n",
    "        print(f\"Copied malignant ROI mask: {roi_dest_path}\")\n",
    "\n",
    "    # Benign and Normal cases: Only copy full mammogram images to \"images\" subfolder\n",
    "    else:\n",
    "        mammo_dest_filename = f\"{os.path.splitext(os.path.basename(full_mammo_image_path))[0]}_id{counter}_{pathology_class}.png\"\n",
    "        mammo_dest_path = os.path.join(clean_cbisddsm_images_dir, mammo_dest_filename)\n",
    "\n",
    "        # Copy full mammogram image\n",
    "        shutil.copy(os.path.join(jpeg_dir, full_mammo_image_path), mammo_dest_path)\n",
    "        print(f\"Copied {pathology_class} full mammogram: {mammo_dest_path}\")\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_masks(mask_folder):\n",
    "    for filename in os.listdir(mask_folder):\n",
    "        if not filename.endswith('_mask.png'):  # Avoid re-renaming\n",
    "            old_path = os.path.join(mask_folder, filename)\n",
    "            new_path = os.path.join(mask_folder, filename.split('.')[0] + '_mask.png')\n",
    "            os.rename(old_path, new_path)\n",
    "\n",
    "rename_masks(clean_cbisddsm_masks_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3. BUSI\n",
    "\n",
    "Ultrasound image dataset\n",
    "\n",
    "https://doi.org/10.3390/healthcare10040729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busi_dir = \"../data/Dataset_BUSI_with_GTdataset\"\n",
    "busi_benign_dir = os.path.join(busi_dir, \"benign\")\n",
    "busi_malignant_dir = os.path.join(busi_dir, \"malignant\")\n",
    "busi_normal_dir = os.path.join(busi_dir, \"normal\")\n",
    "clean_busi_dir = os.path.join(busi_dir, \"cleanmergedBusi\")\n",
    "clean_busi_masks_dir = os.path.join(clean_busi_dir, 'masks')\n",
    "clean_busi_images_dir = os.path.join(clean_busi_dir, 'images')\n",
    "\n",
    "os.makedirs(clean_busi_masks_dir, exist_ok=True)\n",
    "os.makedirs(clean_busi_images_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id(filename):\n",
    "    return filename.replace('_mask', '').replace('.png', '')\n",
    "\n",
    "# Get all image and mask files from multiple directories\n",
    "def get_all_files(*directories):\n",
    "    image_files = []\n",
    "    mask_files = []\n",
    "    \n",
    "    for directory in directories:\n",
    "        image_files.extend([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and '_mask' not in f])\n",
    "        mask_files.extend([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and '_mask' in f])\n",
    "    \n",
    "    return image_files, mask_files\n",
    "\n",
    "# List of all images and masks from the three directories\n",
    "image_files, mask_files = get_all_files(busi_benign_dir, busi_malignant_dir, busi_normal_dir)\n",
    "\n",
    "# Map IDs to filenames\n",
    "image_dict = {extract_id(f): f for f in image_files}\n",
    "mask_dict = {extract_id(f): f for f in mask_files}\n",
    "\n",
    "# Initialize lists to track issues\n",
    "missing_masks = []\n",
    "duplicate_masks = []\n",
    "extra_masks = []\n",
    "\n",
    "# Check correspondence between images and masks\n",
    "for image_id, image_file in image_dict.items():\n",
    "    if image_id not in mask_dict:\n",
    "        missing_masks.append(image_file)  # No corresponding mask\n",
    "    elif list(mask_dict.values()).count(mask_dict[image_id]) > 1:\n",
    "        duplicate_masks.append(mask_dict[image_id])  # Mask used for more than one image\n",
    "\n",
    "print(f\"Total images: {len(image_files)}\")\n",
    "print(f\"Total masks: {len(mask_files)}\")\n",
    "\n",
    "# Masks that don't have a corresponding image check\n",
    "for mask_id, mask_file in mask_dict.items():\n",
    "    if mask_id not in image_dict:\n",
    "        extra_masks.append(mask_id)  # Store mask ID (without \"_mask\") to skip both mask and associated image\n",
    "\n",
    "print(f\"Extra masks (without corresponding images): {len(extra_masks)}\")\n",
    "\n",
    "# Filenames of the extra masks\n",
    "if extra_masks:\n",
    "    print(\"\\nFilenames of extra masks:\")\n",
    "    for mask in extra_masks:\n",
    "        print(f\"{mask}_mask.png\")\n",
    "\n",
    "# Skip both extra masks and their corresponding images\n",
    "def copy_files(class_dir, extra_masks):\n",
    "    files = os.listdir(class_dir)\n",
    "    \n",
    "    for file in files:\n",
    "        file_id = extract_id(file)  # Extract ID to check if it's in the extra masks list\n",
    "        \n",
    "        if file_id in extra_masks:\n",
    "            print(f\"Skipping {file} as it is associated with an extra mask.\")\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(class_dir, file)\n",
    "        \n",
    "        # Check if it's a mask or an image based on the file name\n",
    "        if '_mask' in file:\n",
    "            # Copy mask files to the masks directory\n",
    "            destination_path = os.path.join(clean_busi_masks_dir, file)\n",
    "            shutil.copy(file_path, destination_path)\n",
    "            print(f\"Copied mask: {file} -> {destination_path}\")\n",
    "        else:\n",
    "            # Copy mammogram images to the images directory\n",
    "            destination_path = os.path.join(clean_busi_images_dir, file)\n",
    "            shutil.copy(file_path, destination_path)\n",
    "            print(f\"Copied image: {file} -> {destination_path}\")\n",
    "\n",
    "copy_files(busi_benign_dir, extra_masks)\n",
    "copy_files(busi_malignant_dir, extra_masks)\n",
    "copy_files(busi_normal_dir, extra_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = os.listdir(clean_busi_images_dir) \n",
    "number_files = len(lst)\n",
    "print(number_files)\n",
    "\n",
    "lst2 = os.listdir(clean_busi_masks_dir) \n",
    "number_files2 = len(lst2)\n",
    "print(number_files2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4. Mendeley\n",
    "\n",
    "Ultrasound image dataset\n",
    "\n",
    "https://doi.org/10.17632/wmy84gzngw.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mendeley_dir = \"../data/BreastUltrasoundImageMendeleydataset/originals\"\n",
    "mendeley_benign_dir = os.path.join(mendeley_dir, \"benign\")\n",
    "mendeley_malignant_dir = os.path.join(mendeley_dir, \"malignant\")\n",
    "\n",
    "# No normal class here\n",
    "\n",
    "clean_mendeley_dir = os.path.join(mendeley_dir, \"cleanmergedMendeley\")\n",
    "\n",
    "os.makedirs(clean_mendeley_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_and_rename_images(src_dir, label):\n",
    "    files = os.listdir(src_dir)\n",
    "    \n",
    "    for file in files:\n",
    "        if file.lower().endswith('.bmp'):\n",
    "            file_path = os.path.join(src_dir, file)\n",
    "            \n",
    "            # Load and convert to PNG format\n",
    "            with Image.open(file_path) as img:\n",
    "                # New filename with _benign or _malignant appended\n",
    "                new_filename = file.replace('.bmp', f'_{label}.png')\n",
    "                new_file_path = os.path.join(clean_mendeley_dir, new_filename)\n",
    "                \n",
    "                img.save(new_file_path, 'PNG')\n",
    "                \n",
    "                print(f\"Copied and renamed: {file} -> {new_filename}\")\n",
    "\n",
    "copy_and_rename_images(mendeley_benign_dir, 'benign')\n",
    "copy_and_rename_images(mendeley_malignant_dir, 'malignant')\n",
    "\n",
    "print(\"Copying and renaming complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_merged_images = [f for f in os.listdir(clean_mendeley_dir) if f.lower().endswith('.png')]\n",
    "print(f\"Number of images in cleanmergedMendeley folder: {len(clean_merged_images)}\")\n",
    "\n",
    "benign_images = [f for f in os.listdir(mendeley_benign_dir) if f.lower().endswith('.bmp')]\n",
    "malignant_images = [f for f in os.listdir(mendeley_malignant_dir) if f.lower().endswith('.bmp')]\n",
    "total_images = len(benign_images) + len(malignant_images)\n",
    "print(f\"Total number of images in benign and malignant folders: {total_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5. QAMEBI\n",
    "\n",
    "Ultrasound image dataset\n",
    "\n",
    "https://doi.org/10.1016/j.compbiomed.2022.106438\n",
    "\n",
    "https://doi.org/10.1016/j.ejrad.2022.110591\n",
    "\n",
    "https://doi.org/10.1016/j.bbe.2022.07.004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qamebi_dir = \"../data/QAMEBIdataset\"\n",
    "qamebi_benign_dir = os.path.join(qamebi_dir, \"benign\")\n",
    "qamebi_malignant_dir = os.path.join(qamebi_dir, \"malignant\")\n",
    "\n",
    "# No normal class here\n",
    "\n",
    "clean_qamebi_dir = os.path.join(qamebi_dir, \"cleanmergedQamebi\")\n",
    "\n",
    "os.makedirs(clean_qamebi_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_and_convert_images(src_dir, dest_dir):\n",
    "    files = os.listdir(src_dir)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(src_dir, file)\n",
    "        \n",
    "        if \"Image\" in file and file.endswith(\".bmp\"):  # Only process files with \"Image\"\n",
    "            # Load and convert BMP to PNG\n",
    "            img = Image.open(file_path)\n",
    "            new_filename = file.replace(\".bmp\", \".png\")\n",
    "            destination_path = os.path.join(dest_dir, new_filename)\n",
    "            img.save(destination_path)\n",
    "            print(f\"Copied and converted image: {file} -> {destination_path}\")\n",
    "\n",
    "copy_and_convert_images(qamebi_benign_dir, clean_qamebi_dir)\n",
    "copy_and_convert_images(qamebi_malignant_dir, clean_qamebi_dir)\n",
    "\n",
    "clean_images_count = len([f for f in os.listdir(clean_qamebi_dir) if \"Image\" in f])\n",
    "print(f\"Number of images in the clean merged folder: {clean_images_count}\")\n",
    "\n",
    "# Sum of images in the benign and malignant folders (filtering only those with \"Image\" in the name)\n",
    "benign_images_count = len([f for f in os.listdir(qamebi_benign_dir) if \"Image\" in f and f.endswith(\".bmp\")])\n",
    "malignant_images_count = len([f for f in os.listdir(qamebi_malignant_dir) if \"Image\" in f and f.endswith(\".bmp\")])\n",
    "total_images_count = benign_images_count + malignant_images_count\n",
    "print(f\"Total number of 'Image' files in benign and malignant folders: {total_images_count}\")\n",
    "\n",
    "if clean_images_count == total_images_count:\n",
    "    print(\"Success: The number of images in the clean merged folder matches the sum of 'Image' files from benign and malignant folders.\")\n",
    "else:\n",
    "    print(\"Warning: There is a mismatch between the number of images in the clean merged folder and the total 'Image' files from benign and malignant folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6. Thammaset\n",
    "\n",
    "Ultrasound image dataset\n",
    "\n",
    "https://doi.org/10.1016/j.patcog.2018.01.032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HTML\n",
    "with open('thammasatHospitalSourcePageCodeSnippet.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Initialize an empty dictionary to store image data\n",
    "image_dict = {}\n",
    "\n",
    "table = soup.find('table', {'id': 'imageList'})\n",
    "rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "for row in rows:\n",
    "    try:\n",
    "        image_tag = row.find('a', {'class': 'fancybox'})\n",
    "        image_url = image_tag['href']\n",
    "        image_id = image_url.split('/')[-1].split('.')[0]\n",
    "    except (AttributeError, IndexError):\n",
    "        continue  # Skip row if the image or URL is missing\n",
    "\n",
    "    try:\n",
    "        class_tag = row.find_all('td')[3]  # The class is in the 4th column (index 3)\n",
    "        class_label = class_tag.text.strip()\n",
    "\n",
    "        # empty class field is set to 'missing'\n",
    "        if not class_label:\n",
    "            class_label = 'missing'\n",
    "    except (AttributeError, IndexError):\n",
    "        class_label = 'missing'\n",
    "\n",
    "    image_dict[image_id] = class_label\n",
    "\n",
    "for img_id, cls in image_dict.items():\n",
    "    print(f\"Image ID: {img_id}, Class: {cls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thammasat_dir = '../data/Thammasatdataset'\n",
    "thammasat_images_dir = os.path.join(thammasat_dir, \"images\")\n",
    "clean_thammasat_dir = os.path.join(thammasat_dir, \"cleanmergedThammasat\")\n",
    "\n",
    "os.makedirs(thammasat_images_dir, exist_ok=True)\n",
    "os.makedirs(clean_thammasat_dir, exist_ok=True)\n",
    "\n",
    "# Base URL\n",
    "base_url = 'http://www.onlinemedicalimages.com/media/com_record/'\n",
    "\n",
    "# Download each image\n",
    "for image_id, class_label in image_dict.items():\n",
    "\n",
    "    image_url = base_url + image_id + '.jpg'\n",
    "    \n",
    "    # Local filename with appended class label to the image ID)\n",
    "    filename = f\"{image_id}_{class_label.replace(' ', '_')}.jpg\"\n",
    "    save_path = os.path.join(thammasat_images_dir, filename)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading {image_url} ...\")\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Saved {filename}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {image_url}: {e}\")\n",
    "\n",
    "print(\"Image download process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = Counter(image_dict.values())\n",
    "for cls, count in class_distribution.items():\n",
    "    print(f\"Class: {cls}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the 165 downloaded only the classes below were considered with \"Fibroadenoma\" and \"Cyst\" classifications being merged with the overall class of benign images -> check https://geekymedics.com/benign-breast-disease/\n",
    "\n",
    "malignant_classes = ['malignant', 'Malignant Solid Mass']\n",
    "benign_classes = ['Benign Solid Mass', 'Fibroadenoma', 'Cyst']\n",
    "\n",
    "for filename in os.listdir(thammasat_images_dir):\n",
    "    if filename.endswith('.png') or filename.endswith('.jpg'):\n",
    "        # Image ID from the filename is before the first underscore\n",
    "        image_id = filename.split('_')[0]\n",
    "        print(f\"Processing file: {filename}, extracted ID: {image_id}\")\n",
    "\n",
    "        if image_id in image_dict:\n",
    "            class_label = image_dict[image_id]\n",
    "            print(f\"Found class label for {image_id}: {class_label}\")\n",
    "            \n",
    "            if class_label in malignant_classes:\n",
    "                new_filename = f\"{image_id}_malignant.png\"\n",
    "                shutil.copy2(os.path.join(thammasat_images_dir, filename), \n",
    "                             os.path.join(clean_thammasat_dir, new_filename))\n",
    "                print(f\"Copied {filename} to {new_filename}\")\n",
    "            elif class_label in benign_classes:\n",
    "                new_filename = f\"{image_id}_benign.png\"\n",
    "                shutil.copy2(os.path.join(thammasat_images_dir, filename), \n",
    "                             os.path.join(clean_thammasat_dir, new_filename))\n",
    "                print(f\"Copied {filename} to {new_filename}\")\n",
    "        else:\n",
    "            print(f\"No classification found for {image_id}\")\n",
    "\n",
    "print(\"Images have been copied and renamed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.7. BrEaST or Breast-Lesions-USG (ultra)\n",
    "\n",
    "Ultrasound image dataset\n",
    "\n",
    "https://doi.org/10.7937/9WKK-Q141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BrEaST_dir = \"../data/BrEaSTdataset/BrEaST-Lesions_USG-images_and_masks-Dec-15-2023\"\n",
    "BrEaST_csv_dir = \"../data/BrEaSTdataset/BrEaST-Lesions-USG-clinical-data-Dec-15-2023.xlsx\"\n",
    "BrEaST_imagesAndMasks_dir = os.path.join(BrEaST_dir, \"BrEaST-Lesions_USG-images_and_masks\")\n",
    "clean_BrEaST_dir = os.path.join(BrEaST_dir, \"cleanmergedBrEaST\")\n",
    "\n",
    "os.makedirs(clean_BrEaST_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "BrEaST_csv_dir = \"../data/BrEaSTdataset/BrEaST-Lesions-USG-clinical-data-Dec-15-2023.xlsx\"\n",
    "df = pd.read_excel(BrEaST_csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    original_filename = row['Image_filename']  # Image filenames\n",
    "    class_name = row['Classification']  # Class label\n",
    "\n",
    "    # Construct the full path to the original image\n",
    "    original_image_path = os.path.join(BrEaST_imagesAndMasks_dir, original_filename)\n",
    "    \n",
    "    # Check if the original image exists\n",
    "    if os.path.exists(original_image_path):\n",
    "        new_filename = f\"{os.path.splitext(original_filename)[0]}_{class_name}.png\"\n",
    "        new_image_path = os.path.join(clean_BrEaST_dir, new_filename)\n",
    "\n",
    "        # Copy the image to the new directory with the new filename\n",
    "        shutil.copy2(original_image_path, new_image_path)\n",
    "        print(f\"Copied: {original_filename} to {new_filename}\")\n",
    "    else:\n",
    "        print(f\"File not found: {original_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Creating the 3 used custom datasets from gathered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Setting paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultrasound Dataset\n",
    "ultrasound_dir = os.path.join(data_used_dir, 'UltrasoundDataset')\n",
    "ultrasound_pretrain_dir = os.path.join(ultrasound_dir, 'pretrainData')\n",
    "ultrasound_finetune_dir = os.path.join(ultrasound_dir, 'finetuneData')\n",
    "\n",
    "# Mammography Dataset\n",
    "mammography_dir = os.path.join(data_used_dir, 'MammographyDataset')\n",
    "mammography_pretrain_dir = os.path.join(mammography_dir, 'pretrainData')\n",
    "mammography_finetune_dir = os.path.join(mammography_dir, 'finetuneData')\n",
    "\n",
    "# Multimodal Dataset\n",
    "multimodal_dir = os.path.join(data_used_dir, 'MultimodalDataset')\n",
    "multimodal_pretrain_dir = os.path.join(multimodal_dir, 'pretrainData')\n",
    "multimodal_finetune_dir = os.path.join(multimodal_dir, 'finetuneData')\n",
    "\n",
    "# Actual paths to the cleanmerged folders (avoids re-running code above)\n",
    "clean_qamebi_dir = \"../data/QAMEBIdataset/cleanmergedQamebi\"\n",
    "clean_thammasat_dir = '../data/Thammasatdataset/cleanmergedThammasat'\n",
    "clean_mendeley_dir = \"../data/BreastUltrasoundImageMendeleydataset/originals/cleanmergedMendeley\"\n",
    "clean_BrEaST_dir = \"../data/BrEaSTdataset/BrEaST-Lesions_USG-images_and_masks-Dec-15-2023/cleanmergedBrEaST\"\n",
    "clean_busi_masks_dir = \"../data/Dataset_BUSI_with_GTdataset/cleanmergedBusi/masks\"\n",
    "clean_busi_images_dir = \"../data/Dataset_BUSI_with_GTdataset/cleanmergedBusi/images\"\n",
    "clean_cbisddsm_masks_dir = '../data/CBISDDSMdataset/cleanmergedCbisddsm/masks'\n",
    "clean_cbisddsm_images_dir = '../data/CBISDDSMdataset/cleanmergedCbisddsm/images'\n",
    "clean_inbreast_masks_dir = '../data/INbreast Release 1.0/cleanmergedInbreast/masks'\n",
    "clean_inbreast_images_dir = '../data/INbreast Release 1.0/cleanmergedInbreast/images'\n",
    "\n",
    "dataset_dirs = {\n",
    "    \"BUSI\": clean_busi_images_dir,\n",
    "    \"BUSI_masks\": clean_busi_masks_dir,\n",
    "    \"QAMEBI\": clean_qamebi_dir,\n",
    "    \"Mendeley\": clean_mendeley_dir,\n",
    "    \"INBreast\": clean_inbreast_images_dir,\n",
    "    \"INBreast_masks\": clean_inbreast_masks_dir,\n",
    "    \"CBIS-DDSM\": clean_cbisddsm_images_dir,\n",
    "    \"CBIS-DDSM_masks\": clean_cbisddsm_masks_dir,\n",
    "    \"Thammasat\": clean_thammasat_dir,\n",
    "    \"BrEaST\": clean_BrEaST_dir\n",
    "}\n",
    "\n",
    "#Hold images for each class and their masks\n",
    "image_variables = {\n",
    "    'BUSI': {\n",
    "        'benign': [],\n",
    "        'malignant': [],\n",
    "        'normal': []\n",
    "    },\n",
    "    'BUSI_masks': {\n",
    "        'benign_mask': [],\n",
    "        'malignant_mask': [],\n",
    "        'normal_mask': []\n",
    "    },\n",
    "    'QAMEBI': {\n",
    "        'benign': [],\n",
    "        'malignant': [],\n",
    "        'normal': []\n",
    "    },\n",
    "    'Mendeley': {\n",
    "        'benign': [],\n",
    "        'malignant': [],\n",
    "        'normal': []\n",
    "    },\n",
    "    'Thammasat': {\n",
    "        'benign': [],\n",
    "        'malignant': [],\n",
    "        'normal': []\n",
    "    },\n",
    "    'BrEaST': {\n",
    "        'benign': [],\n",
    "        'malignant': [],\n",
    "        'normal': []\n",
    "    },\n",
    "    'INBreast': {\n",
    "        'benign': [],\n",
    "        'malignant': [],\n",
    "        'normal': []\n",
    "    },\n",
    "    'INBreast_masks': {\n",
    "        'benign_mask': [],\n",
    "        'malignant_mask': [],\n",
    "        'normal_mask': []\n",
    "    },\n",
    "    'CBIS-DDSM': {\n",
    "        'benign': [],\n",
    "        'malignant': [],\n",
    "        'normal': []\n",
    "    },\n",
    "    'CBIS-DDSM_masks': {\n",
    "        'benign_mask': [],\n",
    "        'malignant_mask': [],\n",
    "        'normal_mask': []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate images and masks by class based on keywords\n",
    "def populate_image_variables(dataset_dir, variable, mask=False):\n",
    "    # Determine classification names depending on whether it is mask data\n",
    "    class_names = ['benign_mask', 'malignant_mask', 'normal_mask'] if mask else ['benign', 'malignant', 'normal']\n",
    "    \n",
    "    for img_name in os.listdir(dataset_dir):\n",
    "        img_path = os.path.join(dataset_dir, img_name)\n",
    "        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            found_class = None\n",
    "            for class_name in class_names:\n",
    "                if re.search(class_name.replace('_mask', ''), img_name, re.IGNORECASE):\n",
    "                    variable[class_name].append(img_path)\n",
    "                    found_class = class_name\n",
    "                    break\n",
    "            if not found_class:\n",
    "                print(f\"Unmatched file in {dataset_dir}: {img_name}\")\n",
    "\n",
    "# Match INBreast masks with images based on ID and classify by type\n",
    "def populate_inbreast_masks(image_dir, mask_dir, variable):\n",
    "    \n",
    "    image_name_map = {img.split(\"_\")[0]: img for img in os.listdir(image_dir) if img.lower().endswith(('.png', '.jpg', '.jpeg'))}\n",
    "    \n",
    "    for mask_name in os.listdir(mask_dir):\n",
    "        mask_path = os.path.join(mask_dir, mask_name)\n",
    "        if mask_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            mask_prefix = mask_name.split(\"_\")[0]\n",
    "            if mask_prefix in image_name_map:\n",
    "                image_name = image_name_map[mask_prefix]\n",
    "                \n",
    "                if \"benign\" in image_name.lower():\n",
    "                    variable['benign_mask'].append(mask_path)\n",
    "                elif \"malignant\" in image_name.lower():\n",
    "                    variable['malignant_mask'].append(mask_path)\n",
    "                elif \"normal\" in image_name.lower():\n",
    "                    variable['normal_mask'].append(mask_path)\n",
    "            else:\n",
    "                print(f\"No match for mask file: {mask_name}\")\n",
    "\n",
    "\n",
    "populate_image_variables(clean_busi_images_dir, image_variables['BUSI'])\n",
    "populate_image_variables(clean_busi_masks_dir, image_variables['BUSI_masks'], mask=True)\n",
    "populate_image_variables(clean_qamebi_dir, image_variables['QAMEBI'])\n",
    "populate_image_variables(clean_thammasat_dir, image_variables['Thammasat'])\n",
    "populate_image_variables(clean_mendeley_dir, image_variables['Mendeley'])\n",
    "populate_image_variables(clean_BrEaST_dir, image_variables['BrEaST'])\n",
    "populate_image_variables(clean_inbreast_images_dir, image_variables['INBreast'])\n",
    "populate_inbreast_masks(clean_inbreast_images_dir, clean_inbreast_masks_dir, image_variables['INBreast_masks'])\n",
    "populate_image_variables(clean_cbisddsm_images_dir, image_variables['CBIS-DDSM'])\n",
    "populate_image_variables(clean_cbisddsm_masks_dir, image_variables['CBIS-DDSM_masks'], mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entries in each key/subkey of image_variables\n",
    "def count_entries_in_image_variables(variable):\n",
    "    counts = {}\n",
    "    for main_key, sub_dict in variable.items():\n",
    "        counts[main_key] = {}\n",
    "        for sub_key, images_list in sub_dict.items():\n",
    "            counts[main_key][sub_key] = len(images_list)\n",
    "    return counts\n",
    "\n",
    "image_variable_counts = count_entries_in_image_variables(image_variables)\n",
    "\n",
    "for main_key, sub_counts in image_variable_counts.items():\n",
    "    print(f\"\\n{main_key}:\")\n",
    "    for sub_key, count in sub_counts.items():\n",
    "    \n",
    "        print(f\"  {sub_key}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Ultrasound dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images by class across datasets\n",
    "def count_images_by_class(images):\n",
    "    counts = Counter()\n",
    "    for img in images:\n",
    "        if \"benign\" in img:\n",
    "            counts['benign'] += 1\n",
    "        elif \"malignant\" in img:\n",
    "            counts['malignant'] += 1\n",
    "        elif \"normal\" in img:\n",
    "            counts['normal'] += 1\n",
    "    return counts\n",
    "\n",
    "# Count masks by class\n",
    "def count_masks_by_class(masks):\n",
    "    counts = Counter()\n",
    "    for mask in masks:\n",
    "        if \"benign\" in mask:\n",
    "            counts['benign'] += 1\n",
    "        elif \"malignant\" in mask:\n",
    "            counts['malignant'] += 1\n",
    "        elif \"normal\" in mask:\n",
    "            counts['normal'] += 1\n",
    "    return counts\n",
    "\n",
    "# Random selection of specified amount for pretrain donation\n",
    "pretrain_busi_images = {\n",
    "    'benign': random.sample(image_variables['BUSI']['benign'], 234),\n",
    "    'malignant': random.sample(image_variables['BUSI']['malignant'], 47),\n",
    "    'normal': random.sample(image_variables['BUSI']['normal'], 100)\n",
    "}\n",
    "\n",
    "# Collect remaining images for finetune\n",
    "finetune_busi_images = {\n",
    "    'benign': list(set(image_variables['BUSI']['benign']) - set(pretrain_busi_images['benign'])),\n",
    "    'malignant': list(set(image_variables['BUSI']['malignant']) - set(pretrain_busi_images['malignant'])),\n",
    "    'normal': list(set(image_variables['BUSI']['normal']) - set(pretrain_busi_images['normal']))\n",
    "}\n",
    "\n",
    "# Get corresponding masks for finetune images\n",
    "finetune_busi_masks = {\n",
    "    'benign': [\n",
    "        mask for img in finetune_busi_images['benign']\n",
    "        for mask in image_variables['BUSI_masks']['benign_mask']\n",
    "        if os.path.basename(img).replace('.png', '') in os.path.basename(mask)\n",
    "    ],\n",
    "    'malignant': [\n",
    "        mask for img in finetune_busi_images['malignant']\n",
    "        for mask in image_variables['BUSI_masks']['malignant_mask']\n",
    "        if os.path.basename(img).replace('.png', '') in os.path.basename(mask)\n",
    "    ],\n",
    "    'normal': [\n",
    "        mask for img in finetune_busi_images['normal']\n",
    "        for mask in image_variables['BUSI_masks']['normal_mask']\n",
    "        if os.path.basename(img).replace('.png', '') in os.path.basename(mask)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pretrain_images = []\n",
    "\n",
    "# Populate images for each class\n",
    "for dataset in ['QAMEBI', 'Mendeley', 'Thammasat', 'BrEaST']:\n",
    "    all_pretrain_images.extend(set(image_variables[dataset]['benign']))\n",
    "    all_pretrain_images.extend(set(image_variables[dataset]['malignant']))\n",
    "    all_pretrain_images.extend(set(image_variables[dataset]['normal']))\n",
    "\n",
    "# Confirm count after filtering duplicates\n",
    "benign_count = len([img for img in all_pretrain_images if 'benign' in img.lower()])\n",
    "malignant_count = len([img for img in all_pretrain_images if 'malignant' in img.lower()])\n",
    "normal_count = len([img for img in all_pretrain_images if 'normal' in img.lower()])\n",
    "\n",
    "print(f\"before donation: Benign count: {benign_count}, Malignant count: {malignant_count}, Normal count: {normal_count}\")\n",
    "\n",
    "all_pretrain_images.extend(pretrain_busi_images['benign'])\n",
    "all_pretrain_images.extend(pretrain_busi_images['malignant'])\n",
    "all_pretrain_images.extend(pretrain_busi_images['normal'])\n",
    "\n",
    "# Confirm benign count after filtering duplicates\n",
    "benign_count = len([img for img in all_pretrain_images if 'benign' in img.lower()])\n",
    "malignant_count = len([img for img in all_pretrain_images if 'malignant' in img.lower()])\n",
    "normal_count = len([img for img in all_pretrain_images if 'normal' in img.lower()])\n",
    "\n",
    "print(f\"after donation: Benign count: {benign_count}, Malignant count: {malignant_count}, Normal count: {normal_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_images(images, train_ratio, val_ratio, test_ratio=0):\n",
    "    if len(images) < 2:\n",
    "        return images, [], [] if test_ratio > 0 else images, []\n",
    "    \n",
    "    train_images, temp_images = train_test_split(images, train_size=train_ratio, random_state=42)\n",
    "    \n",
    "    if test_ratio > 0:\n",
    "        val_images, test_images = train_test_split(temp_images, test_size=test_ratio / (val_ratio + test_ratio), random_state=42)\n",
    "        return train_images, val_images, test_images\n",
    "    else:\n",
    "        val_images = temp_images\n",
    "        return train_images, val_images\n",
    "\n",
    "def match_masks(image_list, masks, masks_folder):\n",
    "    matched_masks = []\n",
    "    \n",
    "    for img in image_list:\n",
    "        base_name = img.split('\\\\')[-1].split('.')[0] \n",
    "\n",
    "        expected_mask_name = f\"{base_name}_mask.png\"\n",
    "        expected_mask_path = f\"{masks_folder}\\\\{expected_mask_name}\"  \n",
    "\n",
    "        # Check expected mask path exists in the masks list\n",
    "        if expected_mask_path in masks:\n",
    "            matched_masks.append(expected_mask_path)\n",
    "        else:\n",
    "            matched_masks.append(None)\n",
    "    \n",
    "    return matched_masks\n",
    "\n",
    "# Separate each class in all_pretrain_images and finetune_busi_images\n",
    "pretrain_benign = [img for img in all_pretrain_images if 'benign' in img.lower()]\n",
    "pretrain_malignant = [img for img in all_pretrain_images if 'malignant' in img.lower()]\n",
    "pretrain_normal = [img for img in all_pretrain_images if 'normal' in img.lower()]\n",
    "\n",
    "# Repeat for finetune data, but with test set included\n",
    "finetune_benign = [img for img in finetune_busi_images['benign'] if 'benign' in img.lower()]\n",
    "finetune_malignant = [img for img in finetune_busi_images[\"malignant\"] if 'malignant' in img.lower()]\n",
    "finetune_normal = [img for img in finetune_busi_images['normal'] if 'normal' in img.lower()]\n",
    "\n",
    "# Split each class\n",
    "train_benign, val_benign = split_images(pretrain_benign, 0.8, 0.2)\n",
    "train_malignant, val_malignant = split_images(pretrain_malignant, 0.8, 0.2)\n",
    "train_normal, val_normal = split_images(pretrain_normal, 0.8, 0.2)\n",
    "\n",
    "# Repeat for finetune data, but with test set included\n",
    "train_benign_ft, val_benign_ft, test_benign_ft = split_images(finetune_benign, 0.6, 0.2, 0.2)\n",
    "train_malignant_ft, val_malignant_ft, test_malignant_ft = split_images(finetune_malignant, 0.6, 0.2, 0.2)\n",
    "train_normal_ft, val_normal_ft, test_normal_ft = split_images(finetune_normal, 0.6, 0.2, 0.2)\n",
    "\n",
    "train_benign_masks_ft = match_masks(train_benign_ft, finetune_busi_masks[\"benign\"], clean_busi_masks_dir)\n",
    "val_benign_masks_ft = match_masks(val_benign_ft, finetune_busi_masks[\"benign\"], clean_busi_masks_dir)\n",
    "test_benign_masks_ft = match_masks(test_benign_ft, finetune_busi_masks[\"benign\"], clean_busi_masks_dir)\n",
    "\n",
    "train_malignant_masks_ft = match_masks(train_malignant_ft, finetune_busi_masks[\"malignant\"], clean_busi_masks_dir)\n",
    "val_malignant_masks_ft = match_masks(val_malignant_ft, finetune_busi_masks[\"malignant\"], clean_busi_masks_dir)\n",
    "test_malignant_masks_ft = match_masks(test_malignant_ft, finetune_busi_masks[\"malignant\"], clean_busi_masks_dir)\n",
    "\n",
    "train_normal_masks_ft = match_masks(train_normal_ft, finetune_busi_masks[\"normal\"], clean_busi_masks_dir)\n",
    "val_normal_masks_ft = match_masks(val_normal_ft, finetune_busi_masks[\"normal\"], clean_busi_masks_dir)\n",
    "test_normal_masks_ft = match_masks(test_normal_ft, finetune_busi_masks[\"normal\"], clean_busi_masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_chart(class_name, train_count, val_count, test_count=None):\n",
    "    sizes = [train_count, val_count] if test_count is None else [train_count, val_count, test_count]\n",
    "    labels = ['Train', 'Validation'] if test_count is None else ['Train', 'Validation', 'Test']\n",
    "    colors = ['gold', 'lightcoral', 'lightskyblue']\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title(f'{class_name.capitalize()} Class Distribution')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each class in pretrain\n",
    "plot_pie_chart('Benign', len(train_benign), len(val_benign))\n",
    "plot_pie_chart('Malignant', len(train_malignant), len(val_malignant))\n",
    "plot_pie_chart('Normal', len(train_normal), len(val_normal))\n",
    "\n",
    "# Plot for each class in finetune\n",
    "plot_pie_chart('Benign', len(train_benign_ft), len(val_benign_ft), len(test_benign_ft))\n",
    "plot_pie_chart('Malignant', len(train_malignant_ft), len(val_malignant_ft), len(test_malignant_ft))\n",
    "plot_pie_chart('Normal', len(train_normal_ft), len(val_normal_ft), len(test_normal_ft))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming pretrain phase has been split into these variables\n",
    "total_train_pretrain = len(train_benign) + len(train_malignant) + len(train_normal)\n",
    "total_val_pretrain = len(val_benign) + len(val_malignant) + len(val_normal)\n",
    "\n",
    "# Plotting the overall distribution\n",
    "def plot_overall_pie_chart(train_count, val_count):\n",
    "    sizes = [train_count, val_count]\n",
    "    labels = ['Train', 'Validation']\n",
    "    colors = ['lightcoral', 'lightskyblue']\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title('Overall Distribution in Pretraining Phase')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "plot_overall_pie_chart(total_train_pretrain, total_val_pretrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming finetune phase has been split into these variables\n",
    "total_train_finetune = len(train_benign_ft) + len(train_malignant_ft) + len(train_normal_ft)\n",
    "total_val_finetune = len(val_benign_ft) + len(val_malignant_ft) + len(val_normal_ft)\n",
    "total_test_finetune = len(test_benign_ft) + len(test_malignant_ft) + len(test_normal_ft)\n",
    "\n",
    "# Plotting the overall distribution\n",
    "def plot_overall_pie_chart_finetune(train_count, val_count, test_count):\n",
    "    sizes = [train_count, val_count, test_count]\n",
    "    labels = ['Train', 'Validation', 'Test']\n",
    "    colors = ['lightcoral', 'lightskyblue', 'gold']\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title('Overall Distribution in Finetuning Phase')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "plot_overall_pie_chart_finetune(total_train_finetune, total_val_finetune, total_test_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultrasound Dataset\n",
    "\n",
    "# Pretrain data: train and validation images only (no subfolders by class)\n",
    "for split, images in [(\"train\", train_benign + train_malignant + train_normal),\n",
    "                      (\"validation\", val_benign + val_malignant + val_normal)]:\n",
    "    split_dir = os.path.join(ultrasound_pretrain_dir, split)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    for img_path in images:\n",
    "        shutil.copy(img_path, split_dir)\n",
    "\n",
    "# Finetune data: Organized into train, validation, test with class-specific subfolders for images and masks\n",
    "for split, class_data in [(\"train\", [(train_benign_ft, train_benign_masks_ft, \"benign\"),\n",
    "                                     (train_malignant_ft, train_malignant_masks_ft, \"malignant\"),\n",
    "                                     (train_normal_ft, train_normal_masks_ft, \"normal\")]),\n",
    "                          (\"validation\", [(val_benign_ft, val_benign_masks_ft, \"benign\"),\n",
    "                                          (val_malignant_ft, val_malignant_masks_ft, \"malignant\"),\n",
    "                                          (val_normal_ft, val_normal_masks_ft, \"normal\")]),\n",
    "                          (\"test\", [(test_benign_ft, test_benign_masks_ft, \"benign\"),\n",
    "                                    (test_malignant_ft, test_malignant_masks_ft, \"malignant\"),\n",
    "                                    (test_normal_ft, test_normal_masks_ft, \"normal\")])]:\n",
    "    \n",
    "    split_dir = os.path.join(ultrasound_finetune_dir, split)\n",
    "    for images, masks, class_name in class_data:\n",
    "        class_img_dir = os.path.join(split_dir, class_name, \"images\")\n",
    "        class_mask_dir = os.path.join(split_dir, class_name, \"masks\")\n",
    "        os.makedirs(class_img_dir, exist_ok=True)\n",
    "        os.makedirs(class_mask_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy images\n",
    "        for img_path in images:\n",
    "            shutil.copy(img_path, class_img_dir)\n",
    "        \n",
    "        # Copy corresponding masks\n",
    "        for mask_path in masks:\n",
    "            shutil.copy(mask_path, class_mask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Mammography dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malignant_images = image_variables['CBIS-DDSM']['malignant']\n",
    "\n",
    "# Select specified amount random malignant images to set aside\n",
    "donated_malignant_images = random.sample(malignant_images, 63)\n",
    "\n",
    "# Create new variables for malignant images\n",
    "pretrain_cbis_malignant_images = [img for img in malignant_images if img not in donated_malignant_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the paths of the corresponding masks for the donated malignant images\n",
    "finetune_cbis_malignant_masks = []\n",
    "\n",
    "def extract_cbis_id(filename):\n",
    "    # Match sequence of digits at the start of the filename\n",
    "    match = re.search(r'_id(\\d+)_', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "for image_path in donated_malignant_images:\n",
    "    \n",
    "    image_filename = os.path.basename(image_path)\n",
    "    image_id = extract_cbis_id(image_filename)\n",
    "    if image_id:\n",
    "        for mask_name in os.listdir(clean_cbisddsm_masks_dir):\n",
    "            if re.search(f\"id{image_id}_malignant\", mask_name):\n",
    "                mask_path = os.path.join(clean_cbisddsm_masks_dir, mask_name)\n",
    "                finetune_cbis_malignant_masks.append(mask_path)\n",
    "                break\n",
    "        else:\n",
    "            print(f\"No mask found for image with ID {image_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating individual variables for INBreast dataset images and masks\n",
    "finetune_inbreast_benign_images = image_variables['INBreast']['benign']\n",
    "finetune_inbreast_malignant_images = image_variables['INBreast']['malignant']\n",
    "finetune_inbreast_normal_images = image_variables['INBreast']['normal']\n",
    "finetune_inbreast_benign_masks = image_variables['INBreast_masks']['benign_mask']\n",
    "finetune_inbreast_malignant_masks = image_variables['INBreast_masks']['malignant_mask']\n",
    "finetune_inbreast_normal_masks = image_variables['INBreast_masks']['normal_mask']\n",
    "\n",
    "finetune_cbis_malignant_images = donated_malignant_images\n",
    "\n",
    "# Creating individual variables for CBIS-DDSM dataset images and masks\n",
    "pretrain_cbis_benign_images = image_variables['CBIS-DDSM']['benign']\n",
    "pretrain_cbis_normal_images = image_variables['CBIS-DDSM']['normal']\n",
    "\n",
    "print(f\"INBreast Benign Images: {len(finetune_inbreast_benign_images)}\")\n",
    "print(f\"INBreast Malignant Images: {len(finetune_inbreast_malignant_images)}\")\n",
    "print(f\"INBreast Normal Images: {len(finetune_inbreast_normal_images)}\")\n",
    "print(f\"INBreast Benign Masks: {len(finetune_inbreast_benign_masks)}\")\n",
    "print(f\"INBreast Malignant Masks: {len(finetune_inbreast_malignant_masks)}\")\n",
    "print(f\"INBreast Normal Masks: {len(finetune_inbreast_normal_masks)}\\n\")\n",
    "\n",
    "print(f\"CBIS Normal Images: {len(pretrain_cbis_normal_images)}\")\n",
    "print(f\"CBIS Benign Images: {len(pretrain_cbis_benign_images)}\")\n",
    "print(f\"CBIS Malignant Images (Full Set Minus Donated): {len(pretrain_cbis_malignant_images)}\")\n",
    "print(f\"CBIS Donated Malignant Images: {len(finetune_cbis_malignant_images)}\")\n",
    "print(f\"CBIS Donated Malignant Masks: {len(finetune_cbis_malignant_masks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Discard specified amount of normal CBIS-DDSM images\n",
    "discarded_normals_254 = random.sample(pretrain_cbis_normal_images, 254)\n",
    "pretrain_cbis_normal_images = [img for img in pretrain_cbis_normal_images if img not in discarded_normals_254]\n",
    "\n",
    "# 2. Discard specified amount of benign CBIS-DDSM images\n",
    "discarded_benign_589 = random.sample(pretrain_cbis_benign_images, 589)\n",
    "pretrain_cbis_benign_images = [img for img in pretrain_cbis_benign_images if img not in discarded_benign_589]\n",
    "\n",
    "# 3. Discard additional specified amount of malignant CBIS-DDSM images\n",
    "discarded_malignant_629 = random.sample(pretrain_cbis_malignant_images, 629)\n",
    "pretrain_cbis_malignant_images = [img for img in pretrain_cbis_malignant_images if img not in discarded_malignant_629]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_images_and_masks(images_list, masks_list, num_discard):\n",
    "\n",
    "    discarded_images = random.sample(images_list, num_discard)\n",
    "    discarded_ids = {os.path.basename(path).split('_')[0] for path in discarded_images}\n",
    "\n",
    "    # Filter out discarded images\n",
    "    updated_images = [img for img in images_list if os.path.basename(img).split('_')[0] not in discarded_ids]\n",
    "\n",
    "    # Filter out corresponding masks by matching IDs\n",
    "    updated_masks = [mask for mask in masks_list if os.path.basename(mask).split('_')[0] not in discarded_ids]\n",
    "\n",
    "    return updated_images, updated_masks\n",
    "\n",
    "# 4. Discard 34 normal INBreast images and their corresponding masks\n",
    "finetune_inbreast_normal_images, finetune_inbreast_normal_masks = discard_images_and_masks(\n",
    "    finetune_inbreast_normal_images, finetune_inbreast_normal_masks, 34\n",
    ")\n",
    "\n",
    "# 5. Discard 40 benign images and their corresponding masks\n",
    "finetune_inbreast_benign_images, finetune_inbreast_benign_masks = discard_images_and_masks(\n",
    "    finetune_inbreast_benign_images, finetune_inbreast_benign_masks, 40\n",
    ")\n",
    "\n",
    "combined_finetune_malignant_masks = finetune_inbreast_malignant_masks + finetune_cbis_malignant_masks\n",
    "\n",
    "combined_finetune_malignant_images = finetune_inbreast_malignant_images + finetune_cbis_malignant_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Verification\n",
    "\n",
    "print(f\"Final Pretrain CBIS Normal Images: {len(pretrain_cbis_normal_images)}\")\n",
    "print(f\"Final Pretrain CBIS Benign Images: {len(pretrain_cbis_benign_images)}\")\n",
    "print(f\"Final Pretrain CBIS Malignant Images: {len(pretrain_cbis_malignant_images)}\")\n",
    "print(f\"Final Finetune CBIS Malignant Images: {len(finetune_cbis_malignant_images)}\")\n",
    "print(f\"Final Finetune CBIS Malignant Masks: {len(finetune_cbis_malignant_masks)}\")\n",
    "print(f\"Final Finetune INBreast Normal Images: {len(finetune_inbreast_normal_images)}\")\n",
    "print(f\"Final Finetune INBreast Benign Images: {len(finetune_inbreast_benign_images)}\")\n",
    "print(f\"Final Finetune INBreast Malignant Images: {len(finetune_inbreast_malignant_images)}\")\n",
    "print(f\"Final Finetune INBreast Benign Masks: {len(finetune_inbreast_benign_masks)}\")\n",
    "print(f\"Final Finetune INBreast Normal Masks: {len(finetune_inbreast_normal_masks)}\")\n",
    "print(f\"Final Finetune INBreast Malignant Masks: {len(finetune_inbreast_malignant_masks)}\")\n",
    "print(f\"Final Finetune Combined Malignant Images: {len(combined_finetune_malignant_images)}\")\n",
    "print(f\"Final Finetune Combined Malignant Masks: {len(combined_finetune_malignant_masks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_images(images, train_ratio, val_ratio, test_ratio=0):\n",
    "    if len(images) < 2:\n",
    "        return images, [], [] if test_ratio > 0 else images, []\n",
    "    \n",
    "    train_images, temp_images = train_test_split(images, train_size=train_ratio, random_state=42)\n",
    "    \n",
    "    if test_ratio > 0:\n",
    "        val_images, test_images = train_test_split(temp_images, test_size=test_ratio / (val_ratio + test_ratio), random_state=42)\n",
    "        return train_images, val_images, test_images\n",
    "    else:\n",
    "        val_images = temp_images\n",
    "        return train_images, val_images\n",
    "\n",
    "def match_masks_mammography(image_list, masks, masks_folder, dataset_type):\n",
    "    matched_masks = []\n",
    "    \n",
    "    for img in image_list:\n",
    "        if dataset_type == \"inbreast\":\n",
    "            parts = img.split('_')\n",
    "            img_id = parts[0]  # The ID part\n",
    "            if 'normal' in img:  # Check if it's a normal case\n",
    "                expected_mask_name = f\"{img_id}_normal_syntheticMask.png\"\n",
    "            else:\n",
    "                img_class = parts[-1]  # The class part\n",
    "                expected_mask_name = f\"{img_id}_{img_class}_mask.png\"\n",
    "\n",
    "        elif dataset_type == \"cbis\":\n",
    "            img_id = [part for part in img.split('_') if 'id' in part][0].replace('id', '')\n",
    "            img_class = img.split('_')[-1]  # The class part\n",
    "            expected_mask_name = f\"*_{img_id}_{img_class}_mask.png\"  # Use wildcard to ignore variable prefix\n",
    "\n",
    "        # Construct the full mask path\n",
    "        expected_mask_path = f\"{masks_folder}/{expected_mask_name}\"\n",
    "\n",
    "        # Check expected mask path exists in the masks list\n",
    "        if expected_mask_path in masks:\n",
    "            matched_masks.append(expected_mask_path)\n",
    "        else:\n",
    "            matched_masks.append(None)\n",
    "\n",
    "    return matched_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_masks_mammography_inbreast(image_list, masks):\n",
    "    matched_masks = []\n",
    "\n",
    "    for img in image_list:\n",
    "        img_filename = os.path.basename(img)  # Get only the filename\n",
    "        img_id = img_filename.split('_')[0]  # ID is the part before the first underscore\n",
    "        img_class = img_filename.split('_')[-1].replace(\".png\", \"\")  # Class is the last part of the filename\n",
    "\n",
    "        search_string = f\"{img_id}_{img_class}\"\n",
    "\n",
    "        mask_found = False\n",
    "\n",
    "        for mask in masks:\n",
    "            mask_filename = os.path.basename(mask)  # Get only the filename\n",
    "\n",
    "            # Check search string is present in the mask filename\n",
    "            if search_string in mask_filename:\n",
    "                matched_masks.append(mask)\n",
    "                mask_found = True\n",
    "                break\n",
    "        \n",
    "        if not mask_found:\n",
    "            matched_masks.append(None)\n",
    "            print(f\"No match found for {img} -> expected mask with ID {img_id} and class {img_class}\")\n",
    "\n",
    "    return matched_masks\n",
    "\n",
    "def match_masks_mammography_cbis(image_list, masks):\n",
    "    matched_masks = []\n",
    "\n",
    "    for img in image_list:\n",
    "        parts = img.split('_')\n",
    "        img_id = parts[1]  # ID is right after the first underscore\n",
    "        img_class = parts[-1].replace(\".png\", \"\")  # Class is the last part of the filename\n",
    "\n",
    "        mask_found = False\n",
    "        \n",
    "        for mask in masks:\n",
    "            mask_parts = mask.split('_')\n",
    "            mask_id = mask_parts[1]  # ID follows the first underscore in mask\n",
    "\n",
    "            # Check IDs match and if the mask class matches\n",
    "            if mask_id == img_id and img_class in mask:\n",
    "                matched_masks.append(mask)\n",
    "                mask_found = True\n",
    "                break\n",
    "        \n",
    "        if not mask_found:\n",
    "            matched_masks.append(None)\n",
    "            print(f\"No match found for {img} -> expected mask pattern with ID {img_id} and class {img_class}\")\n",
    "\n",
    "    return matched_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pretrain stuff\n",
    "mm_train_benign, mm_val_benign = split_images(pretrain_cbis_benign_images, 0.8, 0.2)\n",
    "mm_train_malignant, mm_val_malignant = split_images(pretrain_cbis_malignant_images, 0.8, 0.2)\n",
    "mm_train_normal, mm_val_normal = split_images(pretrain_cbis_normal_images, 0.8, 0.2)\n",
    "\n",
    "# 2 Finetune stuff\n",
    "# 2.1. Benign\n",
    "mm_train_benign_ft, mm_val_benign_ft, mm_test_benign_ft = split_images(finetune_inbreast_benign_images, 0.6, 0.2, 0.2)\n",
    "mm_train_benign_masks_ft = match_masks_mammography_inbreast(mm_train_benign_ft, finetune_inbreast_benign_masks)\n",
    "mm_val_benign_masks_ft = match_masks_mammography_inbreast(mm_val_benign_ft, finetune_inbreast_benign_masks)\n",
    "mm_test_benign_masks_ft = match_masks_mammography_inbreast(mm_test_benign_ft, finetune_inbreast_benign_masks)\n",
    "\n",
    "# 2.2. Normal\n",
    "mm_train_normal_ft, mm_val_normal_ft, mm_test_normal_ft = split_images(finetune_inbreast_normal_images, 0.6, 0.2, 0.2)\n",
    "mm_train_normal_masks_ft = match_masks_mammography_inbreast(mm_train_normal_ft, finetune_inbreast_normal_masks)\n",
    "mm_val_normal_masks_ft = match_masks_mammography_inbreast(mm_val_normal_ft, finetune_inbreast_normal_masks)\n",
    "mm_test_normal_masks_ft = match_masks_mammography_inbreast(mm_test_normal_ft, finetune_inbreast_normal_masks)\n",
    "\n",
    "# 2.3. Malignant (some cbis images/masks and some inbreast images/masks)\n",
    "mm_train_cbis_malignant_aux, mm_val_cbis_malignant_aux, mm_test_cbis_malignant_aux = split_images(finetune_cbis_malignant_images, 0.6, 0.2, 0.2)\n",
    "mm_train_cbis_malignant_masks_aux = match_masks_mammography_cbis(mm_train_cbis_malignant_aux, finetune_cbis_malignant_masks)\n",
    "mm_val_cbis_malignant_masks_aux = match_masks_mammography_cbis(mm_val_cbis_malignant_aux, finetune_cbis_malignant_masks)\n",
    "mm_test_cbis_malignant_masks_aux = match_masks_mammography_cbis(mm_test_cbis_malignant_aux, finetune_cbis_malignant_masks)\n",
    "\n",
    "mm_train_inbreast_malignant_aux, mm_val_inbreast_malignant_aux, mm_test_inbreast_malignant_aux = split_images(finetune_inbreast_malignant_images, 0.6, 0.2, 0.2)\n",
    "mm_train_inbreast_malignant_masks_aux = match_masks_mammography_inbreast(mm_train_inbreast_malignant_aux, finetune_inbreast_malignant_masks)\n",
    "mm_val_inbreast_malignant_masks_aux = match_masks_mammography_inbreast(mm_val_inbreast_malignant_aux, finetune_inbreast_malignant_masks)\n",
    "mm_test_inbreast_malignant_masks_aux = match_masks_mammography_inbreast(mm_test_inbreast_malignant_aux, finetune_inbreast_malignant_masks)\n",
    "\n",
    "# Merge sets of images\n",
    "mm_train_malignant_ft = mm_train_cbis_malignant_aux + mm_train_inbreast_malignant_aux\n",
    "mm_val_malignant_ft = mm_val_cbis_malignant_aux + mm_val_inbreast_malignant_aux\n",
    "mm_test_malignant_ft = mm_test_cbis_malignant_aux + mm_test_inbreast_malignant_aux\n",
    "\n",
    "# Merge sets of masks\n",
    "mm_train_malignant_masks_ft = mm_train_cbis_malignant_masks_aux + mm_train_inbreast_malignant_masks_aux\n",
    "mm_val_malignant_masks_ft = mm_val_cbis_malignant_masks_aux + mm_val_inbreast_malignant_masks_aux\n",
    "mm_test_malignant_masks_ft = mm_test_cbis_malignant_masks_aux + mm_test_inbreast_malignant_masks_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected counts\n",
    "expected_pretrain_counts = {\n",
    "    'benign': {'train': 513, 'val': 129},\n",
    "    'normal': {'train': 83, 'val': 21},\n",
    "    'malignant': {'train': 396, 'val': 99}\n",
    "}\n",
    "\n",
    "expected_finetune_counts = {\n",
    "    'benign': {'train': 121, 'val': 41, 'test': 41},\n",
    "    'normal': {'train': 19, 'val': 7, 'test': 7},\n",
    "    'malignant': {\n",
    "        'cbis': {'train': 37, 'val': 13, 'test': 13},\n",
    "        'inbreast': {'train': 60, 'val': 20, 'test': 20}\n",
    "    }\n",
    "}\n",
    "\n",
    "expected_train_benign, expected_val_benign = len(mm_train_benign), len(mm_val_benign)\n",
    "expected_train_malignant, expected_val_malignant = len(mm_train_malignant), len(mm_val_malignant)\n",
    "expected_train_normal, expected_val_normal = len(mm_train_normal), len(mm_val_normal)\n",
    "\n",
    "# Finetune splits for benign\n",
    "expected_train_benign_ft, expected_val_benign_ft, expected_test_benign_ft = (\n",
    "    len(mm_train_benign_ft), len(mm_val_benign_ft), len(mm_test_benign_ft)\n",
    ")\n",
    "\n",
    "# Finetune splits for normal\n",
    "expected_train_normal_ft, expected_val_normal_ft, expected_test_normal_ft = (\n",
    "    len(mm_train_normal_ft), len(mm_val_normal_ft), len(mm_test_normal_ft)\n",
    ")\n",
    "\n",
    "# Finetune splits for malignant (both CBIS and INBreast)\n",
    "expected_train_malignant_ft, expected_val_malignant_ft, expected_test_malignant_ft = (\n",
    "    len(train_malignant_ft), len(val_malignant_ft), len(test_malignant_ft)\n",
    ")\n",
    "\n",
    "# Checks\n",
    "# Pretrain splits\n",
    "assert len(mm_train_benign) == expected_pretrain_counts['benign']['train'], \"Mismatch in train_benign count for pretrain\"\n",
    "assert len(mm_val_benign) == expected_pretrain_counts['benign']['val'], \"Mismatch in val_benign count for pretrain\"\n",
    "assert len(mm_train_normal) == expected_pretrain_counts['normal']['train'], \"Mismatch in train_normal count for pretrain\"\n",
    "assert len(mm_val_normal) == expected_pretrain_counts['normal']['val'], \"Mismatch in val_normal count for pretrain\"\n",
    "assert len(mm_train_malignant) == expected_pretrain_counts['malignant']['train'], \"Mismatch in train_malignant count for pretrain\"\n",
    "assert len(mm_val_malignant) == expected_pretrain_counts['malignant']['val'], \"Mismatch in val_malignant count for pretrain\"\n",
    "\n",
    "# Finetune splits for benign and normal\n",
    "assert len(mm_train_benign_ft) == expected_finetune_counts['benign']['train'], \"Mismatch in train_benign_ft count for finetune\"\n",
    "assert len(mm_val_benign_ft) == expected_finetune_counts['benign']['val'], \"Mismatch in val_benign_ft count for finetune\"\n",
    "assert len(mm_test_benign_ft) == expected_finetune_counts['benign']['test'], \"Mismatch in test_benign_ft count for finetune\"\n",
    "\n",
    "assert len(mm_train_normal_ft) == expected_finetune_counts['normal']['train'], \"Mismatch in train_normal_ft count for finetune\"\n",
    "assert len(mm_val_normal_ft) == expected_finetune_counts['normal']['val'], \"Mismatch in val_normal_ft count for finetune\"\n",
    "assert len(mm_test_normal_ft) == expected_finetune_counts['normal']['test'], \"Mismatch in test_normal_ft count for finetune\"\n",
    "\n",
    "# Finetune splits for malignant from CBIS and INBreast\n",
    "assert len(mm_train_cbis_malignant_aux) == expected_finetune_counts['malignant']['cbis']['train'], \"Mismatch in train_cbis_malignant_aux count for finetune\"\n",
    "assert len(mm_val_cbis_malignant_aux) == expected_finetune_counts['malignant']['cbis']['val'], \"Mismatch in val_cbis_malignant_aux count for finetune\"\n",
    "assert len(mm_test_cbis_malignant_aux) == expected_finetune_counts['malignant']['cbis']['test'], \"Mismatch in test_cbis_malignant_aux count for finetune\"\n",
    "\n",
    "assert len(mm_train_inbreast_malignant_aux) == expected_finetune_counts['malignant']['inbreast']['train'], \"Mismatch in train_inbreast_malignant_aux count for finetune\"\n",
    "assert len(mm_val_inbreast_malignant_aux) == expected_finetune_counts['malignant']['inbreast']['val'], \"Mismatch in val_inbreast_malignant_aux count for finetune\"\n",
    "assert len(mm_test_inbreast_malignant_aux) == expected_finetune_counts['malignant']['inbreast']['test'], \"Mismatch in test_inbreast_malignant_aux count for finetune\"\n",
    "\n",
    "# Final finetune malignant combined checks\n",
    "assert len(mm_train_malignant_ft) == expected_finetune_counts['malignant']['cbis']['train'] + expected_finetune_counts['malignant']['inbreast']['train'], \"Mismatch in combined train_malignant_ft count\"\n",
    "assert len(mm_val_malignant_ft) == expected_finetune_counts['malignant']['cbis']['val'] + expected_finetune_counts['malignant']['inbreast']['val'], \"Mismatch in combined val_malignant_ft count\"\n",
    "assert len(mm_test_malignant_ft) == expected_finetune_counts['malignant']['cbis']['test'] + expected_finetune_counts['malignant']['inbreast']['test'], \"Mismatch in combined test_malignant_ft count\"\n",
    "\n",
    "# Mask alignment checks\n",
    "assert len(mm_train_benign_ft) == len(mm_train_benign_masks_ft), \"Mismatch in benign train image-mask count\"\n",
    "assert len(mm_val_benign_ft) == len(mm_val_benign_masks_ft), \"Mismatch in benign validation image-mask count\"\n",
    "assert len(mm_test_benign_ft) == len(mm_test_benign_masks_ft), \"Mismatch in benign test image-mask count\"\n",
    "\n",
    "assert len(mm_train_normal_ft) == len(mm_train_normal_masks_ft), \"Mismatch in normal train image-mask count\"\n",
    "assert len(mm_val_normal_ft) == len(mm_val_normal_masks_ft), \"Mismatch in normal validation image-mask count\"\n",
    "assert len(mm_test_normal_ft) == len(mm_test_normal_masks_ft), \"Mismatch in normal test image-mask count\"\n",
    "\n",
    "# Malignant masks for finetune\n",
    "assert len(mm_train_malignant_ft) == len(mm_train_malignant_masks_ft), \"Mismatch in malignant train image-mask count\"\n",
    "assert len(mm_val_malignant_ft) == len(mm_val_malignant_masks_ft), \"Mismatch in malignant validation image-mask count\"\n",
    "assert len(mm_test_malignant_ft) == len(mm_test_malignant_masks_ft), \"Mismatch in malignant test image-mask count\"\n",
    "\n",
    "print(\"All clear\")\n",
    "\n",
    "# 2. Assertions for Image-Mask Pair Matching\n",
    "# Finetune benign\n",
    "assert len(mm_train_benign_ft) == len(mm_train_benign_masks_ft), \\\n",
    "    f\"Expected {len(mm_train_benign_ft)} masks, found {len(mm_train_benign_masks_ft)} for train_benign_ft\"\n",
    "assert len(mm_val_benign_ft) == len(mm_val_benign_masks_ft), \\\n",
    "    f\"Expected {len(mm_val_benign_ft)} masks, found {len(mm_val_benign_masks_ft)} for val_benign_ft\"\n",
    "assert len(mm_test_benign_ft) == len(mm_test_benign_masks_ft), \\\n",
    "    f\"Expected {len(mm_test_benign_ft)} masks, found {len(mm_test_benign_masks_ft)} for test_benign_ft\"\n",
    "\n",
    "# Finetune normal\n",
    "assert len(mm_train_normal_ft) == len(mm_train_normal_masks_ft), \\\n",
    "    f\"Expected {len(mm_train_normal_ft)} masks, found {len(mm_train_normal_masks_ft)} for train_normal_ft\"\n",
    "assert len(mm_val_normal_ft) == len(val_normal_masks_ft), \\\n",
    "    f\"Expected {len(mm_val_normal_ft)} masks, found {len(mm_val_normal_masks_ft)} for val_normal_ft\"\n",
    "assert len(mm_test_normal_ft) == len(mm_test_normal_masks_ft), \\\n",
    "    f\"Expected {len(mm_test_normal_ft)} masks, found {len(mm_test_normal_masks_ft)} for test_normal_ft\"\n",
    "\n",
    "# Finetune malignant\n",
    "assert len(mm_train_malignant_ft) == len(mm_train_malignant_masks_ft), \\\n",
    "    f\"Expected {len(mm_train_malignant_ft)} masks, found {len(mm_train_malignant_masks_ft)} for train_malignant_ft\"\n",
    "assert len(mm_val_malignant_ft) == len(mm_val_malignant_masks_ft), \\\n",
    "    f\"Expected {len(mm_val_malignant_ft)} masks, found {len(mm_val_malignant_masks_ft)} for val_malignant_ft\"\n",
    "assert len(mm_test_malignant_ft) == len(mm_test_malignant_masks_ft), \\\n",
    "    f\"Expected {len(mm_test_malignant_ft)} masks, found {len(mm_test_malignant_masks_ft)} for test_malignant_ft\"\n",
    "\n",
    "print(\"All clear: counts of images and masks match across variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie_chart('Benign', len(mm_train_benign), len(mm_val_benign))\n",
    "plot_pie_chart('Malignant', len(mm_train_malignant), len(mm_val_malignant))\n",
    "plot_pie_chart('Normal', len(mm_train_normal), len(mm_val_normal))\n",
    "\n",
    "plot_pie_chart('Benign', len(mm_train_benign_ft), len(mm_val_benign_ft), len(mm_test_benign_ft))\n",
    "plot_pie_chart('Normal', len(mm_train_normal_ft), len(mm_val_normal_ft), len(mm_test_normal_ft))\n",
    "plot_pie_chart('Malignant', len(mm_train_malignant_ft), len(mm_val_malignant_ft), len(mm_test_malignant_ft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming pretrain phase has been split into these variables\n",
    "mm_total_train_pretrain = len(mm_train_benign) + len(mm_train_malignant) + len(mm_train_normal)\n",
    "mm_total_val_pretrain = len(mm_val_benign) + len(mm_val_malignant) + len(mm_val_normal)\n",
    "\n",
    "def plot_overall_pie_chart(train_count, val_count):\n",
    "    sizes = [train_count, val_count]\n",
    "    labels = ['Train', 'Validation']\n",
    "    colors = ['lightcoral', 'lightskyblue']\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title('Overall Distribution in Pretraining Phase')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "plot_overall_pie_chart(mm_total_train_pretrain, mm_total_val_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming finetune phase has been split into these variables\n",
    "mm_total_train_finetune = len(mm_train_benign_ft) + len(mm_train_malignant_ft) + len(mm_train_normal_ft)\n",
    "mm_total_val_finetune = len(mm_val_benign_ft) + len(mm_val_malignant_ft) + len(mm_val_normal_ft)\n",
    "mm_total_test_finetune = len(mm_test_benign_ft) + len(mm_test_malignant_ft) + len(mm_test_normal_ft)\n",
    "\n",
    "def plot_overall_pie_chart_finetune(train_count, val_count, test_count):\n",
    "    sizes = [train_count, val_count, test_count]\n",
    "    labels = ['Train', 'Validation', 'Test']\n",
    "    colors = ['lightcoral', 'lightskyblue', 'gold']\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title('Overall Distribution in Finetuning Phase')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "plot_overall_pie_chart_finetune(mm_total_train_finetune, mm_total_val_finetune, mm_total_test_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain data: train and validation images only (no subfolders by class)\n",
    "for split, images in [(\"train\", mm_train_benign + mm_train_malignant + mm_train_normal),\n",
    "                      (\"validation\", mm_val_benign + mm_val_malignant + mm_val_normal)]:\n",
    "    split_dir = os.path.join(mammography_pretrain_dir, split)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    for img_path in images:\n",
    "        shutil.copy(img_path, split_dir)\n",
    "\n",
    "# Finetune data: Organized into train, validation, test with class-specific subfolders for images and masks\n",
    "for split, class_data in [(\"train\", [(mm_train_benign_ft, mm_train_benign_masks_ft, \"benign\"),\n",
    "                                     (mm_train_malignant_ft, mm_train_malignant_masks_ft, \"malignant\"),\n",
    "                                     (mm_train_normal_ft, mm_train_normal_masks_ft, \"normal\")]),\n",
    "                          (\"validation\", [(mm_val_benign_ft, mm_val_benign_masks_ft, \"benign\"),\n",
    "                                          (mm_val_malignant_ft, mm_val_malignant_masks_ft, \"malignant\"),\n",
    "                                          (mm_val_normal_ft, mm_val_normal_masks_ft, \"normal\")]),\n",
    "                          (\"test\", [(mm_test_benign_ft, mm_test_benign_masks_ft, \"benign\"),\n",
    "                                    (mm_test_malignant_ft, mm_test_malignant_masks_ft, \"malignant\"),\n",
    "                                    (mm_test_normal_ft, mm_test_normal_masks_ft, \"normal\")])]:\n",
    "    \n",
    "    split_dir = os.path.join(mammography_finetune_dir, split)\n",
    "    for images, masks, class_name in class_data:\n",
    "        class_img_dir = os.path.join(split_dir, class_name, \"images\")\n",
    "        class_mask_dir = os.path.join(split_dir, class_name, \"masks\")\n",
    "        os.makedirs(class_img_dir, exist_ok=True)\n",
    "        os.makedirs(class_mask_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy images\n",
    "        for img_path in images:\n",
    "            shutil.copy(img_path, class_img_dir)\n",
    "        \n",
    "        # Copy corresponding masks\n",
    "        for mask_path in masks:\n",
    "            shutil.copy(mask_path, class_mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to check for None values\n",
    "variables = {\n",
    "    \"train_benign\": mm_train_benign, \"val_benign\": mm_val_benign,\n",
    "    \"train_malignant\": mm_train_malignant, \"val_malignant\": mm_val_malignant,\n",
    "    \"train_normal\": mm_train_normal, \"val_normal\": mm_val_normal,\n",
    "    \n",
    "    # Finetune variables\n",
    "    \"train_benign_ft\": mm_train_benign_ft, \"val_benign_ft\": mm_val_benign_ft, \"test_benign_ft\": mm_test_benign_ft,\n",
    "    \"train_benign_masks_ft\": mm_train_benign_masks_ft, \"val_benign_masks_ft\": mm_val_benign_masks_ft, \"test_benign_masks_ft\": mm_test_benign_masks_ft,\n",
    "    \n",
    "    \"train_normal_ft\": mm_train_normal_ft, \"val_normal_ft\": mm_val_normal_ft, \"test_normal_ft\": mm_test_normal_ft,\n",
    "    \"train_normal_masks_ft\": mm_train_normal_masks_ft, \"val_normal_masks_ft\": mm_val_normal_masks_ft, \"test_normal_masks_ft\": mm_test_normal_masks_ft,\n",
    "    \n",
    "    # Malignant variables with auxiliary splits\n",
    "    \"train_cbis_malignant_aux\": mm_train_cbis_malignant_aux, \"val_cbis_malignant_aux\": mm_val_cbis_malignant_aux, \"test_cbis_malignant_aux\": mm_test_cbis_malignant_aux,\n",
    "    \"train_cbis_malignant_masks_aux\": mm_train_cbis_malignant_masks_aux, \"val_cbis_malignant_masks_aux\": mm_val_cbis_malignant_masks_aux, \"test_cbis_malignant_masks_aux\": mm_test_cbis_malignant_masks_aux,\n",
    "    \n",
    "    \"train_inbreast_malignant_aux\": mm_train_inbreast_malignant_aux, \"val_inbreast_malignant_aux\": mm_val_inbreast_malignant_aux, \"test_inbreast_malignant_aux\": mm_test_inbreast_malignant_aux,\n",
    "    \"train_inbreast_malignant_masks_aux\": mm_train_inbreast_malignant_masks_aux, \"val_inbreast_malignant_masks_aux\": mm_val_inbreast_malignant_masks_aux, \"test_inbreast_malignant_masks_aux\": mm_test_inbreast_malignant_masks_aux\n",
    "}\n",
    "\n",
    "# Check for None values in each variable\n",
    "for var_name, paths in variables.items():\n",
    "    if any(path is None for path in paths):\n",
    "        print(f\"Warning: None value(s) found in variable '{var_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Multimodal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a 50% random sample from each given list\n",
    "def random_50_percent(data_list):\n",
    "    # Calculate half the length of the list, rounded down for odd counts\n",
    "    sample_size = len(data_list) // 2\n",
    "    # Randomly select sample_size items from the data_list\n",
    "    return random.sample(data_list, sample_size)\n",
    "\n",
    "# Function to pick 50% of images and get the corresponding masks\n",
    "def get_half_images_and_masks(image_list, mask_list, match_function, **match_args):\n",
    "    # Select 50% of the images randomly\n",
    "    half_image_list = random.sample(image_list, len(image_list) // 2)\n",
    "    \n",
    "    # Use the matching function to find corresponding masks for the 50% images\n",
    "    half_mask_list = match_function(half_image_list, mask_list, **match_args)\n",
    "    \n",
    "    return half_image_list, half_mask_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain\n",
    "# Ultra\n",
    "us_train_benign_50 = random_50_percent(train_benign)\n",
    "us_train_malignant_50 = random_50_percent(train_malignant)\n",
    "us_train_normal_50 = random_50_percent(train_normal)\n",
    "us_val_benign_50 = random_50_percent(val_benign)\n",
    "us_val_malignant_50 = random_50_percent(val_malignant)\n",
    "us_val_normal_50 = random_50_percent(val_normal)\n",
    "\n",
    "# Mammo\n",
    "mm_train_benign_50 = random_50_percent(mm_train_benign)\n",
    "mm_train_malignant_50 = random_50_percent(mm_train_malignant)\n",
    "mm_train_normal_50 = random_50_percent(mm_train_normal)\n",
    "mm_val_benign_50 = random_50_percent(mm_val_benign)\n",
    "mm_val_malignant_50 = random_50_percent(mm_val_malignant)\n",
    "mm_val_normal_50 = random_50_percent(mm_val_normal)\n",
    "\n",
    "# Finetune\n",
    "# Ultrasound\n",
    "us_train_benign_ft_50, us_train_benign_masks_ft_50 = get_half_images_and_masks(\n",
    "    train_benign_ft,\n",
    "    finetune_busi_masks[\"benign\"],\n",
    "    match_function=match_masks,\n",
    "    masks_folder=clean_busi_masks_dir\n",
    ")\n",
    "\n",
    "us_train_malignant_ft_50, us_train_malignant_masks_ft_50 = get_half_images_and_masks(\n",
    "    train_malignant_ft,\n",
    "    finetune_busi_masks[\"malignant\"],\n",
    "    match_function=match_masks,\n",
    "    masks_folder=clean_busi_masks_dir\n",
    ")\n",
    "\n",
    "us_train_normal_ft_50, us_train_normal_masks_ft_50 = get_half_images_and_masks(\n",
    "    train_normal_ft,\n",
    "    finetune_busi_masks[\"normal\"],\n",
    "    match_function=match_masks,\n",
    "    masks_folder=clean_busi_masks_dir\n",
    ")\n",
    "\n",
    "us_val_benign_ft_50, us_val_benign_masks_ft_50 = get_half_images_and_masks(\n",
    "    val_benign_ft,\n",
    "    finetune_busi_masks[\"benign\"],\n",
    "    match_function=match_masks,\n",
    "    masks_folder=clean_busi_masks_dir\n",
    ")\n",
    "\n",
    "us_val_malignant_ft_50, us_val_malignant_masks_ft_50 = get_half_images_and_masks(\n",
    "    val_malignant_ft,\n",
    "    finetune_busi_masks[\"malignant\"],\n",
    "    match_function=match_masks,\n",
    "    masks_folder=clean_busi_masks_dir\n",
    ")\n",
    "\n",
    "us_val_normal_ft_50, us_val_normal_masks_ft_50 = get_half_images_and_masks(\n",
    "    val_normal_ft,\n",
    "    finetune_busi_masks[\"normal\"],\n",
    "    match_function=match_masks,\n",
    "    masks_folder=clean_busi_masks_dir\n",
    ")\n",
    "\n",
    "us_test_benign_ft_50, us_test_benign_masks_ft_50 = get_half_images_and_masks(\n",
    "    test_benign_ft,\n",
    "    finetune_busi_masks[\"benign\"],\n",
    "    match_function=match_masks,\n",
    "    masks_folder=clean_busi_masks_dir\n",
    ")\n",
    "\n",
    "us_test_malignant_ft_50, us_test_malignant_masks_ft_50 = get_half_images_and_masks(\n",
    "    test_malignant_ft,\n",
    "    finetune_busi_masks[\"malignant\"],\n",
    "    match_function=match_masks,\n",
    "    masks_folder=clean_busi_masks_dir\n",
    ")\n",
    "\n",
    "us_test_normal_ft_50, us_test_normal_masks_ft_50 = get_half_images_and_masks(\n",
    "    test_normal_ft,\n",
    "    finetune_busi_masks[\"normal\"],\n",
    "    match_function=match_masks,\n",
    "    masks_folder=clean_busi_masks_dir\n",
    ")\n",
    "\n",
    "# Mammography\n",
    "mm_train_benign_ft_50, mm_train_benign_masks_ft_50 = get_half_images_and_masks(\n",
    "    mm_train_benign_ft,\n",
    "    finetune_inbreast_benign_masks,\n",
    "    match_function=match_masks_mammography_inbreast\n",
    ")\n",
    "\n",
    "mm_train_normal_ft_50, mm_train_normal_masks_ft_50 = get_half_images_and_masks(\n",
    "    mm_train_normal_ft,\n",
    "    finetune_inbreast_normal_masks,\n",
    "    match_function=match_masks_mammography_inbreast\n",
    ")\n",
    "\n",
    "mm_val_benign_ft_50, mm_val_benign_masks_ft_50 = get_half_images_and_masks(\n",
    "    mm_val_benign_ft,\n",
    "    finetune_inbreast_benign_masks,\n",
    "    match_function=match_masks_mammography_inbreast\n",
    ")\n",
    "\n",
    "mm_val_normal_ft_50, mm_val_normal_masks_ft_50 = get_half_images_and_masks(\n",
    "    mm_val_normal_ft,\n",
    "    finetune_inbreast_normal_masks,\n",
    "    match_function=match_masks_mammography_inbreast\n",
    ")\n",
    "\n",
    "mm_test_benign_ft_50, mm_test_benign_masks_ft_50 = get_half_images_and_masks(\n",
    "    mm_test_benign_ft,\n",
    "    finetune_inbreast_benign_masks,\n",
    "    match_function=match_masks_mammography_inbreast\n",
    ")\n",
    "\n",
    "mm_test_normal_ft_50, mm_test_normal_masks_ft_50 = get_half_images_and_masks(\n",
    "    mm_test_normal_ft,\n",
    "    finetune_inbreast_normal_masks,\n",
    "    match_function=match_masks_mammography_inbreast\n",
    ")\n",
    "\n",
    "# Malignant case set aside\n",
    "mm_train_inbreast_malignant_ft_50_aux, mm_train_inbreast_malignant_masks_ft_50_aux = get_half_images_and_masks(\n",
    "    mm_train_inbreast_malignant_aux,\n",
    "    finetune_inbreast_malignant_masks,\n",
    "    match_function=match_masks_mammography_inbreast\n",
    ")\n",
    "\n",
    "mm_val_inbreast_malignant_ft_50_aux, mm_val_inbreast_malignant_masks_ft_50_aux = get_half_images_and_masks(\n",
    "    mm_val_inbreast_malignant_aux,\n",
    "    finetune_inbreast_malignant_masks,\n",
    "    match_function=match_masks_mammography_inbreast\n",
    ")\n",
    "\n",
    "mm_test_inbreast_malignant_ft_50_aux, mm_test_inbreast_malignant_masks_ft_50_aux = get_half_images_and_masks(\n",
    "    mm_test_inbreast_malignant_aux,\n",
    "    finetune_inbreast_malignant_masks,\n",
    "    match_function=match_masks_mammography_inbreast\n",
    ")\n",
    "\n",
    "mm_train_cbis_malignant_ft_50_aux, mm_train_cbis_malignant_masks_ft_50_aux = get_half_images_and_masks(\n",
    "    mm_train_cbis_malignant_aux,\n",
    "    finetune_cbis_malignant_masks,\n",
    "    match_function=match_masks_mammography_cbis\n",
    ")\n",
    "\n",
    "mm_val_cbis_malignant_ft_50_aux, mm_val_cbis_malignant_masks_ft_50_aux = get_half_images_and_masks(\n",
    "    mm_val_cbis_malignant_aux,\n",
    "    finetune_cbis_malignant_masks,\n",
    "    match_function=match_masks_mammography_cbis\n",
    ")\n",
    "\n",
    "mm_test_cbis_malignant_ft_50_aux, mm_test_cbis_malignant_masks_ft_50_aux = get_half_images_and_masks(\n",
    "    mm_test_cbis_malignant_aux,\n",
    "    finetune_cbis_malignant_masks,\n",
    "    match_function=match_masks_mammography_cbis\n",
    ")\n",
    "\n",
    "# Merging of modalities for pretrain data \n",
    "multimodal_train_benign = us_train_benign_50 + mm_train_benign_50\n",
    "multimodal_train_malignant = us_train_malignant_50 + mm_train_malignant_50\n",
    "multimodal_train_normal = us_train_normal_50 + mm_train_normal_50\n",
    "multimodal_val_benign = us_val_benign_50 + mm_val_benign_50\n",
    "multimodal_val_malignant = us_val_malignant_50 + mm_val_malignant_50\n",
    "multimodal_val_normal = us_val_normal_50 + mm_val_normal_50\n",
    "\n",
    "# Now for finetune\n",
    "multimodal_train_benign_ft = us_train_benign_ft_50 + mm_train_benign_ft_50\n",
    "multimodal_train_benign_masks_ft = us_train_benign_masks_ft_50 + mm_train_benign_masks_ft_50\n",
    "multimodal_train_malignant_ft = us_train_malignant_ft_50\n",
    "multimodal_train_malignant_masks_ft = us_train_malignant_masks_ft_50\n",
    "multimodal_train_normal_ft = us_train_normal_ft_50 + mm_train_normal_ft_50\n",
    "multimodal_train_normal_masks_ft = us_train_normal_masks_ft_50 + mm_train_normal_masks_ft_50\n",
    "\n",
    "multimodal_val_benign_ft = us_val_benign_ft_50 + mm_val_benign_ft_50\n",
    "multimodal_val_benign_masks_ft = us_val_benign_masks_ft_50 + mm_val_benign_masks_ft_50\n",
    "multimodal_val_malignant_ft = us_val_malignant_ft_50 + mm_val_cbis_malignant_ft_50_aux\n",
    "multimodal_val_malignant_masks_ft = us_val_malignant_masks_ft_50\n",
    "multimodal_val_normal_ft = us_val_normal_ft_50 + mm_val_normal_ft_50\n",
    "multimodal_val_normal_masks_ft = mm_val_normal_masks_ft_50 + mm_val_normal_masks_ft_50\n",
    "\n",
    "multimodal_test_benign_ft = us_test_benign_ft_50 + mm_test_benign_ft_50\n",
    "multimodal_test_benign_masks_ft = mm_test_benign_masks_ft_50 + mm_test_benign_masks_ft_50\n",
    "multimodal_test_malignant_ft = us_test_malignant_ft_50\n",
    "multimodal_test_malignant_masks_ft = us_test_malignant_masks_ft_50 + mm_test_inbreast_malignant_ft_50_aux\n",
    "multimodal_test_normal_ft = us_test_normal_ft_50 + mm_test_normal_ft_50\n",
    "multimodal_test_normal_masks_ft = mm_test_normal_masks_ft_50 + mm_test_normal_masks_ft_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"us only finetune\")\n",
    "print(len(train_benign_ft), len(val_benign_ft), len(test_benign_ft))\n",
    "print(len(train_malignant_ft), len(val_malignant_ft), len(test_malignant_ft))\n",
    "print(len(train_normal_ft), len(val_normal_ft), len(test_normal_ft))\n",
    "\n",
    "print(\"mm only finetune\")\n",
    "print(len(mm_train_benign_ft), len(mm_val_benign_ft), len(mm_test_benign_ft))\n",
    "print(len(mm_train_normal_ft), len(mm_val_normal_ft), len(mm_test_normal_ft))\n",
    "print(len(mm_train_inbreast_malignant_aux), len(mm_val_inbreast_malignant_aux), len(mm_test_inbreast_malignant_aux))\n",
    "print(len(mm_train_cbis_malignant_aux), len(mm_val_cbis_malignant_aux), len(mm_test_cbis_malignant_aux))\n",
    "\n",
    "print(\"50 picked from us only finetune\")\n",
    "print(len(us_train_benign_ft_50), len(us_val_benign_ft_50), len(us_test_benign_ft_50))\n",
    "print(len(us_train_malignant_ft_50), len(us_val_malignant_ft_50), len(us_test_malignant_ft_50))\n",
    "print(len(us_train_normal_ft_50), len(us_val_normal_ft_50), len(us_test_normal_ft_50))\n",
    "\n",
    "print(\"50 picked mm only finetune\")\n",
    "print(len(mm_train_benign_ft_50), len(mm_val_benign_ft_50), len(mm_test_benign_ft_50))\n",
    "print(len(mm_train_normal_ft_50), len(mm_val_normal_ft_50), len(mm_test_normal_ft_50))\n",
    "print(len(mm_train_inbreast_malignant_ft_50_aux), len(mm_val_inbreast_malignant_ft_50_aux), len(mm_test_inbreast_malignant_ft_50_aux))\n",
    "print(len(mm_train_cbis_malignant_ft_50_aux), len(mm_val_cbis_malignant_ft_50_aux), len(mm_test_cbis_malignant_ft_50_aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print count and a sample value\n",
    "def print_info(variable, source_variable, variable_name):\n",
    "    print(f\"Count of {variable_name}: {len(variable)}\")\n",
    "    print(f\"Count of source {variable_name}: {len(source_variable)}\")\n",
    "    if len(variable) > 0:\n",
    "        print(f\"Sample value from {variable_name}: {variable[0]}\")\n",
    "    else:\n",
    "        print(f\"No values in {variable_name}\")\n",
    "\n",
    "# Assuming random_50_percent is a function that takes a list and returns 50% random samples\n",
    "# Replace train_benign, train_malignant, etc., with your actual dataset variables\n",
    "\n",
    "# For Ultrasound Data\n",
    "print_info(us_train_benign_50, train_benign, 'us_train_benign_50')\n",
    "print_info(us_train_malignant_50, train_malignant, 'us_train_malignant_50')\n",
    "print_info(us_train_normal_50, train_normal, 'us_train_normal_50')\n",
    "print_info(us_val_benign_50, val_benign, 'us_val_benign_50')\n",
    "print_info(us_val_malignant_50, val_malignant, 'us_val_malignant_50')\n",
    "print_info(us_val_normal_50, val_normal, 'us_val_normal_50')\n",
    "\n",
    "# For Mammography Data\n",
    "print_info(mm_train_benign_50, mm_train_benign, 'mm_train_benign_50')\n",
    "print_info(mm_train_malignant_50, mm_train_malignant, 'mm_train_malignant_50')\n",
    "print_info(mm_train_normal_50, mm_train_normal, 'mm_train_normal_50')\n",
    "print_info(mm_val_benign_50, mm_val_benign, 'mm_val_benign_50')\n",
    "print_info(mm_val_malignant_50, mm_val_malignant, 'mm_val_malignant_50')\n",
    "print_info(mm_val_normal_50, mm_val_normal, 'mm_val_normal_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for original datasets (ultrasound and mammography sources)\n",
    "data = {\n",
    "    'Classes': ['Benign', 'Malignant', 'Normal'],\n",
    "    'US Train Original': [513, 396, 83],\n",
    "    'US Val Original': [129, 99, 21],\n",
    "    'MM Train Original': [513, 396, 83],\n",
    "    'MM Val Original': [129, 99, 21],\n",
    "    'US Train Selected (50%)': [256, 198, 41],\n",
    "    'US Val Selected (50%)': [64, 49, 10],\n",
    "    'MM Train Selected (50%)': [256, 198, 41],\n",
    "    'MM Val Selected (50%)': [64, 49, 10],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Bar width and position adjustment\n",
    "bar_width = 0.2\n",
    "r1 = np.arange(len(df['Classes']))  # Positions for the first set of bars\n",
    "r2 = [x + bar_width for x in r1]    # Shift positions for the second set\n",
    "r3 = [x + bar_width for x in r2]    # Shift positions for the third set\n",
    "r4 = [x + bar_width for x in r3]    # Shift positions for the fourth set\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot for train data\n",
    "axs[0].bar(r1, df['US Train Original'], color='lightblue', width=bar_width, label='US Train Original')\n",
    "axs[0].bar(r2, df['US Train Selected (50%)'], color='blue', width=bar_width, label='US Train Selected (50%)')\n",
    "axs[0].bar(r3, df['MM Train Original'], color='lightgreen', width=bar_width, label='MM Train Original')\n",
    "axs[0].bar(r4, df['MM Train Selected (50%)'], color='green', width=bar_width, label='MM Train Selected (50%)')\n",
    "axs[0].set_title('Comparison of Train Data (Original vs Selected 50%)')\n",
    "\n",
    "# Plot for validation data\n",
    "axs[1].bar(r1, df['US Val Original'], color='lightblue', width=bar_width, label='US Val Original')\n",
    "axs[1].bar(r2, df['US Val Selected (50%)'], color='blue', width=bar_width, label='US Val Selected (50%)')\n",
    "axs[1].bar(r3, df['MM Val Original'], color='lightgreen', width=bar_width, label='MM Val Original')\n",
    "axs[1].bar(r4, df['MM Val Selected (50%)'], color='green', width=bar_width, label='MM Val Selected (50%)')\n",
    "axs[1].set_title('Comparison of Validation Data (Original vs Selected 50%)')\n",
    "\n",
    "# Add legends and labels\n",
    "for ax in axs:\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_xlabel('Classes')\n",
    "    ax.set_xticks([r + bar_width for r in range(len(df['Classes']))])\n",
    "    ax.set_xticklabels(df['Classes'])\n",
    "    ax.legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_finetune_data():\n",
    "    # Expected values for the number of images in each category\n",
    "    expected_counts = {\n",
    "        'benign': {'train': 121, 'val': 41, 'test': 41},\n",
    "        'malignant': {'train': 97, 'val': 33, 'test': 33},\n",
    "        'normal': {'train': 19, 'val': 7, 'test': 7}\n",
    "    }\n",
    "    \n",
    "    # Mammography malignant case: split between CBIS and INBreast\n",
    "    expected_malignant_mammo = {\n",
    "        'cbis': {'train': 37, 'val': 13, 'test': 13},\n",
    "        'inbreast': {'train': 60, 'val': 20, 'test': 20}\n",
    "    }\n",
    "\n",
    "    # Verify benign counts\n",
    "    print_info(us_train_benign_ft_50, train_benign_ft, \"Benign Train (Ultrasound)\")\n",
    "    print_info(us_val_benign_ft_50, val_benign_ft, \"Benign Validation (Ultrasound)\")\n",
    "    print_info(us_test_benign_ft_50, test_benign_ft, \"Benign Test (Ultrasound)\")\n",
    "    \n",
    "    print_info(mm_train_benign_ft_50, mm_train_benign_ft, \"Benign Train (Mammography)\")\n",
    "    print_info(mm_val_benign_ft_50, mm_val_benign_ft, \"Benign Validation (Mammography)\")\n",
    "    print_info(mm_test_benign_ft_50, mm_test_benign_ft, \"Benign Test (Mammography)\")\n",
    "\n",
    "    # Verify malignant counts (ultrasound only)\n",
    "    print_info(us_train_malignant_ft_50, train_malignant_ft, \"Malignant Train (Ultrasound)\")\n",
    "    print_info(us_val_malignant_ft_50, val_malignant_ft, \"Malignant Validation (Ultrasound)\")\n",
    "    print_info(us_test_malignant_ft_50, test_malignant_ft, \"Malignant Test (Ultrasound)\")\n",
    "\n",
    "    # Verify mammography malignant counts: CBIS and INBreast separately\n",
    "    print_info(mm_train_cbis_malignant_ft_50_aux, mm_train_cbis_malignant_aux, \"Malignant Train (CBIS - Mammography)\")\n",
    "    print_info(mm_val_cbis_malignant_ft_50_aux, mm_val_cbis_malignant_aux, \"Malignant Validation (CBIS - Mammography)\")\n",
    "    print_info(mm_test_cbis_malignant_ft_50_aux, mm_test_cbis_malignant_aux, \"Malignant Test (CBIS - Mammography)\")\n",
    "    \n",
    "    print_info(mm_train_inbreast_malignant_ft_50_aux, mm_train_inbreast_malignant_aux, \"Malignant Train (INBreast - Mammography)\")\n",
    "    print_info(mm_val_inbreast_malignant_ft_50_aux, mm_val_inbreast_malignant_aux, \"Malignant Validation (INBreast - Mammography)\")\n",
    "    print_info(mm_test_inbreast_malignant_ft_50_aux, mm_test_inbreast_malignant_aux, \"Malignant Test (INBreast - Mammography)\")\n",
    "\n",
    "    # Verify normal counts\n",
    "    print_info(us_train_normal_ft_50, train_normal_ft, \"Normal Train (Ultrasound)\")\n",
    "    print_info(us_val_normal_ft_50, val_normal_ft, \"Normal Validation (Ultrasound)\")\n",
    "    print_info(us_test_normal_ft_50, test_normal_ft, \"Normal Test (Ultrasound)\")\n",
    "    \n",
    "    print_info(mm_train_normal_ft_50, mm_train_normal_ft, \"Normal Train (Mammography)\")\n",
    "    print_info(mm_val_normal_ft_50, mm_val_normal_ft, \"Normal Validation (Mammography)\")\n",
    "    print_info(mm_test_normal_ft_50, mm_test_normal_ft, \"Normal Test (Mammography)\")\n",
    "\n",
    "verify_finetune_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(paths, phaseFolder, splitFolder, classFolder, imagetypeFolder, modalityFolder):\n",
    "    if phaseFolder == 'pretrainData':\n",
    "        destination_dir = os.path.join(multimodal_dir, phaseFolder, splitFolder, modalityFolder)\n",
    "    else:\n",
    "        destination_dir = os.path.join(multimodal_dir, phaseFolder, splitFolder, classFolder, imagetypeFolder, modalityFolder)\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    \n",
    "    c=0\n",
    "    for file_path in paths:\n",
    "        shutil.copy(file_path, destination_dir)\n",
    "        c+=1\n",
    "    print(\"we did\", c, \"copies for folder:\", destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultrasound pretrain\n",
    "copy_files(us_train_benign_50, 'pretrainData', 'train', 'N/A', 'N/A', 'ultrasoundImages')\n",
    "copy_files(us_train_malignant_50, 'pretrainData', 'train', 'N/A', 'N/A', 'ultrasoundImages')\n",
    "copy_files(us_train_normal_50, 'pretrainData', 'train', 'N/A', 'N/A', 'ultrasoundImages')\n",
    "\n",
    "copy_files(us_val_benign_50, 'pretrainData', 'validation', 'N/A', 'N/A', 'ultrasoundImages')\n",
    "copy_files(us_val_malignant_50, 'pretrainData', 'validation', 'N/A', 'N/A', 'ultrasoundImages')\n",
    "copy_files(us_val_normal_50, 'pretrainData', 'validation', 'N/A', 'N/A', 'ultrasoundImages')\n",
    "\n",
    "# Mammography pretrain\n",
    "copy_files(mm_train_benign_50, 'pretrainData', 'train', 'N/A', 'N/A', 'mammographyImages')\n",
    "copy_files(mm_train_malignant_50, 'pretrainData', 'train', 'N/A', 'N/A', 'mammographyImages')\n",
    "copy_files(mm_train_normal_50, 'pretrainData', 'train', 'N/A', 'N/A', 'mammographyImages')\n",
    "\n",
    "copy_files(mm_val_benign_50, 'pretrainData', 'validation', 'N/A', 'N/A', 'mammographyImages')\n",
    "copy_files(mm_val_malignant_50, 'pretrainData', 'validation', 'N/A', 'N/A', 'mammographyImages')\n",
    "copy_files(mm_val_normal_50, 'pretrainData', 'validation', 'N/A', 'N/A', 'mammographyImages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune ultrasound benign\n",
    "copy_files(us_train_benign_ft_50, 'finetuneData', 'train', 'benign', 'images', 'ultrasoundImages')\n",
    "copy_files(us_train_benign_masks_ft_50, 'finetuneData', 'train', 'benign', 'masks', 'UltrasoundMasks')\n",
    "\n",
    "copy_files(us_val_benign_ft_50, 'finetuneData', 'validation', 'benign', 'images', 'ultrasoundImages')\n",
    "copy_files(us_val_benign_masks_ft_50, 'finetuneData', 'validation', 'benign', 'masks', 'UltrasoundMasks')\n",
    "\n",
    "copy_files(us_test_benign_ft_50, 'finetuneData', 'test', 'benign', 'images', 'ultrasoundImages')\n",
    "copy_files(us_test_benign_masks_ft_50, 'finetuneData', 'test', 'benign', 'masks', 'UltrasoundMasks')\n",
    "\n",
    "# Finetune ultrasound malignant\n",
    "copy_files(us_train_malignant_ft_50, 'finetuneData', 'train', 'malignant', 'images', 'ultrasoundImages')\n",
    "copy_files(us_train_malignant_masks_ft_50, 'finetuneData', 'train', 'malignant', 'masks', 'UltrasoundMasks')\n",
    "\n",
    "copy_files(us_val_malignant_ft_50, 'finetuneData', 'validation', 'malignant', 'images', 'ultrasoundImages')\n",
    "copy_files(us_val_malignant_masks_ft_50, 'finetuneData', 'validation', 'malignant', 'masks', 'UltrasoundMasks')\n",
    "\n",
    "copy_files(us_test_malignant_ft_50, 'finetuneData', 'test', 'malignant', 'images', 'ultrasoundImages')\n",
    "copy_files(us_test_malignant_masks_ft_50, 'finetuneData', 'test', 'malignant', 'masks', 'UltrasoundMasks')\n",
    "\n",
    "# Finetune ultrasound normal\n",
    "copy_files(us_train_normal_ft_50, 'finetuneData', 'train', 'normal', 'images', 'ultrasoundImages')\n",
    "copy_files(us_train_normal_masks_ft_50, 'finetuneData', 'train', 'normal', 'masks', 'UltrasoundMasks')\n",
    "\n",
    "copy_files(us_val_normal_ft_50, 'finetuneData', 'validation', 'normal', 'images', 'ultrasoundImages')\n",
    "copy_files(us_val_normal_masks_ft_50, 'finetuneData', 'validation', 'normal', 'masks', 'UltrasoundMasks')\n",
    "\n",
    "copy_files(us_test_normal_ft_50, 'finetuneData', 'test', 'normal', 'images', 'ultrasoundImages')\n",
    "copy_files(us_test_normal_masks_ft_50, 'finetuneData', 'test', 'normal', 'masks', 'UltrasoundMasks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CBIS-DDSM and INBreast malignant data\n",
    "combined_train_malignant_ft_50 = mm_train_cbis_malignant_ft_50_aux + mm_train_inbreast_malignant_ft_50_aux\n",
    "combined_train_malignant_masks_ft_50 = mm_train_cbis_malignant_masks_ft_50_aux  + mm_train_inbreast_malignant_masks_ft_50_aux\n",
    "\n",
    "combined_val_malignant_ft_50 = mm_val_cbis_malignant_ft_50_aux + mm_val_inbreast_malignant_ft_50_aux\n",
    "combined_val_malignant_masks_ft_50 = mm_val_cbis_malignant_masks_ft_50_aux + mm_val_inbreast_malignant_masks_ft_50_aux\n",
    "\n",
    "combined_test_malignant_ft_50 = mm_test_cbis_malignant_ft_50_aux + mm_test_inbreast_malignant_ft_50_aux\n",
    "combined_test_malignant_masks_ft_50 = mm_test_cbis_malignant_masks_ft_50_aux + mm_test_inbreast_malignant_masks_ft_50_aux\n",
    "\n",
    "# Call the function for combined malignant cases\n",
    "copy_files(combined_train_malignant_ft_50, 'finetuneData', 'train', 'malignant', 'images', 'mammographyImages')\n",
    "copy_files(combined_train_malignant_masks_ft_50, 'finetuneData', 'train', 'malignant', 'masks', 'MammographyMasks')\n",
    "\n",
    "copy_files(combined_val_malignant_ft_50, 'finetuneData', 'validation', 'malignant', 'images', 'mammographyImages')\n",
    "copy_files(combined_val_malignant_masks_ft_50, 'finetuneData', 'validation', 'malignant', 'masks', 'MammographyMasks')\n",
    "\n",
    "copy_files(combined_test_malignant_ft_50, 'finetuneData', 'test', 'malignant', 'images', 'mammographyImages')\n",
    "copy_files(combined_test_malignant_masks_ft_50, 'finetuneData', 'test', 'malignant', 'masks', 'MammographyMasks')\n",
    "\n",
    "# Mammography benign\n",
    "copy_files(mm_train_benign_ft_50, 'finetuneData', 'train', 'benign', 'images', 'mammographyImages')\n",
    "copy_files(mm_train_benign_masks_ft_50, 'finetuneData', 'train', 'benign', 'masks', 'MammographyMasks')\n",
    "\n",
    "copy_files(mm_val_benign_ft_50, 'finetuneData', 'validation', 'benign', 'images', 'mammographyImages')\n",
    "copy_files(mm_val_benign_masks_ft_50, 'finetuneData', 'validation', 'benign', 'masks', 'MammographyMasks')\n",
    "\n",
    "copy_files(mm_test_benign_ft_50, 'finetuneData', 'test', 'benign', 'images', 'mammographyImages')\n",
    "copy_files(mm_test_benign_masks_ft_50, 'finetuneData', 'test', 'benign', 'masks', 'MammographyMasks')\n",
    "\n",
    "# Mammography normal\n",
    "copy_files(mm_train_normal_ft_50, 'finetuneData', 'train', 'normal', 'images', 'mammographyImages')\n",
    "copy_files(mm_train_normal_masks_ft_50, 'finetuneData', 'train', 'normal', 'masks', 'MammographyMasks')\n",
    "\n",
    "copy_files(mm_val_normal_ft_50, 'finetuneData', 'validation', 'normal', 'images', 'mammographyImages')\n",
    "copy_files(mm_val_normal_masks_ft_50, 'finetuneData', 'validation', 'normal', 'masks', 'MammographyMasks')\n",
    "\n",
    "copy_files(mm_test_normal_ft_50, 'finetuneData', 'test', 'normal', 'images', 'mammographyImages')\n",
    "copy_files(mm_test_normal_masks_ft_50, 'finetuneData', 'test', 'normal', 'masks', 'MammographyMasks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing datasets for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Setting classes and variables for all datapoints across datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_used_dir = os.path.join(data_dir, \"dataUsed\")\n",
    "classes = [\"benign\", \"malignant\", \"normal\"]\n",
    "classes_numbercoded = {'benign': 0, 'malignant': 1, 'normal': 2}\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mammography pretraining data\n",
    "mg_pt_train_im_dir = os.path.join(data_used_dir, 'MammographyDataset', 'pretrainData', 'train')\n",
    "mg_pt_val_im_dir = os.path.join(data_used_dir, 'MammographyDataset', 'pretrainData', 'validation')\n",
    "\n",
    "# Mammography finetuning data\n",
    "mg_ft_train_b_im_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'train', 'benign', 'images')\n",
    "mg_ft_train_b_msk_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'train', 'benign', 'masks')\n",
    "mg_ft_train_m_im_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'train', 'malignant', 'images')\n",
    "mg_ft_train_m_msk_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'train', 'malignant', 'masks')\n",
    "mg_ft_train_n_im_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'train', 'normal', 'images')\n",
    "mg_ft_train_n_msk_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'train', 'normal', 'masks')\n",
    "\n",
    "mg_ft_val_b_im_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'validation', 'benign', 'images')\n",
    "mg_ft_val_b_msk_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'validation', 'benign', 'masks')\n",
    "mg_ft_val_m_im_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'validation', 'malignant', 'images')\n",
    "mg_ft_val_m_msk_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'validation', 'malignant', 'masks')\n",
    "mg_ft_val_n_im_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'validation', 'normal', 'images')\n",
    "mg_ft_val_n_msk_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'validation', 'normal', 'masks')\n",
    "\n",
    "mg_ft_test_b_im_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'test', 'benign', 'images')\n",
    "mg_ft_test_b_msk_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'test', 'benign', 'masks')\n",
    "mg_ft_test_m_im_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'test', 'malignant', 'images')\n",
    "mg_ft_test_m_msk_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'test', 'malignant', 'masks')\n",
    "mg_ft_test_n_im_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'test', 'normal', 'images')\n",
    "mg_ft_test_n_msk_dir = os.path.join(data_used_dir, 'MammographyDataset', 'finetuneData', 'test', 'normal', 'masks')\n",
    "\n",
    "# Ultrasound pretraining data\n",
    "us_pt_train_im_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'pretrainData', 'train')\n",
    "us_pt_val_im_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'pretrainData', 'validation')\n",
    "\n",
    "# Ultrasound finetuning data\n",
    "us_ft_train_b_im_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'train', 'benign', 'images')\n",
    "us_ft_train_b_msk_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'train', 'benign', 'masks')\n",
    "us_ft_train_m_im_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'train', 'malignant', 'images')\n",
    "us_ft_train_m_msk_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'train', 'malignant', 'masks')\n",
    "us_ft_train_n_im_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'train', 'normal', 'images')\n",
    "us_ft_train_n_msk_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'train', 'normal', 'masks')\n",
    "\n",
    "us_ft_val_b_im_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'validation', 'benign', 'images')\n",
    "us_ft_val_b_msk_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'validation', 'benign', 'masks')\n",
    "us_ft_val_m_im_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'validation', 'malignant', 'images')\n",
    "us_ft_val_m_msk_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'validation', 'malignant', 'masks')\n",
    "us_ft_val_n_im_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'validation', 'normal', 'images')\n",
    "us_ft_val_n_msk_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'validation', 'normal', 'masks')\n",
    "\n",
    "us_ft_test_b_im_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'test', 'benign', 'images')\n",
    "us_ft_test_b_msk_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'test', 'benign', 'masks')\n",
    "us_ft_test_m_im_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'test', 'malignant', 'images')\n",
    "us_ft_test_m_msk_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'test', 'malignant', 'masks')\n",
    "us_ft_test_n_im_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'test', 'normal', 'images')\n",
    "us_ft_test_n_msk_dir = os.path.join(data_used_dir, 'UltrasoundDataset', 'finetuneData', 'test', 'normal', 'masks')\n",
    "\n",
    "# Multimodal pretraining data (Ultrasound part)\n",
    "multi_pt_train_im_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'pretrainData', 'train', 'ultrasoundImages')\n",
    "multi_pt_val_im_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'pretrainData', 'validation', 'ultrasoundImages')\n",
    "\n",
    "# Multimodal pretraining data (Mammography part)\n",
    "multi_pt_train_im_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'pretrainData', 'train', 'mammographyImages')\n",
    "multi_pt_val_im_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'pretrainData', 'validation', 'mammographyImages')\n",
    "\n",
    "# Train phase for Multimodal finetuning data (Ultrasound part)\n",
    "multi_ft_train_b_im_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'benign', 'images', 'ultrasoundImages')\n",
    "multi_ft_train_b_msk_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'benign', 'masks', 'UltrasoundMasks')\n",
    "multi_ft_train_m_im_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'malignant', 'images', 'ultrasoundImages')\n",
    "multi_ft_train_m_msk_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'malignant', 'masks', 'UltrasoundMasks')\n",
    "multi_ft_train_n_im_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'normal', 'images', 'ultrasoundImages')\n",
    "multi_ft_train_n_msk_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'normal', 'masks', 'UltrasoundMasks')\n",
    "\n",
    "# Train phase for Multimodal finetuning data (Mammography part)\n",
    "multi_ft_train_b_im_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'benign', 'images', 'mammographyImages')\n",
    "multi_ft_train_b_msk_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'benign', 'masks', 'MammographyMasks')\n",
    "multi_ft_train_m_im_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'malignant', 'images', 'mammographyImages')\n",
    "multi_ft_train_m_msk_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'malignant', 'masks', 'MammographyMasks')\n",
    "multi_ft_train_n_im_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'normal', 'images', 'mammographyImages')\n",
    "multi_ft_train_n_msk_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'train', 'normal', 'masks', 'MammographyMasks')\n",
    "\n",
    "# Validation phase for Multimodal finetuning (Ultrasound part)\n",
    "multi_ft_val_b_im_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'benign', 'images', 'ultrasoundImages')\n",
    "multi_ft_val_b_msk_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'benign', 'masks', 'UltrasoundMasks')\n",
    "multi_ft_val_m_im_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'malignant', 'images', 'ultrasoundImages')\n",
    "multi_ft_val_m_msk_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'malignant', 'masks', 'UltrasoundMasks')\n",
    "multi_ft_val_n_im_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'normal', 'images', 'ultrasoundImages')\n",
    "multi_ft_val_n_msk_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'normal', 'masks', 'UltrasoundMasks')\n",
    "\n",
    "# Validation phase for Multimodal finetuning (Mammography part)\n",
    "multi_ft_val_b_im_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'benign', 'images', 'mammographyImages')\n",
    "multi_ft_val_b_msk_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'benign', 'masks', 'MammographyMasks')\n",
    "multi_ft_val_m_im_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'malignant', 'images', 'mammographyImages')\n",
    "multi_ft_val_m_msk_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'malignant', 'masks', 'MammographyMasks')\n",
    "multi_ft_val_n_im_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'normal', 'images', 'mammographyImages')\n",
    "multi_ft_val_n_msk_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'validation', 'normal', 'masks', 'MammographyMasks')\n",
    "\n",
    "# Test phase for Multimodal finetuning (Ultrasound)\n",
    "multi_ft_test_b_im_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'benign', 'images', 'ultrasoundImages')\n",
    "multi_ft_test_b_msk_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'benign', 'masks', 'UltrasoundMasks')\n",
    "multi_ft_test_m_im_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'malignant', 'images', 'ultrasoundImages')\n",
    "multi_ft_test_m_msk_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'malignant', 'masks', 'UltrasoundMasks')\n",
    "multi_ft_test_n_im_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'normal', 'images', 'ultrasoundImages')\n",
    "multi_ft_test_n_msk_usdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'normal', 'masks', 'UltrasoundMasks')\n",
    "\n",
    "# Test phase for Multimodal finetuning (Mammography)\n",
    "multi_ft_test_b_im_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'benign', 'images', 'mammographyImages')\n",
    "multi_ft_test_b_msk_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'benign', 'masks', 'MammographyMasks')\n",
    "multi_ft_test_m_im_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'malignant', 'images', 'mammographyImages')\n",
    "multi_ft_test_m_msk_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'malignant', 'masks', 'MammographyMasks')\n",
    "multi_ft_test_n_im_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'normal', 'images', 'mammographyImages')\n",
    "multi_ft_test_n_msk_mgdata_dir = os.path.join(data_used_dir, 'MultimodalDataset', 'finetuneData', 'test', 'normal', 'masks', 'MammographyMasks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_in_dir(directory):\n",
    "    return len([file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))])\n",
    "\n",
    "counts = {\n",
    "    \"MammographyDataset\": 0,\n",
    "    \"UltrasoundDataset\": 0,\n",
    "    \"MultimodalDataset\": 0\n",
    "}\n",
    "\n",
    "directories = [\n",
    "    # Mammography dataset\n",
    "    mg_pt_train_im_dir, mg_pt_val_im_dir,\n",
    "    mg_ft_train_b_im_dir, mg_ft_train_b_msk_dir, mg_ft_train_m_im_dir, mg_ft_train_m_msk_dir, mg_ft_train_n_im_dir, mg_ft_train_n_msk_dir,\n",
    "    mg_ft_val_b_im_dir, mg_ft_val_b_msk_dir, mg_ft_val_m_im_dir, mg_ft_val_m_msk_dir, mg_ft_val_n_im_dir, mg_ft_val_n_msk_dir,\n",
    "    mg_ft_test_b_im_dir, mg_ft_test_b_msk_dir, mg_ft_test_m_im_dir, mg_ft_test_m_msk_dir, mg_ft_test_n_im_dir, mg_ft_test_n_msk_dir,\n",
    "\n",
    "    # Ultrasound dataset\n",
    "    us_pt_train_im_dir, us_pt_val_im_dir,\n",
    "    us_ft_train_b_im_dir, us_ft_train_b_msk_dir, us_ft_train_m_im_dir, us_ft_train_m_msk_dir, us_ft_train_n_im_dir, us_ft_train_n_msk_dir,\n",
    "    us_ft_val_b_im_dir, us_ft_val_b_msk_dir, us_ft_val_m_im_dir, us_ft_val_m_msk_dir, us_ft_val_n_im_dir, us_ft_val_n_msk_dir,\n",
    "    us_ft_test_b_im_dir, us_ft_test_b_msk_dir, us_ft_test_m_im_dir, us_ft_test_m_msk_dir, us_ft_test_n_im_dir, us_ft_test_n_msk_dir,\n",
    "\n",
    "    # Multimodal dataset\n",
    "    multi_pt_train_im_usdata_dir, multi_pt_val_im_usdata_dir, multi_pt_train_im_mgdata_dir, multi_pt_val_im_mgdata_dir,\n",
    "    multi_ft_train_b_im_usdata_dir, multi_ft_train_b_msk_usdata_dir, multi_ft_train_b_im_mgdata_dir, multi_ft_train_b_msk_mgdata_dir,\n",
    "    multi_ft_train_m_im_usdata_dir, multi_ft_train_m_msk_usdata_dir, multi_ft_train_m_im_mgdata_dir, multi_ft_train_m_msk_mgdata_dir,\n",
    "    multi_ft_train_n_im_usdata_dir, multi_ft_train_n_msk_usdata_dir, multi_ft_train_n_im_mgdata_dir, multi_ft_train_n_msk_mgdata_dir,\n",
    "    multi_ft_val_b_im_usdata_dir, multi_ft_val_b_msk_usdata_dir, multi_ft_val_b_im_mgdata_dir, multi_ft_val_b_msk_mgdata_dir,\n",
    "    multi_ft_val_m_im_usdata_dir, multi_ft_val_m_msk_usdata_dir, multi_ft_val_m_im_mgdata_dir, multi_ft_val_m_msk_mgdata_dir,\n",
    "    multi_ft_val_n_im_usdata_dir, multi_ft_val_n_msk_usdata_dir, multi_ft_val_n_im_mgdata_dir, multi_ft_val_n_msk_mgdata_dir,\n",
    "    multi_ft_test_b_im_usdata_dir, multi_ft_test_b_msk_usdata_dir, multi_ft_test_b_im_mgdata_dir, multi_ft_test_b_msk_mgdata_dir,\n",
    "    multi_ft_test_m_im_usdata_dir, multi_ft_test_m_msk_usdata_dir, multi_ft_test_m_im_mgdata_dir, multi_ft_test_m_msk_mgdata_dir,\n",
    "    multi_ft_test_n_im_usdata_dir, multi_ft_test_n_msk_usdata_dir, multi_ft_test_n_im_mgdata_dir, multi_ft_test_n_msk_mgdata_dir,\n",
    "]\n",
    "\n",
    "def count_files(directories):\n",
    "    counts = {}\n",
    "    dataset_sums = {'mammography': 0, 'ultrasound': 0, 'multimodal': 0}\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        if os.path.exists(dir_path):\n",
    "            num_files = len(os.listdir(dir_path))\n",
    "            counts[dir_path] = num_files\n",
    "            \n",
    "            # Determine dataset type and add to corresponding sum\n",
    "            if 'mammography' in dir_path:\n",
    "                dataset_sums['mammography'] += num_files\n",
    "            elif 'ultrasound' in dir_path:\n",
    "                dataset_sums['ultrasound'] += num_files\n",
    "            elif 'Multimodal' in dir_path:\n",
    "                dataset_sums['multimodal'] += num_files\n",
    "                \n",
    "    return counts, dataset_sums\n",
    "\n",
    "file_counts, dataset_totals = count_files(directories)\n",
    "file_counts, dataset_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View example image and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_images = [f for f in os.listdir(us_ft_train_b_im_dir) if f.endswith('.png')]\n",
    "if benign_images:\n",
    "    # Show first benign image\n",
    "    benign_images.sort()\n",
    "    \n",
    "    benign_image_name = benign_images[0]\n",
    "    benign_image_path = os.path.join(us_ft_train_b_im_dir, benign_image_name)\n",
    "\n",
    "    benign_image = cv2.imread(benign_image_path)\n",
    "    benign_image = cv2.cvtColor(benign_image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    plt.imshow(benign_image)\n",
    "    plt.title(f'Benign Image: {benign_image_name}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    mask_image_name = benign_image_name.replace('.png', '_mask.png')\n",
    "    mask_image_path = os.path.join(us_ft_train_b_msk_dir, mask_image_name)\n",
    "\n",
    "    if os.path.exists(mask_image_path):\n",
    "        mask_image = cv2.imread(mask_image_path)\n",
    "        mask_image = cv2.cvtColor(mask_image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        plt.imshow(mask_image)\n",
    "        plt.title(f'Mask Image: {mask_image_name}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f'Mask image not found: {mask_image_path}')\n",
    "else:\n",
    "    print('No benign images found in the specified directory.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Setting augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTransformations(object):\n",
    "    def __init__(\n",
    "        self, base_transforms, n_views=2):\n",
    "        self.base_transforms = base_transforms\n",
    "        self.n_views = n_views\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.base_transforms(x) for i in range(self.n_views)]\n",
    "\n",
    "class addBackgroundPadding:\n",
    "    def __call__(self, img):\n",
    "        width, height = img.size\n",
    "\n",
    "        # Calculate padding to add on each side to make the image square\n",
    "        max_side = max(width, height)\n",
    "        padding = (\n",
    "            (max_side - width) // 2,  # left\n",
    "            (max_side - height) // 2, # top\n",
    "            (max_side - width + 1) // 2,  # right\n",
    "            (max_side - height + 1) // 2  # bottom\n",
    "        )\n",
    "        # Apply padding with 0s\n",
    "        return transforms.functional.pad(img, padding, fill=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_transformsTHEV2VERSION = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Color is irrelevant\n",
    "    transforms.Resize((110, 110)),  # Increase resize to allow varied cropping\n",
    "    transforms.RandomRotation(degrees=10),  # Small rotation for minor variance that happens before resize/crop\n",
    "    transforms.RandomResizedCrop(size=64, scale=(0.5, 0.9)),  # Adjust scale for variable crop\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)\n",
    "    ], p=0.8),\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(kernel_size=9)\n",
    "    ], p=0.3),  # Adjusted probability for Gaussian blur\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "contrast_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Color is irrelevant\n",
    "    transforms.Resize((110, 110)),  # Increase resize to allow varied cropping\n",
    "    transforms.RandomResizedCrop(size=64, scale=(0.5, 0.9)),  # Adjust scale for variable crop\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)\n",
    "    ], p=0.8),\n",
    "    transforms.GaussianBlur(kernel_size=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Finetune only\n",
    "only_resize_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Color is irrelevant\n",
    "    addBackgroundPadding(), # Pad the image to square with black\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Finetune only on segmentation (masks)\n",
    "only_resize_transforms_masks = transforms.Compose([\n",
    "    addBackgroundPadding(),  # Same padding for masks\n",
    "    transforms.Resize((64, 64)),  # Resize to final target size\n",
    "    transforms.ToTensor()  # No normalization for masks\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Building data loaders for all splits of the three big datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_batch_size = 64\n",
    "finetune_batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. Pretraining dataloaders construction for both classification and segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainingDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert(\"L\") # Color is irrelevant\n",
    "\n",
    "        # Generate two augmented views of the same image\n",
    "        view1, view2 = self._get_augmented_image(image)\n",
    "        return view1, view2\n",
    "\n",
    "    def _get_augmented_image(self, image):\n",
    "        if self.transform:\n",
    "            # Use the transform to create two views\n",
    "            views = self.transform(image)  # List of augmented images\n",
    "            return views[0], views[1]  # Return the first two views\n",
    "        return image, image  # Return original if no transform provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain unlabeled\n",
    "us_pretrain_train_set = PretrainingDataset(\n",
    "    image_dir = us_pt_train_im_dir,\n",
    "    transform = ContrastiveTransformations(contrast_transforms, n_views=2)\n",
    ")\n",
    "us_pretrain_validation_set = PretrainingDataset(\n",
    "    image_dir = us_pt_val_im_dir,\n",
    "    transform = ContrastiveTransformations(contrast_transforms, n_views=2)\n",
    ")\n",
    "\n",
    "mg_pretrain_train_set = PretrainingDataset(\n",
    "    image_dir = mg_pt_train_im_dir,\n",
    "    transform = ContrastiveTransformations(contrast_transforms, n_views=2)\n",
    ")\n",
    "mg_pretrain_validation_set = PretrainingDataset(\n",
    "    image_dir = mg_pt_val_im_dir,\n",
    "    transform = ContrastiveTransformations(contrast_transforms, n_views=2)\n",
    ")\n",
    "\n",
    "multi_pretrain_train_usdata = PretrainingDataset(\n",
    "    image_dir = multi_pt_train_im_usdata_dir,\n",
    "    transform = ContrastiveTransformations(contrast_transforms, n_views=2)\n",
    ")\n",
    "multi_pretrain_train_mgdata = PretrainingDataset(\n",
    "    image_dir = multi_pt_train_im_mgdata_dir,\n",
    "    transform = ContrastiveTransformations(contrast_transforms, n_views=2)\n",
    ")\n",
    "multi_pretrain_validation_usdata = PretrainingDataset(\n",
    "    image_dir = multi_pt_val_im_usdata_dir,\n",
    "    transform = ContrastiveTransformations(contrast_transforms, n_views=2)\n",
    ")\n",
    "multi_pretrain_validation_mgdata = PretrainingDataset(\n",
    "    image_dir = multi_pt_val_im_mgdata_dir,\n",
    "    transform = ContrastiveTransformations(contrast_transforms, n_views=2)\n",
    ")\n",
    "\n",
    "# Concatenate the ultrasound and mammography pretrain datasets\n",
    "multi_pretrain_train_set = torch.utils.data.ConcatDataset([multi_pretrain_train_usdata, multi_pretrain_train_mgdata])\n",
    "\n",
    "# Concatenate the ultrasound and mammography pretrain datasets\n",
    "multi_pretrain_validation_set = torch.utils.data.ConcatDataset([multi_pretrain_validation_usdata, multi_pretrain_validation_mgdata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_pretrain_train_dataloader = torch.utils.data.DataLoader(\n",
    "    us_pretrain_train_set,\n",
    "    batch_size = pretrain_batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = False,\n",
    "    num_workers = 8\n",
    ")\n",
    "\n",
    "us_pretrain_val_dataloader = torch.utils.data.DataLoader(\n",
    "    us_pretrain_validation_set,\n",
    "    batch_size = pretrain_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    num_workers = 8\n",
    ")\n",
    "\n",
    "mg_pretrain_train_dataloader = torch.utils.data.DataLoader(\n",
    "    mg_pretrain_train_set,\n",
    "    batch_size = pretrain_batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = False, \n",
    "    num_workers = 8\n",
    ")\n",
    "\n",
    "mg_pretrain_val_dataloader = torch.utils.data.DataLoader(\n",
    "    mg_pretrain_validation_set,\n",
    "    batch_size = pretrain_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    num_workers = 8\n",
    ")\n",
    "\n",
    "multi_pretrain_train_dataloader = torch.utils.data.DataLoader(\n",
    "    multi_pretrain_train_set,\n",
    "    batch_size = pretrain_batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = False,\n",
    "    num_workers = 8\n",
    ")\n",
    "\n",
    "multi_pretrain_val_dataloader = torch.utils.data.DataLoader(\n",
    "    multi_pretrain_validation_set,\n",
    "    batch_size = pretrain_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    num_workers = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Finetune dataloaders construction for classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2.1. Setting finetune classification dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuningClassifDataset(Dataset):\n",
    "  def __init__(self, image_dir, label, transform = None):\n",
    "    self.image_dir = image_dir\n",
    "    self.transform = transform\n",
    "    self.images = os.listdir(image_dir)\n",
    "    self.label = label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "    image = Image.open(img_path).convert(\"L\")\n",
    "\n",
    "    if self.transform is not None:\n",
    "      image = self.transform(image)\n",
    "\n",
    "    label = torch.tensor(classes_numbercoded[self.label], dtype=torch.long)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2.2. Ultrasound finetune dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune labeled\n",
    "classif_us_finetune_train_benign = FinetuningClassifDataset(\n",
    "    image_dir = us_ft_train_b_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "classif_us_finetune_train_malignant = FinetuningClassifDataset(\n",
    "    image_dir = us_ft_train_m_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "classif_us_finetune_train_normal = FinetuningClassifDataset(\n",
    "    image_dir = us_ft_train_n_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "classif_us_finetune_train_set = torch.utils.data.ConcatDataset([classif_us_finetune_train_benign, classif_us_finetune_train_malignant, classif_us_finetune_train_normal])\n",
    "\n",
    "classif_us_finetune_val_benign = FinetuningClassifDataset(\n",
    "    image_dir = us_ft_val_b_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "classif_us_finetune_val_malignant = FinetuningClassifDataset(\n",
    "    image_dir = us_ft_val_m_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "classif_us_finetune_val_normal = FinetuningClassifDataset(\n",
    "    image_dir = us_ft_val_n_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "classif_us_finetune_validation_set = torch.utils.data.ConcatDataset([classif_us_finetune_val_benign, classif_us_finetune_val_malignant, classif_us_finetune_val_normal])\n",
    "\n",
    "classif_us_finetune_test_benign = FinetuningClassifDataset(\n",
    "    image_dir = us_ft_test_b_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "classif_us_finetune_test_malignant = FinetuningClassifDataset(\n",
    "    image_dir = us_ft_test_m_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "classif_us_finetune_test_normal = FinetuningClassifDataset(\n",
    "    image_dir = us_ft_test_n_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "classif_us_finetune_test_set = torch.utils.data.ConcatDataset([classif_us_finetune_test_benign, classif_us_finetune_test_malignant, classif_us_finetune_test_normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "classif_us_finetune_train_dataloader = torch.utils.data.DataLoader(\n",
    "    classif_us_finetune_train_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "classif_us_finetune_val_dataloader = torch.utils.data.DataLoader(\n",
    "    classif_us_finetune_validation_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "classif_us_finetune_test_dataloader = torch.utils.data.DataLoader(\n",
    "    classif_us_finetune_test_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2.3. Mammography finetune dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune labeled\n",
    "classif_mg_finetune_train_benign = FinetuningClassifDataset(\n",
    "    image_dir = mg_ft_train_b_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "classif_mg_finetune_train_malignant = FinetuningClassifDataset(\n",
    "    image_dir = mg_ft_train_m_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "classif_mg_finetune_train_normal = FinetuningClassifDataset(\n",
    "    image_dir = mg_ft_train_n_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "classif_mg_finetune_train_set = torch.utils.data.ConcatDataset([classif_mg_finetune_train_benign, classif_mg_finetune_train_malignant, classif_mg_finetune_train_normal])\n",
    "\n",
    "classif_mg_finetune_val_benign = FinetuningClassifDataset(\n",
    "    image_dir = mg_ft_val_b_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "classif_mg_finetune_val_malignant = FinetuningClassifDataset(\n",
    "    image_dir = mg_ft_val_m_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "classif_mg_finetune_val_normal = FinetuningClassifDataset(\n",
    "    image_dir = mg_ft_val_n_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "classif_mg_finetune_validation_set = torch.utils.data.ConcatDataset([classif_mg_finetune_val_benign, classif_mg_finetune_val_malignant, classif_mg_finetune_val_normal])\n",
    "\n",
    "classif_mg_finetune_test_benign = FinetuningClassifDataset(\n",
    "    image_dir = mg_ft_test_b_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "classif_mg_finetune_test_malignant = FinetuningClassifDataset(\n",
    "    image_dir = mg_ft_test_m_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "classif_mg_finetune_test_normal = FinetuningClassifDataset(\n",
    "    image_dir = mg_ft_test_n_im_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "classif_mg_finetune_test_set = torch.utils.data.ConcatDataset([classif_mg_finetune_test_benign, classif_mg_finetune_test_malignant, classif_mg_finetune_test_normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "classif_mg_finetune_train_dataloader = torch.utils.data.DataLoader(\n",
    "    classif_mg_finetune_train_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "classif_mg_finetune_val_dataloader = torch.utils.data.DataLoader(\n",
    "    classif_mg_finetune_validation_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "classif_mg_finetune_test_dataloader = torch.utils.data.DataLoader(\n",
    "    classif_mg_finetune_test_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2.4. Multimodal finetune dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal Finetune - Train\n",
    "classif_multi_finetune_train_benign_usdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_train_b_im_usdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_train_benign_mgdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_train_b_im_mgdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_train_malignant_usdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_train_m_im_usdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_train_malignant_mgdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_train_m_im_mgdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_train_normal_usdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_train_n_im_usdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_train_normal_mgdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_train_n_im_mgdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "\n",
    "# Final Multimodal Finetune Training Set\n",
    "classif_multi_finetune_train_set = torch.utils.data.ConcatDataset([classif_multi_finetune_train_benign_usdata, classif_multi_finetune_train_benign_mgdata, classif_multi_finetune_train_malignant_usdata, classif_multi_finetune_train_malignant_mgdata, classif_multi_finetune_train_normal_usdata, classif_multi_finetune_train_normal_mgdata])\n",
    "\n",
    "# Multimodal Finetune - Validation\n",
    "classif_multi_finetune_val_benign_usdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_val_b_im_usdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_val_benign_mgdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_val_b_im_mgdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_val_malignant_usdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_val_m_im_usdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_val_malignant_mgdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_val_m_im_mgdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_val_normal_usdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_val_n_im_usdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_val_normal_mgdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_val_n_im_mgdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "\n",
    "# Final Multimodal Finetune Validation Set\n",
    "classif_multi_finetune_val_set = torch.utils.data.ConcatDataset([classif_multi_finetune_val_benign_usdata, classif_multi_finetune_val_benign_mgdata, classif_multi_finetune_val_malignant_usdata, classif_multi_finetune_val_malignant_mgdata, classif_multi_finetune_val_normal_usdata, classif_multi_finetune_val_normal_mgdata])\n",
    "\n",
    "# Multimodal Finetune - Test\n",
    "classif_multi_finetune_test_benign_usdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_test_b_im_usdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_test_benign_mgdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_test_b_im_mgdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"benign\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_test_malignant_usdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_test_m_im_usdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_test_malignant_mgdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_test_m_im_mgdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"malignant\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_test_normal_usdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_test_n_im_usdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "\n",
    "classif_multi_finetune_test_normal_mgdata = FinetuningClassifDataset(\n",
    "    image_dir = multi_ft_test_n_im_mgdata_dir,\n",
    "    transform = only_resize_transforms,\n",
    "    label = \"normal\"\n",
    ")\n",
    "\n",
    "# Final Multimodal Finetune Training Set\n",
    "classif_multi_finetune_test_set = torch.utils.data.ConcatDataset([classif_multi_finetune_test_benign_usdata, classif_multi_finetune_test_benign_mgdata, classif_multi_finetune_test_malignant_usdata, classif_multi_finetune_test_malignant_mgdata, classif_multi_finetune_test_normal_usdata, classif_multi_finetune_test_normal_mgdata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "classif_multi_finetune_train_dataloader = torch.utils.data.DataLoader(\n",
    "    classif_multi_finetune_train_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "classif_multi_finetune_val_dataloader = torch.utils.data.DataLoader(\n",
    "    classif_multi_finetune_val_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "classif_multi_finetune_test_dataloader = torch.utils.data.DataLoader(\n",
    "    classif_multi_finetune_test_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3. Finetune dataloaders construction for segmentation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.3.1. Setting finetune segmentation dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuningSegmDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, modality, transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.modality = modality\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "        # Sorted list of image filenames\n",
    "        self.image_filenames = sorted(os.listdir(image_dir))\n",
    "\n",
    "        # Create corresponding list of mask filenames\n",
    "        self.mask_filenames = [self._generate_mask_filename(filename) for filename in self.image_filenames]\n",
    "\n",
    "    def _generate_mask_filename(self, filename):\n",
    "        mask_name = None\n",
    "        if self.modality == \"mammo\":\n",
    "            if \"ANON\" in filename:\n",
    "                # Mammography ANON format\n",
    "                parts = filename.split('_')\n",
    "                id_token = parts[0]\n",
    "                class_token = parts[-1].split('.')[0]\n",
    "                mask_name = f\"{id_token}_{class_token}_syntheticMask.png\" if class_token == \"normal\" else f\"{id_token}_{class_token}_mask.png\"\n",
    "\n",
    "                #Catch all benign cases and the malignant cases that dont use \"id\" in the filename\n",
    "            elif \"_id\" in filename:\n",
    "                # Malignant mammography format with \"id\" in filename\n",
    "                id_token = filename.split('_')[1].replace('id', '')  # Extract part after 'id' and remove 'id'\n",
    "\n",
    "                # Search for the corresponding mask in the mask directory\n",
    "                for mask_filename in os.listdir(self.mask_dir):\n",
    "                    if f\"_id{id_token}_\" in mask_filename:\n",
    "                        mask_name = mask_filename\n",
    "                        break\n",
    "    \n",
    "        elif self.modality == \"ultra\":\n",
    "            # Simple naming convention for ultrasound\n",
    "            if filename.endswith('.png'):\n",
    "                mask_name = filename.replace('.png', '_mask.png')\n",
    "\n",
    "        return mask_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "\n",
    "        # Load the corresponding mask\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = (mask > 0.5).float()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.3.2. Ultrasound finetune dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune labeled\n",
    "segm_us_finetune_train_benign = FinetuningSegmDataset(\n",
    "    image_dir = us_ft_train_b_im_dir,\n",
    "    mask_dir = us_ft_train_b_msk_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_us_finetune_train_malignant = FinetuningSegmDataset(\n",
    "    image_dir = us_ft_train_m_im_dir,\n",
    "    mask_dir = us_ft_train_m_msk_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_us_finetune_train_normal = FinetuningSegmDataset(\n",
    "    image_dir = us_ft_train_n_im_dir,\n",
    "    mask_dir = us_ft_train_n_msk_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_us_finetune_train_set = torch.utils.data.ConcatDataset([segm_us_finetune_train_benign, segm_us_finetune_train_malignant, segm_us_finetune_train_normal])\n",
    "\n",
    "segm_us_finetune_val_benign = FinetuningSegmDataset(\n",
    "    image_dir = us_ft_val_b_im_dir,\n",
    "    mask_dir = us_ft_val_b_msk_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_us_finetune_val_malignant = FinetuningSegmDataset(\n",
    "    image_dir = us_ft_val_m_im_dir,\n",
    "    mask_dir = us_ft_val_m_msk_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_us_finetune_val_normal = FinetuningSegmDataset(\n",
    "    image_dir = us_ft_val_n_im_dir,\n",
    "    mask_dir = us_ft_val_n_msk_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_us_finetune_validation_set = torch.utils.data.ConcatDataset([segm_us_finetune_val_benign, segm_us_finetune_val_malignant, segm_us_finetune_val_normal])\n",
    "\n",
    "segm_us_finetune_test_benign = FinetuningSegmDataset(\n",
    "    image_dir = us_ft_test_b_im_dir,\n",
    "    mask_dir = us_ft_test_b_msk_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_us_finetune_test_malignant = FinetuningSegmDataset(\n",
    "    image_dir = us_ft_test_m_im_dir,\n",
    "    mask_dir = us_ft_test_m_msk_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_us_finetune_test_normal = FinetuningSegmDataset(\n",
    "    image_dir = us_ft_test_n_im_dir,\n",
    "    mask_dir = us_ft_test_n_msk_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_us_finetune_test_set = torch.utils.data.ConcatDataset([segm_us_finetune_test_benign, segm_us_finetune_test_malignant, segm_us_finetune_test_normal])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "segm_us_finetune_train_dataloader = torch.utils.data.DataLoader(\n",
    "    segm_us_finetune_train_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = False\n",
    "\n",
    "segm_us_finetune_val_dataloader = torch.utils.data.DataLoader(\n",
    "    segm_us_finetune_validation_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "segm_us_finetune_test_dataloader = torch.utils.data.DataLoader(\n",
    "    segm_us_finetune_test_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.3.3. Mammography finetune dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune labeled\n",
    "segm_mg_finetune_train_benign = FinetuningSegmDataset(\n",
    "    image_dir = mg_ft_train_b_im_dir,\n",
    "    mask_dir = mg_ft_train_b_msk_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_mg_finetune_train_malignant = FinetuningSegmDataset(\n",
    "    image_dir = mg_ft_train_m_im_dir,\n",
    "    mask_dir = mg_ft_train_m_msk_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_mg_finetune_train_normal = FinetuningSegmDataset(\n",
    "    image_dir = mg_ft_train_n_im_dir,\n",
    "    mask_dir = mg_ft_train_n_msk_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_mg_finetune_train_set = torch.utils.data.ConcatDataset([segm_mg_finetune_train_benign, segm_mg_finetune_train_malignant, segm_mg_finetune_train_normal])\n",
    "\n",
    "segm_mg_finetune_val_benign = FinetuningSegmDataset(\n",
    "    image_dir = mg_ft_val_b_im_dir,\n",
    "    mask_dir = mg_ft_val_b_msk_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_mg_finetune_val_malignant = FinetuningSegmDataset(\n",
    "    image_dir = mg_ft_val_m_im_dir,\n",
    "    mask_dir = mg_ft_val_m_msk_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_mg_finetune_val_normal = FinetuningSegmDataset(\n",
    "    image_dir = mg_ft_val_n_im_dir,\n",
    "    mask_dir = mg_ft_val_n_msk_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_mg_finetune_validation_set = torch.utils.data.ConcatDataset([segm_mg_finetune_val_benign, segm_mg_finetune_val_malignant, segm_mg_finetune_val_normal])\n",
    "\n",
    "segm_mg_finetune_test_benign = FinetuningSegmDataset(\n",
    "    image_dir = mg_ft_test_b_im_dir,\n",
    "    mask_dir = mg_ft_test_b_msk_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_mg_finetune_test_malignant = FinetuningSegmDataset(\n",
    "    image_dir = mg_ft_test_m_im_dir,\n",
    "    mask_dir = mg_ft_test_m_msk_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_mg_finetune_test_normal = FinetuningSegmDataset(\n",
    "    image_dir = mg_ft_test_n_im_dir,\n",
    "    mask_dir = mg_ft_test_n_msk_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "segm_mg_finetune_test_set = torch.utils.data.ConcatDataset([segm_mg_finetune_test_benign, segm_mg_finetune_test_malignant, segm_mg_finetune_test_normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "segm_mg_finetune_train_dataloader = torch.utils.data.DataLoader(\n",
    "    segm_mg_finetune_train_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "segm_mg_finetune_val_dataloader = torch.utils.data.DataLoader(\n",
    "    segm_mg_finetune_validation_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "segm_mg_finetune_test_dataloader = torch.utils.data.DataLoader(\n",
    "    segm_mg_finetune_test_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.3.4. Multimodal finetune dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal Finetune - Train\n",
    "segm_multi_finetune_train_benign_usdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_train_b_im_usdata_dir,\n",
    "    mask_dir = multi_ft_train_b_msk_usdata_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_train_benign_mgdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_train_b_im_mgdata_dir,\n",
    "    mask_dir = multi_ft_train_b_msk_mgdata_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_train_malignant_usdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_train_m_im_usdata_dir,\n",
    "    mask_dir = multi_ft_train_m_msk_usdata_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_train_malignant_mgdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_train_m_im_mgdata_dir,\n",
    "    mask_dir = multi_ft_train_m_msk_mgdata_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_train_normal_usdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_train_n_im_usdata_dir,\n",
    "    mask_dir = multi_ft_train_n_msk_usdata_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_train_normal_mgdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_train_n_im_mgdata_dir,\n",
    "    mask_dir = multi_ft_train_n_msk_mgdata_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "# Final Multimodal Finetune Training Set\n",
    "segm_multi_finetune_train_set = torch.utils.data.ConcatDataset([segm_multi_finetune_train_benign_usdata, segm_multi_finetune_train_benign_mgdata, segm_multi_finetune_train_malignant_usdata, segm_multi_finetune_train_malignant_mgdata, segm_multi_finetune_train_normal_usdata, segm_multi_finetune_train_normal_mgdata])\n",
    "\n",
    "# Multimodal Finetune - Validation\n",
    "segm_multi_finetune_val_benign_usdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_val_b_im_usdata_dir,\n",
    "    mask_dir = multi_ft_val_b_msk_usdata_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_val_benign_mgdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_val_b_im_mgdata_dir,\n",
    "    mask_dir = multi_ft_val_b_msk_mgdata_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_val_malignant_usdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_val_m_im_usdata_dir,\n",
    "    mask_dir = multi_ft_val_m_msk_usdata_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_val_malignant_mgdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_val_m_im_mgdata_dir,\n",
    "    mask_dir = multi_ft_val_m_msk_mgdata_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_val_normal_usdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_val_n_im_usdata_dir,\n",
    "    mask_dir = multi_ft_val_n_msk_usdata_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_val_normal_mgdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_val_n_im_mgdata_dir,\n",
    "    mask_dir = multi_ft_val_n_msk_mgdata_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "# Final Multimodal Finetune Validation Set\n",
    "segm_multi_finetune_val_set = torch.utils.data.ConcatDataset([segm_multi_finetune_val_benign_usdata, segm_multi_finetune_val_benign_mgdata, segm_multi_finetune_val_malignant_usdata, segm_multi_finetune_val_malignant_mgdata, segm_multi_finetune_val_normal_usdata, segm_multi_finetune_val_normal_mgdata])\n",
    "\n",
    "# Multimodal Finetune - Test\n",
    "segm_multi_finetune_test_benign_usdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_test_b_im_usdata_dir,\n",
    "    mask_dir = multi_ft_test_b_msk_usdata_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_test_benign_mgdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_test_b_im_mgdata_dir,\n",
    "    mask_dir = multi_ft_test_b_msk_mgdata_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_test_malignant_usdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_test_m_im_usdata_dir,\n",
    "    mask_dir = multi_ft_test_m_msk_usdata_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_test_malignant_mgdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_test_m_im_mgdata_dir,\n",
    "    mask_dir = multi_ft_test_m_msk_mgdata_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_test_normal_usdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_test_n_im_usdata_dir,\n",
    "    mask_dir = multi_ft_test_n_msk_usdata_dir,\n",
    "    modality = \"ultra\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "segm_multi_finetune_test_normal_mgdata = FinetuningSegmDataset(\n",
    "    image_dir = multi_ft_test_n_im_mgdata_dir,\n",
    "    mask_dir = multi_ft_test_n_msk_mgdata_dir,\n",
    "    modality = \"mammo\",\n",
    "    transform = only_resize_transforms,\n",
    "    mask_transform = only_resize_transforms_masks\n",
    ")\n",
    "\n",
    "# Final Multimodal Finetune Training Set\n",
    "segm_multi_finetune_test_set = torch.utils.data.ConcatDataset([segm_multi_finetune_test_benign_usdata, segm_multi_finetune_test_benign_mgdata, segm_multi_finetune_test_malignant_usdata, segm_multi_finetune_test_malignant_mgdata, segm_multi_finetune_test_normal_usdata, segm_multi_finetune_test_normal_mgdata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "segm_multi_finetune_train_dataloader = torch.utils.data.DataLoader(\n",
    "    segm_multi_finetune_train_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "segm_multi_finetune_val_dataloader = torch.utils.data.DataLoader(\n",
    "    segm_multi_finetune_val_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "segm_multi_finetune_test_dataloader = torch.utils.data.DataLoader(\n",
    "    segm_multi_finetune_test_set,\n",
    "    batch_size = finetune_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. View pretrain augmented example and finetune normalized image example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmented images\n",
    "NUM_IMAGES = 3\n",
    "imgs = []\n",
    "\n",
    "for idx in range(NUM_IMAGES):\n",
    "    view1, view2 = us_pretrain_train_set[idx]\n",
    "    \n",
    "    # Add both views directly to the list\n",
    "    imgs.append(view1)  # view1 is already a tensor\n",
    "    imgs.append(view2)  # view2 is already a tensor\n",
    "\n",
    "# Stack images along the batch dimension\n",
    "imgs = torch.stack(imgs)\n",
    "\n",
    "print(imgs.size())\n",
    "\n",
    "# Display a grid of images\n",
    "img_grid = torchvision.utils.make_grid(imgs, nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Augmented image examples')\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES = 6\n",
    "imgs = []\n",
    "\n",
    "for idx in range(NUM_IMAGES):\n",
    "    img, label = classif_us_finetune_train_set[idx]  # Retrieve image and ignore label\n",
    "    imgs.append(img)\n",
    "\n",
    "# Stack images along the batch dimension\n",
    "imgs = torch.stack(imgs)\n",
    "\n",
    "print(\"Stacked image tensor size:\", imgs.size())\n",
    "\n",
    "# Display a grid of images\n",
    "img_grid = torchvision.utils.make_grid(imgs, nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Normalized and padded finetuning image examples')\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES = 6\n",
    "imgs = []\n",
    "masks = []\n",
    "\n",
    "for idx in range(NUM_IMAGES):\n",
    "    img, mask = segm_us_finetune_train_set[idx]  # Retrieve image and mask\n",
    "    imgs.append(img)\n",
    "    masks.append(mask)\n",
    "\n",
    "# Stack images and masks along the batch dimension\n",
    "imgs = torch.stack(imgs)\n",
    "masks = torch.stack(masks)\n",
    "\n",
    "print(\"Stacked image tensor size:\", imgs.size())\n",
    "print(\"Stacked mask tensor size:\", masks.size())\n",
    "\n",
    "# Create and display a grid of images and masks\n",
    "img_grid = torchvision.utils.make_grid(imgs, nrow=6, normalize=True, pad_value=0.9)\n",
    "mask_grid = torchvision.utils.make_grid(masks, nrow=6, normalize=True, pad_value=0.9)\n",
    "\n",
    "# Plot images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(img_grid.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title('Images')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot masks\n",
    "axes[1].imshow(mask_grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "axes[1].set_title('Masks')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch from the DataLoader\n",
    "batch = next(iter(segm_us_finetune_train_dataloader))\n",
    "\n",
    "# Unpack images and masks\n",
    "images, masks = batch\n",
    "\n",
    "# Check shapes\n",
    "print(f\"Shape of images: {images.shape}\")  # Should be [batch_size, channels, height, width]\n",
    "print(f\"Shape of masks: {masks.shape}\")    # Should be [batch_size, 1, height, width]\n",
    "\n",
    "# Visualize the first few images and masks in the batch\n",
    "NUM_IMAGES = 3\n",
    "imgs_to_plot = images[:NUM_IMAGES]\n",
    "masks_to_plot = masks[:NUM_IMAGES]\n",
    "\n",
    "# Unnormalize images for visualization\n",
    "unnormalized_imgs = imgs_to_plot * 0.5 + 0.5  # normalization was [-1, 1] -> [0, 1]\n",
    "unnormalized_imgs = unnormalized_imgs.permute(0, 2, 3, 1)  # [N, H, W, C] for visualization\n",
    "\n",
    "plt.figure(figsize=(10, NUM_IMAGES * 5))\n",
    "for i in range(NUM_IMAGES):\n",
    "    # Plot image\n",
    "    plt.subplot(NUM_IMAGES, 2, 2 * i + 1)\n",
    "    plt.imshow(unnormalized_imgs[i].squeeze(-1), cmap='gray')  # Grayscale\n",
    "    plt.title(f\"Image {i + 1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot mask\n",
    "    plt.subplot(NUM_IMAGES, 2, 2 * i + 2)\n",
    "    plt.imshow(masks_to_plot[i].squeeze(0), cmap='gray')  # Single-channel mask\n",
    "    plt.title(f\"Mask {i + 1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing model design components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Setting loss criterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRNTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(SimCLRNTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "\n",
    "        # Normalize the feature vectors\n",
    "        z_i = F.normalize(z_i, p=2, dim=-1)\n",
    "        z_j = F.normalize(z_j, p=2, dim=-1)\n",
    "\n",
    "        # Concatenate the representations from both views\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "\n",
    "        # Compute cosine similarity between all pairs\n",
    "        similarity_matrix = torch.matmul(representations, representations.T) / self.temperature\n",
    "\n",
    "        # Remove diagonal elements (self-similarity) from the similarity matrix\n",
    "        # Mask the diagonal with a large negative constant (-1e9) instead of -inf\n",
    "        mask = torch.eye(similarity_matrix.size(0), dtype=torch.bool).to(similarity_matrix.device)\n",
    "        similarity_matrix = similarity_matrix.masked_fill(mask, -1e9)\n",
    "\n",
    "        #  Clamp similarity values if they're too large - add again if problems arise\n",
    "        similarity_matrix = torch.clamp(similarity_matrix, -1, 1)\n",
    "\n",
    "        # Create labels for positive pairs (view 1 vs view 2 of the same image)\n",
    "        batch_size = z_i.size(0)\n",
    "        labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)]).to(z_i.device)\n",
    "\n",
    "        # Before calculating the loss\n",
    "        if torch.isnan(similarity_matrix).any() or torch.isinf(similarity_matrix).any():\n",
    "            print(\"Warning: NaN or Inf detected in similarity matrix\")\n",
    "            print(f\"Similarity matrix: {similarity_matrix}\")\n",
    "            return torch.tensor(0.0).to(similarity_matrix.device)  # Return 0 (no crashing)\n",
    "        \n",
    "        # Apply cross-entropy loss\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Moco incorporates uses NTXentLoss too but needs adapting for the memory bank for negative samples.\n",
    "class MoCoNTXentLoss(nn.Module):\n",
    "    def __init__(self, queue_size, temperature):\n",
    "        super(MoCoNTXentLoss, self).__init__()\n",
    "        self.queue_size = queue_size\n",
    "        self.temperature = temperature\n",
    "        self.similarity_fn = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, query_features, key_features, queue):\n",
    "        # Compute similarities between query and key\n",
    "        positive_similarity = self.similarity_fn(query_features, key_features)\n",
    "\n",
    "        # Calculate negative similarities from the queue\n",
    "        negative_similarity = self.similarity_fn(query_features.unsqueeze(1), queue.unsqueeze(0))\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        positive_similarity /= self.temperature\n",
    "        negative_similarity /= self.temperature\n",
    "        \n",
    "        # Concatenate positive and negative logits\n",
    "        logits = torch.cat([positive_similarity.unsqueeze(1), negative_similarity], dim=1)\n",
    "\n",
    "        assert logits.size(0) == query_features.size(0), \\\n",
    "        f\"Logits size {logits.size()} doesn't match batch size {query_features.size(0)}\"\n",
    "        \n",
    "        # For preventing NaNs or Infinites for loss, could be adapted if -10 to 10 isnt ideal range for this case\n",
    "        logits = torch.clamp(logits, min=-10, max=10)\n",
    "\n",
    "        # Labels for contrastive loss (positive sample at index 0)\n",
    "        labels = torch.zeros(query_features.size(0), dtype=torch.long).to(query_features.device)\n",
    "\n",
    "        # Cross-entropy loss for positive-negative pairs\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n",
    "\n",
    "# In BYOL contrastive loss isnt explicitly used.\n",
    "class BYOLLoss(nn.Module):\n",
    "    def __init__(self, epsilon = 1e-6):\n",
    "        super(BYOLLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \n",
    "        # Normalize the feature vectors to unit vectors\n",
    "        z_i = F.normalize(z_i, p=2, dim=-1)\n",
    "        z_j = F.normalize(z_j, p=2, dim=-1)\n",
    "\n",
    "        # Negative cosine similarity (1 - dot product between unit vectors)\n",
    "        loss = 1 - F.cosine_similarity(z_i, z_j, dim=-1)\n",
    "\n",
    "        # The final loss is averaged over the batch\n",
    "        return loss.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Setting contrastive learning model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used by Mocov2 and SimCLRv2\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim), #not in original Mocov2 and Simclrv2 but adds stability without changing functionality\n",
    "        )\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Pretrain model class for SimCLR v2\n",
    "class SimCLR_v2(nn.Module):\n",
    "    def __init__(self, backbone, projection_dim = 128):\n",
    "        super(SimCLR_v2, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "        #The key component outside of deeper net backbone (like a resnet 128) to change from simclr v1 to v2 \n",
    "        #is having additional layers in the MLP projection head\n",
    "        # Use the custom ProjectionHead class\n",
    "        self.projection_head = ProjectionHead(input_dim=512, hidden_dim=512, output_dim=projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x).flatten(start_dim = 1)\n",
    "        z = self.projection_head(features)\n",
    "        return z\n",
    "\n",
    "# Source used queue_size = 65536, in experiments here is reduced to 25%\n",
    "class MoCo_v2(nn.Module):\n",
    "    def __init__(self, backbone, projection_dim=128, queue_size=16384, momentum=0.999, queue_decay=0.99):\n",
    "        \n",
    "        super(MoCo_v2, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = ProjectionHead(input_dim=512, hidden_dim=512, output_dim=projection_dim)\n",
    "        self.queue_size = queue_size\n",
    "        self.momentum = momentum\n",
    "        self.queue_decay = queue_decay\n",
    "\n",
    "        # Initialize the projection head for keys\n",
    "        self.projection_head_k = ProjectionHead(input_dim=512, hidden_dim=512, output_dim=projection_dim)\n",
    "\n",
    "        # Initialize the momentum encoder (key encoder)\n",
    "        self.key_encoder = nn.Sequential(\n",
    "            *list(self.backbone.children())[:-1],  # Backbone layers minus the last layer (classifier)\n",
    "            self.projection_head  # Add the projection head on top\n",
    "        )\n",
    "\n",
    "        # Initialize the momentum encoder (key encoder) with the projection head for keys\n",
    "        self.momentum_encoder = nn.Sequential(\n",
    "            *list(self.backbone.children())[:-1],  # Backbone layers minus the last layer (classifier)\n",
    "            self.projection_head_k  # Add projection head for keys\n",
    "        )\n",
    "\n",
    "        # Initialize the queue and normalize\n",
    "        self.register_buffer(\"queue\", torch.randn(queue_size, projection_dim))\n",
    "        self.queue = F.normalize(self.queue, dim=1)\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))  # Queue pointer\n",
    "\n",
    "        # Initialize momentum encoder and projection head for keys\n",
    "        self.momentum_encoder = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_k = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        # Freeze gradients for momentum encoder\n",
    "        for param in self.momentum_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.projection_head_k.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def momentum_update_key_encoder(self):\n",
    "        # Momentum update for key encoder parameters\n",
    "        for param_q, param_k in zip(self.backbone.parameters(), self.momentum_encoder.parameters()):\n",
    "            param_k.data = param_k.data * self.momentum + param_q.data * (1. - self.momentum)\n",
    "        for param_q, param_k in zip(self.projection_head.parameters(), self.projection_head_k.parameters()):\n",
    "            param_k.data = param_k.data * self.momentum + param_q.data * (1. - self.momentum)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_queue(self, key_features):\n",
    "        \n",
    "        key_features = key_features.detach()  # Detach to avoid backpropagation through the queue\n",
    "        key_features = F.normalize(key_features, dim=1)  # Normalize the key features\n",
    "\n",
    "        batch_size = key_features.size(0)\n",
    "        ptr = int(self.queue_ptr)\n",
    "\n",
    "        # If queue_decay is active, blend the new keys with the existing queue values\n",
    "        if self.queue_decay < 1.0:\n",
    "            for i in range(batch_size):\n",
    "                index = (ptr + i) % self.queue_size\n",
    "                self.queue[index] = self.queue_decay * self.queue[index] + (1 - self.queue_decay) * key_features[i]\n",
    "        else:  # Directly replace the keys in the queue\n",
    "            end_pos = (ptr + batch_size) % self.queue_size\n",
    "            if ptr + batch_size <= self.queue_size:\n",
    "                self.queue[ptr:ptr + batch_size] = key_features\n",
    "            else:\n",
    "                part_1_len = self.queue_size - ptr\n",
    "                self.queue[ptr:] = key_features[:part_1_len]\n",
    "                self.queue[:end_pos] = key_features[part_1_len:]\n",
    "\n",
    "        # Update the queue pointer\n",
    "        self.queue_ptr[0] = (ptr + batch_size) % self.queue_size\n",
    "\n",
    "    def forward(self, x_q, x_k):\n",
    "        \n",
    "        # Forward pass for query\n",
    "        q = self.backbone(x_q).flatten(start_dim=1)\n",
    "        q = self.projection_head(q)\n",
    "\n",
    "        # Forward pass for key using momentum encoder\n",
    "        with torch.no_grad():\n",
    "            self.momentum_update_key_encoder()\n",
    "            k = self.momentum_encoder(x_k).flatten(start_dim=1)\n",
    "            k = self.projection_head_k(k)\n",
    "\n",
    "        # Normalize q and k\n",
    "        q = F.normalize(q, dim=1)\n",
    "        k = F.normalize(k, dim=1)\n",
    "\n",
    "        # Update the queue with the new key features\n",
    "        self.update_queue(k)\n",
    "\n",
    "        return q, k\n",
    "\n",
    "# Pretrain model class for BYOL\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(self, backbone, projection_dim = 128, momentum=0.99):\n",
    "        super(BYOL, self).__init__()\n",
    "        # Online encoder\n",
    "        self.online_encoder = nn.Sequential(\n",
    "            backbone,\n",
    "            nn.Linear(backbone.output_dim, projection_dim),\n",
    "            nn.BatchNorm1d(projection_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Prediction head\n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.Linear(projection_dim, projection_dim),\n",
    "            nn.BatchNorm1d(projection_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(projection_dim, projection_dim)\n",
    "        )\n",
    "        \n",
    "        # Target encoder (deepcopy from online encoder)\n",
    "        self.target_encoder = copy.deepcopy(self.online_encoder)\n",
    "\n",
    "        # Freeze target encoder parameters\n",
    "        for param in self.target_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.momentum = momentum\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_target_encoder(self):\n",
    "        for param_online, param_target in zip(self.online_encoder.parameters(), self.target_encoder.parameters()):\n",
    "            param_target.data = self.momentum * param_target.data + (1 - self.momentum) * param_online.data\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Online encoder forward pass\n",
    "        z1_online = self.online_encoder(x1)\n",
    "        z2_online = self.online_encoder(x2)\n",
    "\n",
    "        # Prediction head forward pass\n",
    "        p1_online = self.prediction_head(z1_online)\n",
    "        p2_online = self.prediction_head(z2_online)\n",
    "\n",
    "        # Target encoder forward pass (no gradients)\n",
    "        with torch.no_grad():\n",
    "            z1_target = self.target_encoder(x1)\n",
    "            z2_target = self.target_encoder(x2)\n",
    "\n",
    "        return p1_online, p2_online, z1_target, z2_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Setting saving and loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_save(model_id, phase, save_path, results, current_epoch, total_epoch_count):\n",
    "    # Save the checkpoint information: results, model ID, phase, and epoch.\n",
    "       \n",
    "    results_filename = f\"logs_{model_id}_{phase}.pkl\"\n",
    "    results_path = os.path.join(save_path, results_filename)\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Check results file already exists\n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, 'rb') as f:\n",
    "            existing_results = pickle.load(f)\n",
    "\n",
    "        # Get last epoch saved\n",
    "        last_saved_epoch = max(existing_results['epoch'], default=0)\n",
    "\n",
    "        # If last saved epoch is greater than or equal to the total epoch count, training is done\n",
    "        if last_saved_epoch >= total_epoch_count:\n",
    "            raise RuntimeError(f\"Training is already complete for {model_id} in {phase} phase. \"\n",
    "                               f\"Last saved epoch: {last_saved_epoch}. Restart training if necessary.\")\n",
    "\n",
    "        # Append new epochs' results, skipping already saved epochs\n",
    "        for key in results.keys():\n",
    "            existing_results[key].extend(results[key])\n",
    "\n",
    "        results_to_save = existing_results\n",
    "    else:\n",
    "        # No existing file, save new results\n",
    "        results_to_save = results\n",
    "\n",
    "    # Save the updated or new results using pickle\n",
    "    with open(results_path, 'wb') as f:\n",
    "        pickle.dump(results_to_save, f)\n",
    "\n",
    "    # If the current epoch equals the total work is complete\n",
    "    if current_epoch == total_epoch_count:\n",
    "        print(f\"Training for {model_id} in {phase} phase is complete at epoch {current_epoch}.\")\n",
    "\n",
    "    return results_path\n",
    "    \n",
    "def load_results(results_file):\n",
    "    with open(results_file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Setting SimCLR Pretrain functions usable by classification and segmentation models alike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_simclr(pretrain_model, pretrain_criterion, train_dataloader, pretrain_optimizer, device):\n",
    "    pretrain_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (x0, x1) in train_dataloader:\n",
    "        pretrain_optimizer.zero_grad()\n",
    "        \n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        z0 = pretrain_model(x0)\n",
    "        z1 = pretrain_model(x1)\n",
    "\n",
    "        loss = pretrain_criterion(z0, z1)\n",
    "\n",
    "        batch_size = x0.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(pretrain_model.parameters(), max_norm=1.0)\n",
    "\n",
    "        pretrain_optimizer.step()\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader.dataset)\n",
    "    \n",
    "    return avg_train_loss\n",
    "\n",
    "def validate_one_epoch_simclr(pretrain_model, pretrain_criterion, val_dataloader, device):\n",
    "    pretrain_model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x0, x1) in val_dataloader:\n",
    "            x0 = x0.to(device)\n",
    "            x1 = x1.to(device)\n",
    "            \n",
    "            z0 = pretrain_model(x0)\n",
    "            z1 = pretrain_model(x1)\n",
    "\n",
    "            loss = pretrain_criterion(z0, z1)\n",
    "\n",
    "            batch_size = x0.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            \n",
    "    avg_val_loss = total_loss / len(val_dataloader.dataset)\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "def run_pretraining_simclr(pretrain_model, pretrain_criterion, train_dataloader, val_dataloader, pretrain_optimizer, pretrain_scheduler, device, total_epoch_count, storing_path, identifier, checkpoint_interval):\n",
    "    pretrain_model.to(device)\n",
    "\n",
    "    results_path = None\n",
    "    results = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "\n",
    "    print(\"»»» Starting Network Pretraining process \\n\")\n",
    "\n",
    "    for epoch in range(total_epoch_count):\n",
    "        avg_train_loss = train_one_epoch_simclr(\n",
    "            pretrain_model, pretrain_criterion, train_dataloader, pretrain_optimizer, device\n",
    "        )\n",
    "        avg_val_loss = validate_one_epoch_simclr(\n",
    "            pretrain_model, pretrain_criterion, val_dataloader, device\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{total_epoch_count}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Append results for the current epoch\n",
    "        results['epoch'].append(epoch + 1) # Stored as part of the results data to analyze performance metrics later\n",
    "        results['train_loss'].append(avg_train_loss)\n",
    "        results['val_loss'].append(avg_val_loss)\n",
    "\n",
    "        # Inside the epoch loop\n",
    "        lr_before_scheduler = pretrain_optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch [{epoch+1}/{total_epoch_count}], Learning Rate before scheduler: {lr_before_scheduler:.6f}\")\n",
    "\n",
    "        # Apply the scheduler if present\n",
    "        if pretrain_scheduler:\n",
    "            pretrain_scheduler.step(avg_val_loss)\n",
    "            lr_after_scheduler = pretrain_optimizer.param_groups[0]['lr']\n",
    "            print(f\"Learning Rate after scheduler: {lr_after_scheduler:.6f}\")\n",
    "\n",
    "        if (epoch + 1) % checkpoint_interval == 0 or epoch == total_epoch_count - 1:\n",
    "            print(\"»»» Checkpoint reached: Saving model state and results\")\n",
    "            try:\n",
    "                results_path = results_save(identifier, \"pt\", storing_path, results, epoch, total_epoch_count)\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "                return None\n",
    "        \n",
    "            # Clear results to avoid duplication\n",
    "            results = {key: [] for key in results.keys()}\n",
    "\n",
    "    print(\"\\n»»» Network Pretraining process complete\\n»»» Logs saved at: \", results_path)\n",
    "    \n",
    "    #returns path of file with results\n",
    "    return results_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Setting MoCo Pretrain functions usable by classification and segmentation models alike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_moco(pretrain_model, pretrain_criterion, train_dataloader, pretrain_optimizer, device):\n",
    "    # Set the online encoder to training mode\n",
    "    pretrain_model.train()\n",
    "\n",
    "    # Set the momentum encoder to eval mode (suggestion 6)\n",
    "    pretrain_model.momentum_encoder.eval()\n",
    "\n",
    "    # Freeze momentum encoder gradients (suggestion 7)\n",
    "    for param in pretrain_model.momentum_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (x_q, x_k) in enumerate(train_dataloader):\n",
    "        # Move to device\n",
    "        x_q = x_q.to(device)\n",
    "        x_k = x_k.to(device)\n",
    "\n",
    "        # Get the query and key representations\n",
    "        q, k = pretrain_model(x_q, x_k)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = pretrain_criterion(q, k, pretrain_model.queue)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        pretrain_optimizer.step()\n",
    "        pretrain_optimizer.zero_grad()\n",
    "\n",
    "        # Update the queue inside the MoCo model\n",
    "        pretrain_model.update_queue(k)\n",
    "\n",
    "        batch_size = x_q.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader.dataset)\n",
    "\n",
    "    return avg_train_loss\n",
    "\n",
    "def validate_one_epoch_moco(pretrain_model, pretrain_criterion, val_dataloader, device):\n",
    "    pretrain_model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_q, x_k) in enumerate(val_dataloader):\n",
    "            x_q = x_q.to(device)\n",
    "            x_k = x_k.to(device)\n",
    "\n",
    "            # Get the query and key representations\n",
    "            q, k = pretrain_model(x_q, x_k)\n",
    "\n",
    "            # Calculate the validation loss using the fixed queue\n",
    "            loss = pretrain_criterion(q, k, pretrain_model.queue)\n",
    "\n",
    "            batch_size = x_q.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_dataloader.dataset)\n",
    "    \n",
    "    return avg_val_loss\n",
    "\n",
    "def run_pretraining_moco(pretrain_model, pretrain_criterion, train_dataloader, val_dataloader, pretrain_optimizer, pretrain_scheduler, device, total_epoch_count, storing_path, identifier, checkpoint_interval):\n",
    "    pretrain_model.to(device)\n",
    "\n",
    "    results_path = None\n",
    "    results = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "\n",
    "    print(\"»»» Starting MoCo Pretraining process \\n\")\n",
    "\n",
    "    for epoch in range(total_epoch_count):\n",
    "        # Training\n",
    "        avg_train_loss = train_one_epoch_moco(\n",
    "            pretrain_model, pretrain_criterion, train_dataloader, pretrain_optimizer, device\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        avg_val_loss = validate_one_epoch_moco(\n",
    "            pretrain_model, pretrain_criterion, val_dataloader, device\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{total_epoch_count}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Append results for the current epoch\n",
    "        results['epoch'].append(epoch + 1)\n",
    "        results['train_loss'].append(avg_train_loss)\n",
    "        results['val_loss'].append(avg_val_loss)\n",
    "\n",
    "        if pretrain_scheduler:\n",
    "            pretrain_scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % checkpoint_interval == 0 or epoch == total_epoch_count - 1:\n",
    "            print(\"»»» Checkpoint reached: Saving model state and results\")\n",
    "            try:\n",
    "                results_path = results_save(identifier, \"pt\", storing_path, results, epoch, total_epoch_count)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error saving checkpoint at epoch {epoch + 1}: {e}\")\n",
    "                return None\n",
    "\n",
    "            # Clear results to avoid duplication\n",
    "            results = {key: [] for key in results.keys()}\n",
    "\n",
    "    print(\"\\n»»» MoCo Pretraining process complete\\n»»» Logs saved at: \", results_path)\n",
    "    \n",
    "    return results_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Setting BYOL Pretrain functions usable by classification and segmentation models alike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_byol(pretrain_model, pretrain_criterion, train_dataloader, pretrain_optimizer, device):\n",
    "    # Set online encoder to training mode\n",
    "    pretrain_model.train()  \n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (x1, x2) in enumerate(train_dataloader):\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        p1_online, p2_online, z1_target, z2_target = pretrain_model(x1, x2)\n",
    "\n",
    "        # Compute BYOL loss\n",
    "        loss = pretrain_criterion(p1_online, z1_target) + pretrain_criterion(p2_online, z2_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        pretrain_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        pretrain_optimizer.step()\n",
    "\n",
    "        # Update target encoder\n",
    "        pretrain_model.update_target_encoder()\n",
    "\n",
    "        batch_size = x1.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        \n",
    "    avg_val_loss = total_loss / len(train_dataloader.dataset)\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "def validate_one_epoch_byol(pretrain_model, pretrain_criterion, val_dataloader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    pretrain_model.eval()  \n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x1, x2) in enumerate(val_dataloader):\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            p1_online, p2_online, z1_target, z2_target = pretrain_model(x1, x2)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = pretrain_criterion(p1_online, z1_target) + pretrain_criterion(p2_online, z2_target)\n",
    "\n",
    "            batch_size = x1.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_dataloader.dataset)\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "def run_pretraining_byol(pretrain_model, pretrain_criterion, train_dataloader, val_dataloader, pretrain_optimizer, device, total_epoch_count, storing_path, identifier, checkpoint_interval):\n",
    "    pretrain_model.to(device)\n",
    "\n",
    "    results_path = None\n",
    "    results = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "\n",
    "    print(\"»»» Starting BYOL Pretraining process \\n\")\n",
    "\n",
    "    for epoch in range(total_epoch_count):\n",
    "        \n",
    "        # Training\n",
    "        avg_train_loss = train_one_epoch_byol(\n",
    "            pretrain_model, pretrain_criterion, train_dataloader, pretrain_optimizer, device\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        avg_val_loss = validate_one_epoch_byol(\n",
    "            pretrain_model, pretrain_criterion, val_dataloader, device\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{total_epoch_count}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Append results for the current epoch\n",
    "        results['epoch'].append(epoch + 1)\n",
    "        results['train_loss'].append(avg_train_loss)\n",
    "        results['val_loss'].append(avg_val_loss)\n",
    "\n",
    "\n",
    "        if (epoch + 1) % checkpoint_interval == 0 or epoch == total_epoch_count - 1:\n",
    "            print(\"»»» Checkpoint reached: Saving model state and results\")\n",
    "            try:\n",
    "                results_path = results_save(identifier, \"pt\", storing_path, results, epoch, total_epoch_count)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error saving checkpoint at epoch {epoch + 1}: {e}\")\n",
    "                return None\n",
    "\n",
    "            # Clear results to avoid duplication\n",
    "            results = {key: [] for key in results.keys()}\n",
    "\n",
    "    print(\"\\n»»» BYOL Pretraining process complete\\n»»» Logs saved at: \", results_path)\n",
    "    \n",
    "    return results_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Setting Pretrain phase loss plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pretrain_results(results_path):\n",
    "    if not os.path.exists(results_path):\n",
    "        print(f\"File not found: {results_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"»»» Data taken from path: {results_path}\")\n",
    "    with open(results_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    # Extract the directory path from the results_path\n",
    "    directory_path = os.path.dirname(results_path)\n",
    "\n",
    "    # Extract the identifier correctly\n",
    "    filename = os.path.basename(results_path)\n",
    "    parts = filename.split('_')\n",
    "    \n",
    "    if len(parts) < 2:\n",
    "        print(f\"»»» Unexpected filename format: {filename}\")\n",
    "        return\n",
    "    \n",
    "    # Assume the identifier is the part between the underscores\n",
    "    identifier = parts[1]  # Adjust if needed based on your file naming convention\n",
    "\n",
    "    # Define the plot filename and path\n",
    "    plot_filename = f\"{identifier}_loss_pt.png\"\n",
    "    plot_path = os.path.join(os.path.dirname(results_path), plot_filename)\n",
    "\n",
    "    # Verify and print loss data\n",
    "    train_losses = results.get('train_loss', [])\n",
    "    val_losses = results.get('val_loss', [])\n",
    "    epochs = results.get('epoch', [])\n",
    "\n",
    "    if not train_losses or not val_losses:\n",
    "        print(\"»»» No loss data available for plotting.\")\n",
    "        return\n",
    "    \n",
    "    # Check for loss trends\n",
    "    if train_losses[-1] > train_losses[0]:\n",
    "        print(\"»»» Warning: Training loss increased over epochs.\")\n",
    "    if val_losses[-1] > val_losses[0]:\n",
    "        print(\"»»» Warning: Validation loss increased over epochs.\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results['epoch'], results['train_loss'], label=f'Train Loss ({train_losses[-1]:.4f})', color='blue')\n",
    "    plt.plot(results['epoch'], results['val_loss'], label=f'Val Loss ({val_losses[-1]:.4f})', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Pretrained model {identifier} - Loss graph (Train / Val)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Create tick positions at every 10th epoch starting from 1\n",
    "    epoch_ticks = list(range(1, len(epochs) + 1, 10))\n",
    "\n",
    "    # Ensure that the final tick (last epoch) is included\n",
    "    if epoch_ticks[-1] != len(epochs):\n",
    "        epoch_ticks.append(len(epochs))\n",
    "\n",
    "    # Adjust ticks to subtract 1 from all ticks except the first and the last\n",
    "    adjusted_ticks = [tick - 1 if tick != 1 and tick != len(epochs) else tick for tick in epoch_ticks]\n",
    "\n",
    "    # Set the ticks on the x-axis with the appropriate labels\n",
    "    plt.xticks(adjusted_ticks, labels=[str(epoch) for epoch in adjusted_ticks])\n",
    "\n",
    "\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FormatStrFormatter('%.1f'))\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n»»» Train/Val Loss Plot saved at {directory_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Dataset class setting for t-distributed stochastic neighbour embedding visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretraining unlabeled set - validation set for t-sne process (no augmentations)\n",
    "class UnchangedDataset(Dataset):\n",
    "  def __init__(self, image_dir, transform = None):\n",
    "    self.image_dir = image_dir\n",
    "    self.transform = transform\n",
    "    self.images = [f for f in os.listdir(image_dir) if f.endswith(('jpg', 'png', 'jpeg'))]  # Filter for image files\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "    image = Image.open(img_path).convert(\"L\")\n",
    "    if self.transform is not None:\n",
    "        image = self.transform(image)\n",
    "    return image\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Assuming each item in batch is a tensor of images\n",
    "    images = [item[0] for item in batch]  # Extract the images\n",
    "    return torch.stack(images)  # Stack them into a single tensor\n",
    "\n",
    "def extract_embeddings(model, dataloader, device, is_byol=False):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Check the type and structure of the batch\n",
    "            if isinstance(batch, torch.Tensor):\n",
    "                images = batch.to(device)\n",
    "            elif isinstance(batch, list):\n",
    "                # Convert list of tensors to a single tensor\n",
    "                images = torch.stack(batch).to(device)\n",
    "            elif isinstance(batch, tuple):\n",
    "                images = batch[0].to(device)\n",
    "            else:\n",
    "                raise TypeError(f\"Unexpected batch type: {type(batch)}\")\n",
    "\n",
    "            # Extract features\n",
    "            if is_byol:\n",
    "                features = model(images).flatten(start_dim=1)  # Directly use the online encoder for BYOL\n",
    "            else:\n",
    "                features = model.backbone(images).flatten(start_dim=1)  # Use backbone for SimCLR and MoCo\n",
    "            embeddings.append(features.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(embeddings)\n",
    "\n",
    "def plot_tsne(embeddings, results_path):\n",
    "    # Extract directory and model_id from results_path\n",
    "    directory_path = os.path.dirname(results_path)\n",
    "    filename = os.path.basename(results_path)\n",
    "    model_id = filename.split('_')[1]  # Assuming model_id is between underscores\n",
    "\n",
    "    # Create t-SNE plot\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.6)\n",
    "\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    fig = plt.gcf()  # Get current figure\n",
    "    fig.suptitle(f'Pretrained model {model_id} - t-distributed stochastic neighbor embedding visualization', fontsize=16)\n",
    "    \n",
    "    # Save the t-SNE plot image\n",
    "    plot_path = os.path.join(directory_path, f\"{model_id}_tsne.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"»»» t-SNE plot saved at: {directory_path}\")\n",
    "\n",
    "def tsne_pipeline(model, dataloader, device, results_path, is_byol=False):\n",
    "    if not is_byol:\n",
    "        # Freeze backbone parameters for SimCLR/MoCo\n",
    "        for param in model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Extract embeddings\n",
    "    embeddings = extract_embeddings(model, dataloader, device, is_byol)\n",
    "    plot_tsne(embeddings, results_path)\n",
    "\n",
    "    if not is_byol:\n",
    "        # Unfreeze backbone parameters for SimCLR/MoCo\n",
    "        for param in model.backbone.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE_us_dataset = UnchangedDataset(\n",
    "  image_dir = us_pt_val_im_dir,\n",
    "  transform = ContrastiveTransformations(base_transforms = only_resize_transforms, n_views = 1)\n",
    ")\n",
    "\n",
    "tSNE_us_dataloader = torch.utils.data.DataLoader(\n",
    "    tSNE_us_dataset,\n",
    "    batch_size = pretrain_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    num_workers=8,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE_mg_dataset = UnchangedDataset(\n",
    "  image_dir = mg_pt_val_im_dir,\n",
    "  transform = ContrastiveTransformations(base_transforms = only_resize_transforms, n_views = 1)\n",
    ")\n",
    "\n",
    "tSNE_mg_dataloader = torch.utils.data.DataLoader(\n",
    "    tSNE_mg_dataset,\n",
    "    batch_size = pretrain_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    num_workers=8,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi case requires merging things\n",
    "tSNE_multi_pretrain_validation_usdata = UnchangedDataset(\n",
    "    image_dir = multi_pt_val_im_usdata_dir,\n",
    "    transform = ContrastiveTransformations(base_transforms = only_resize_transforms, n_views = 1)\n",
    ")\n",
    "\n",
    "tSNE_multi_pretrain_validation_mgdata = UnchangedDataset(\n",
    "    image_dir = multi_pt_val_im_mgdata_dir,\n",
    "    transform = ContrastiveTransformations(base_transforms = only_resize_transforms, n_views = 1)\n",
    ")\n",
    "\n",
    "# Concatenate the ultrasound and mammography pretrain datasets\n",
    "tSNE_multi_pretrain_validation_dataset = torch.utils.data.ConcatDataset([tSNE_multi_pretrain_validation_usdata, tSNE_multi_pretrain_validation_mgdata])\n",
    "\n",
    "tSNE_multi_dataloader = torch.utils.data.DataLoader(\n",
    "    tSNE_multi_pretrain_validation_dataset,\n",
    "    batch_size = pretrain_batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    num_workers=8,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9. Setting Finetune / Test / Plotting functions specific to classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning Functions\n",
    "def classif_train_one_epoch(finetune_model, finetune_criterion, train_dataloader, finetune_optimizer, device):\n",
    "    finetune_model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        finetune_optimizer.zero_grad()\n",
    "\n",
    "        # Transfer Data to Device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = finetune_model(images)\n",
    "\n",
    "        # Calculate Loss\n",
    "        loss = finetune_criterion(outputs, labels)\n",
    "        #total_loss += loss.item()\n",
    "\n",
    "        #just checking\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Compute Predictions and Probabilities\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        all_probs.extend(probs.detach().cpu().numpy())\n",
    "\n",
    "        # Backward Pass and Optimization Step\n",
    "        loss.backward()\n",
    "        \n",
    "        finetune_optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader.dataset)\n",
    "\n",
    "    # Convert lists to numpy arrays for consistency\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    return avg_loss, all_labels, all_preds, all_probs\n",
    "\n",
    "def classif_validate_one_epoch(finetune_model, finetune_criterion, val_dataloader, device):\n",
    "    finetune_model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            # Transfer Data to Device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = finetune_model(images)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = finetune_criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Compute Predictions and Probabilities\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(val_dataloader.dataset)\n",
    "\n",
    "    # Convert lists to numpy arrays for consistency\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    return avg_loss, all_labels, all_preds, all_probs\n",
    "\n",
    "def classif_run_finetuning(finetune_model, finetune_criterion, train_dataloader, val_dataloader, finetune_optimizer, finetune_scheduler, device, total_epoch_count, storing_path, identifier, checkpoint_interval):\n",
    "\n",
    "    results_path = None\n",
    "    results = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_labels': [],\n",
    "        'train_preds': [],\n",
    "        'train_probs': [],\n",
    "        'val_labels': [],\n",
    "        'val_preds': [],\n",
    "        'val_probs': []\n",
    "    }\n",
    "\n",
    "    print(\"»»» Starting Classification Network Finetuning process\")\n",
    "\n",
    "    for epoch in range(total_epoch_count):\n",
    "        # Training\n",
    "        train_metrics = classif_train_one_epoch(finetune_model, finetune_criterion, train_dataloader, finetune_optimizer, device)\n",
    "\n",
    "        #Validation\n",
    "        val_metrics = classif_validate_one_epoch(finetune_model, finetune_criterion, val_dataloader, device)\n",
    "        \n",
    "        # Unpack the train and validation metrics\n",
    "        avg_train_loss, train_labels, train_preds, train_probs = train_metrics\n",
    "        avg_val_loss, val_labels, val_preds, val_probs = val_metrics\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{total_epoch_count}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        results['epoch'].append(epoch + 1)\n",
    "        results['train_loss'].append(avg_train_loss)\n",
    "        results['train_labels'].append(train_labels.tolist())\n",
    "        results['train_preds'].append(train_preds.tolist())      \n",
    "        results['train_probs'].append(train_probs.tolist())      \n",
    "        results['val_loss'].append(avg_val_loss)\n",
    "        results['val_labels'].append(val_labels.tolist())    \n",
    "        results['val_preds'].append(val_preds.tolist())  \n",
    "        results['val_probs'].append(val_probs.tolist()) \n",
    "        \n",
    "        if finetune_scheduler:\n",
    "            finetune_scheduler.step(avg_val_loss)\n",
    "\n",
    "        if (epoch + 1) % checkpoint_interval == 0 or (epoch + 1) == total_epoch_count:\n",
    "            print(f\"»»» Checkpoint reached at epoch {(epoch + 1)}: Saving model state and results\")\n",
    "            try:\n",
    "                results_path = results_save(identifier, \"ft\", storing_path, results, epoch, total_epoch_count)\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "                return None\n",
    "            \n",
    "            # Clear results to avoid duplication\n",
    "            results = {key: [] for key in results.keys()}\n",
    "\n",
    "    print(\"\\n»»» Classification Network Finetuning process complete\\n»»» Logs saved at: \", results_path)\n",
    "    \n",
    "    return results_path # Path of files with model and results\n",
    "\n",
    "def classif_plot_finetuning_results(results_path, classes):\n",
    "    \n",
    "    if not os.path.exists(results_path):\n",
    "        print(f\"File not found: {results_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"»»» Data taken from path: {results_path}\")\n",
    "    with open(results_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    # Extract directory and identifier from results_path\n",
    "    directory_path = os.path.dirname(results_path)\n",
    "    identifier = os.path.basename(results_path).split('_')[1]\n",
    "\n",
    "    plot_paths = {\n",
    "        'box1_pr': os.path.join(directory_path, f\"{identifier}_precision_recall_ft.png\"),\n",
    "        'box2_roc_auc': os.path.join(directory_path, f\"{identifier}_roc_auc_ft.png\"),\n",
    "        'box3_loss_and_value_metrics': os.path.join(directory_path, f\"{identifier}_loss_value_metrics_ft.png\")\n",
    "    }\n",
    "    \n",
    "    # Extract data\n",
    "    epochs = np.array(results['epoch'])  \n",
    "    train_loss = np.array(results['train_loss'])\n",
    "    val_loss = np.array(results['val_loss'])\n",
    "    train_labels = np.concatenate(results['train_labels'])\n",
    "    train_probs = np.concatenate(results['train_probs'])\n",
    "    val_labels = np.concatenate(results['val_labels'])\n",
    "    val_probs = np.concatenate(results['val_probs'])\n",
    "    train_preds = np.concatenate(results['train_preds'])\n",
    "    val_preds = np.concatenate(results['val_preds'])\n",
    "\n",
    "    # Colors for each class for consistent styling\n",
    "    colors = {'benign': 'blue', 'malignant': 'orange', 'normal': 'green'}\n",
    "\n",
    "    # 4x2 grid for the subplots\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 25))\n",
    "\n",
    "    # Convert train_labels to one-hot encoding to match the shape of train_probs\n",
    "    train_labels_one_hot = np.eye(len(classes))[train_labels]\n",
    "    val_labels_one_hot = np.eye(len(classes))[val_labels]\n",
    "\n",
    "    # Calculate Overall AP for Train and Validation\n",
    "    overall_ap_train = average_precision_score(train_labels_one_hot, train_probs, average='weighted')\n",
    "    overall_ap_val = average_precision_score(val_labels_one_hot, val_probs, average='weighted')\n",
    "\n",
    "    # Top Row - Precision-Recall for Train and Validation\n",
    "    # Box 1: Precision-Recall (Train)\n",
    "    for i, class_name in enumerate(classes):\n",
    "        binary_labels = (train_labels == i).astype(int)\n",
    "        try:\n",
    "            precision, recall, _ = precision_recall_curve(binary_labels, train_probs[:, i])\n",
    "            avg_precision = average_precision_score(binary_labels, train_probs[:, i])\n",
    "            axes[0, 0].plot(recall, precision, label=f'{class_name} (AP={avg_precision:.2f})', color=colors[class_name])\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting Precision-Recall for {class_name} (Train): {e}\")\n",
    "\n",
    "    axes[0, 0].set_xlabel('Recall')\n",
    "    axes[0, 0].set_ylabel('Precision')\n",
    "    axes[0, 0].set_title(f'Train - merged PR curves and overall weighted AP score = {overall_ap_train:.2f}')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # Box 2: Precision-Recall (Validation)\n",
    "    for i, class_name in enumerate(classes):\n",
    "        binary_labels = (val_labels == i).astype(int)\n",
    "        try:\n",
    "            precision, recall, _ = precision_recall_curve(binary_labels, val_probs[:, i])\n",
    "            avg_precision = average_precision_score(binary_labels, val_probs[:, i])\n",
    "            axes[0, 1].plot(recall, precision, label=f'{class_name} (AP={avg_precision:.2f})', color=colors[class_name])\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting Precision-Recall for {class_name} (Validation): {e}\")\n",
    "\n",
    "    axes[0, 1].set_xlabel('Recall')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title(f'Validation - merged PR curves and overall weighted AP score = {overall_ap_val:.2f}')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Row 2-4: Class-specific Precision-Recall curves (Train and Validation)\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_name = class_name.lower()  # Convert to lowercase to match the dictionary keys\n",
    "\n",
    "        # PR curve for each class\n",
    "        binary_train_labels = (train_labels == i).astype(int)\n",
    "        binary_val_labels = (val_labels == i).astype(int)\n",
    "\n",
    "        precision_train, recall_train, _ = precision_recall_curve(binary_train_labels, train_probs[:, i])\n",
    "        precision_val, recall_val, _ = precision_recall_curve(binary_val_labels, val_probs[:, i])\n",
    "\n",
    "        avg_precision_train_class = average_precision_score(binary_train_labels, train_probs[:, i])\n",
    "        avg_precision_val_class = average_precision_score(binary_val_labels, val_probs[:, i])\n",
    "\n",
    "        # Plotting each class separately\n",
    "        axes[i+1, 0].plot(recall_train, precision_train, color=colors[class_name], label=f'{class_name} (AP={avg_precision_train_class:.2f})')\n",
    "        axes[i+1, 0].set_title(f'Train - {class_name} PR Curve')\n",
    "        axes[i+1, 0].set_xlabel('Recall')\n",
    "        axes[i+1, 0].set_ylabel('Precision')\n",
    "        axes[i+1, 0].legend()\n",
    "\n",
    "        axes[i+1, 1].plot(recall_val, precision_val, color=colors[class_name], label=f'{class_name} (AP={avg_precision_val_class:.2f})')\n",
    "        axes[i+1, 1].set_title(f'Validation - {class_name} PR Curve')\n",
    "        axes[i+1, 1].set_xlabel('Recall')\n",
    "        axes[i+1, 1].set_ylabel('Precision')\n",
    "        axes[i+1, 1].legend()\n",
    "\n",
    "    # Adjust layout and save the figure\n",
    "    fig.suptitle(f'Classification model {identifier} - Weighted Precision-Recall Curves (Train / Val)', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(plot_paths['box1_pr'])\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"»»» Combined Precision-Recall plot saved at {plot_paths['box1_pr']}\")\n",
    "\n",
    "    #----------------\n",
    "    # Create a 4x2 grid for the subplots\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 25))\n",
    "\n",
    "    # Top Row - Overall Weighted ROC-AUC for Train and Validation\n",
    "    try:\n",
    "        # Weighted ROC-AUC for train and validation\n",
    "        roc_auc_weighted_train = roc_auc_score(train_labels, train_probs, multi_class='ovr', average='weighted')\n",
    "        roc_auc_weighted_val = roc_auc_score(val_labels, val_probs, multi_class='ovr', average='weighted')\n",
    "\n",
    "        # Class-specific ROC curves for Train\n",
    "        for i, class_name in enumerate(classes):\n",
    "            fpr, tpr, _ = roc_curve((train_labels == i).astype(int), train_probs[:, i])\n",
    "            roc_auc = roc_auc_score((train_labels == i).astype(int), train_probs[:, i])\n",
    "            axes[0, 0].plot(fpr, tpr, label=f'{class_name} (AUC={roc_auc:.2f})', color=colors[class_name])\n",
    "        \n",
    "        # Line for random guess and a legend with weighted AUC\n",
    "        axes[0, 0].plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "        axes[0, 0].set_title(f'Train - merged ROC curves and overall weighted ROC-AUC score = {roc_auc_weighted_train:.2f}')\n",
    "        axes[0, 0].set_xlabel('False Positive Rate')\n",
    "        axes[0, 0].set_ylabel('True Positive Rate')\n",
    "        axes[0, 0].legend()\n",
    "\n",
    "        # Plot class-specific ROC curves for Validation\n",
    "        for i, class_name in enumerate(classes):\n",
    "            fpr, tpr, _ = roc_curve((val_labels == i).astype(int), val_probs[:, i])\n",
    "            roc_auc = roc_auc_score((val_labels == i).astype(int), val_probs[:, i])\n",
    "            axes[0, 1].plot(fpr, tpr, label=f'{class_name} (AUC={roc_auc:.2f})', color=colors[class_name])\n",
    "        \n",
    "        # Line for random guess and a legend with weighted AUC\n",
    "        axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "        axes[0, 1].set_title(f'Validation - merged ROC curves and overall weighted ROC-AUC score = {roc_auc_weighted_val:.2f}')\n",
    "        axes[0, 1].set_xlabel('False Positive Rate')\n",
    "        axes[0, 1].set_ylabel('True Positive Rate')\n",
    "        axes[0, 1].legend()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting weighted ROC-AUC: {e}\")\n",
    "\n",
    "    # Rows 2-4: Class-specific ROC-AUC curves (Train and Validation)\n",
    "    for i, class_name in enumerate(classes):\n",
    "        # Class-specific ROC-AUC for Train\n",
    "        try:\n",
    "            fpr_train, tpr_train, _ = roc_curve((train_labels == i).astype(int), train_probs[:, i])\n",
    "            roc_auc_train = roc_auc_score((train_labels == i).astype(int), train_probs[:, i])\n",
    "            axes[i + 1, 0].plot(fpr_train, tpr_train, label=f'AUC={roc_auc_train:.2f}', color=colors[class_name])\n",
    "            axes[i + 1, 0].plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "            axes[i + 1, 0].set_title(f'Train - {class_name} ROC Curve')\n",
    "            axes[i + 1, 0].set_xlabel('False Positive Rate')\n",
    "            axes[i + 1, 0].set_ylabel('True Positive Rate')\n",
    "            axes[i + 1, 0].legend()\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting ROC-AUC for {class_name} (Train): {e}\")\n",
    "\n",
    "        # Class-specific ROC-AUC for Validation\n",
    "        try:\n",
    "            fpr_val, tpr_val, _ = roc_curve((val_labels == i).astype(int), val_probs[:, i])\n",
    "            roc_auc_val = roc_auc_score((val_labels == i).astype(int), val_probs[:, i])\n",
    "            axes[i + 1, 1].plot(fpr_val, tpr_val, label=f'{class_name} (AUC={roc_auc:.2f})', color=colors[class_name])\n",
    "            axes[i + 1, 1].plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "            axes[i + 1, 1].set_title(f'Validation - {class_name} ROC Curve')\n",
    "            axes[i + 1, 1].set_xlabel('False Positive Rate')\n",
    "            axes[i + 1, 1].set_ylabel('True Positive Rate')\n",
    "            axes[i + 1, 1].legend()\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting ROC-AUC for {class_name} (Validation): {e}\")\n",
    "\n",
    "    # Adjust layout\n",
    "    fig.suptitle(f'Classification model {identifier} - Weighted ROC Curves (Train / Val)', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(plot_paths['box2_roc_auc'])\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"»»» Combined ROC-AUC plot saved at {plot_paths['box2_roc_auc']}\")\n",
    "\n",
    "    #----------------\n",
    "    # # Define metrics dictionary\n",
    "    metrics = {\n",
    "        \"MCC (Train)\": round(matthews_corrcoef(train_labels, train_preds), 5),\n",
    "        \"MCC (Validation)\": round(matthews_corrcoef(val_labels, val_preds), 5),\n",
    "        \"Balanced Accuracy (Train)\": round(balanced_accuracy_score(train_labels, train_preds), 5),\n",
    "        \"Balanced Accuracy (Validation)\": round(balanced_accuracy_score(val_labels, val_preds), 5),\n",
    "        \"Weighted F1 Score (Train)\": round(f1_score(train_labels, train_preds, average='weighted'), 5),\n",
    "        \"Weighted F1 Score (Validation)\": round(f1_score(val_labels, val_preds, average='weighted'), 5)\n",
    "    }\n",
    "\n",
    "    # # Convert to DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics, index=[0]).T\n",
    "\n",
    "    # Create figure with subplots (left for table, right for loss curves)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # Adjust size for both table and plot\n",
    "\n",
    "    # Extract the last loss values\n",
    "    final_train_loss = results['train_loss'][-1] if results['train_loss'] else None\n",
    "    final_val_loss = results['val_loss'][-1] if results['val_loss'] else None\n",
    "    \n",
    "    # Plotting Train vs Validation Loss on the right side (axes[1])\n",
    "    axes[1].plot(epochs, train_loss, label=f'Train Loss ({final_train_loss:.4f})', color='orange')\n",
    "    axes[1].plot(epochs, val_loss, label=f'Val Loss ({final_val_loss:.4f})', color='blue')\n",
    "\n",
    "    # Adding labels and title for loss plot\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend(loc='upper right')\n",
    "\n",
    "    #--\n",
    "    epoch_ticks = list(range(1, len(epochs) + 1, 10))\n",
    "\n",
    "    # Ensure the final epoch is included\n",
    "    if epoch_ticks[-1] != len(epochs):\n",
    "        epoch_ticks.append(len(epochs))\n",
    "\n",
    "    # Adjust ticks to subtract 1 from all except first and last\n",
    "    adjusted_ticks = [tick - 1 if tick != 1 and tick != len(epochs) else tick for tick in epoch_ticks]\n",
    "\n",
    "    # Apply to the axes[1] plot\n",
    "    axes[1].set_xticks(adjusted_ticks)\n",
    "    axes[1].set_xticklabels([str(epoch) for epoch in adjusted_ticks])\n",
    "    #--\n",
    "\n",
    "    # Left: Metrics table without header\n",
    "    axes[0].axis('off')\n",
    "    table = axes[0].table(cellText=metrics_df.values, rowLabels=metrics_df.index, cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width(col=list(range(len(metrics_df.columns))))\n",
    "\n",
    "    # Add overall title for the figure\n",
    "    fig.suptitle(f'Classification model {identifier} - Single value metrics and Loss graph (Train / Val)', fontsize=16)\n",
    "\n",
    "    # Save the combined figure\n",
    "    plt.savefig(plot_paths['box3_loss_and_value_metrics'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Print confirmation\n",
    "    print(f\"»»» Combined (loss plot + metrics table) image saved at {plot_paths['box3_loss_and_value_metrics']}\")\n",
    "\n",
    "# Testing Functions\n",
    "def classif_run_testing(test_model, test_criterion, test_dataloader, storing_path, identifier, device):\n",
    "    test_model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    print(\"»»» Starting Classification Network Testing process \\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            # Transfer Data to Device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = test_model(images)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = test_criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Compute Predictions and Probabilities\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.detach().cpu().numpy().tolist())\n",
    "\n",
    "    # Calculate Confusion Matrix for Multiclass Classification\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Calculate Sensitivity and Specificity per class\n",
    "    sensitivity = {}\n",
    "    specificity = {}\n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]  # True Positive: Diagonal elements\n",
    "        fn = cm[i, :].sum() - tp  # False Negative: Row sum minus TP\n",
    "        fp = cm[:, i].sum() - tp  # False Positive: Column sum minus TP\n",
    "        tn = cm.sum() - (tp + fn + fp)  # True Negative: Total sum minus FP, FN, TP\n",
    "        \n",
    "        sensitivity[i] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity[i] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    results_filename = f\"logs_{identifier}_test.pkl\"\n",
    "    results_path = os.path.join(storing_path, results_filename)\n",
    "\n",
    "    results = {\n",
    "        'test_loss': total_loss / len(test_dataloader.dataset),\n",
    "        'test_labels': all_labels,  # True labels\n",
    "        'test_preds': all_preds,    # Predicted labels\n",
    "        'test_probs': all_probs,    # Probabilities\n",
    "        'sensitivity': sensitivity, \n",
    "        'specificity': specificity,\n",
    "        'cm': cm.tolist()\n",
    "    }\n",
    "    \n",
    "    # Save results using pickle\n",
    "    with open(results_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    print(f\"\\n»»» Classification Network Testing process complete\\n»»» Logs saved at: {results_path}\")\n",
    "\n",
    "    return results_path\n",
    "\n",
    "def classif_plot_testing_results(results_path, classes):\n",
    "    if not os.path.exists(results_path):\n",
    "        print(f\"File not found: {results_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"»»» Data taken from path: {results_path}\")\n",
    "    with open(results_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    # Extract directory and identifier from results_path\n",
    "    directory_path = os.path.dirname(results_path)\n",
    "    identifier = os.path.basename(results_path).split('_')[1]  # Adjust if needed\n",
    "\n",
    "    # Define plot paths for the images\n",
    "    plot_paths = {\n",
    "        'box1_pr': os.path.join(directory_path, f\"{identifier}_precision_recall_test.png\"),\n",
    "        'box2_roc_auc': os.path.join(directory_path, f\"{identifier}_roc_auc_test.png\"),\n",
    "        'box3_value_metrics_cmatrix': os.path.join(directory_path, f\"{identifier}_value_metrics_cmatrix_test.png\")\n",
    "    }\n",
    "\n",
    "    # Extract data for test phase from results\n",
    "    test_loss = np.array(results.get('test_loss', []))  # Convert to NumPy array if it exists\n",
    "    test_labels = results['test_labels']\n",
    "    test_probs = np.array(results['test_probs'])\n",
    "    test_preds = results['test_preds']\n",
    "    cm = np.array(results['cm'])\n",
    "\n",
    "    # Define colors for each class\n",
    "    colors = {'benign': 'blue', 'malignant': 'orange', 'normal': 'green', 'overall': 'black'}\n",
    "\n",
    "    # Precision-Recall Curves\n",
    "    try:\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(10, 20))\n",
    "\n",
    "        # Overall Precision-Recall curve (just showing the AP value, no line)\n",
    "        binary_labels = np.array([1 if label in classes else 0 for label in test_labels])  # Combined binary labels\n",
    "        avg_precision = average_precision_score(binary_labels, test_probs.mean(axis=1)) if np.any(binary_labels) else 0.0\n",
    "        axes[0].set_title(f'Test - merged PR curves and overall weighted AP score = {avg_precision:.2f}')  # Title only with the overall value\n",
    "        axes[0].set_xlabel('Recall')\n",
    "        axes[0].set_ylabel('Precision')\n",
    "\n",
    "        # Class-specific Precision-Recall curves in separate plots\n",
    "        for i, class_name in enumerate(classes):\n",
    "            binary_labels = np.where(np.array(test_labels) == i, 1, 0)\n",
    "            \n",
    "            # Ensure both positive and negative samples for AP calculation\n",
    "            if np.any(binary_labels):  # Only compute if there are positive samples\n",
    "                precision, recall, _ = precision_recall_curve(binary_labels, test_probs[:, i])\n",
    "                avg_precision = average_precision_score(binary_labels, test_probs[:, i]) if np.any(binary_labels) else 0.0\n",
    "            else:\n",
    "                precision, recall = [0], [1]  # No positive samples, default to no PR curve\n",
    "                avg_precision = 0.0  # Assign default value for AP\n",
    "            \n",
    "            # Plot each class curve\n",
    "            axes[0].plot(recall, precision, label=f'{class_name} (AP={avg_precision:.2f})', color=colors[class_name])\n",
    "\n",
    "        axes[0].legend()\n",
    "\n",
    "        # Class-specific Precision-Recall curves in separate plots\n",
    "        for i, class_name in enumerate(classes):\n",
    "            binary_labels = np.where(np.array(test_labels) == i, 1, 0)\n",
    "            \n",
    "            # Ensure both positive and negative samples for AP calculation\n",
    "            if np.any(binary_labels):  # Only compute positive samples\n",
    "                precision, recall, _ = precision_recall_curve(binary_labels, test_probs[:, i])\n",
    "                avg_precision = average_precision_score(binary_labels, test_probs[:, i]) if np.any(binary_labels) else 0.0\n",
    "            else:\n",
    "                precision, recall = [0], [1]  # No positive samples, default to no PR curve\n",
    "                avg_precision = 0.0  # Assign a default value for AP\n",
    "\n",
    "            axes[i + 1].plot(recall, precision, label=f'{class_name} (AP={avg_precision:.2f})', color=colors[class_name])\n",
    "            axes[i + 1].set_title(f'Test - {class_name} PR Curve')\n",
    "            axes[i + 1].set_xlabel('Recall')\n",
    "            axes[i + 1].set_ylabel('Precision')\n",
    "            axes[i + 1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.95)\n",
    "        fig.suptitle(f'Classification model {identifier} - Weighted Precision-Recall Curves (Test)', fontsize=16)\n",
    "        plt.savefig(plot_paths['box1_pr'])\n",
    "        plt.close()\n",
    "        print(f\"»»» Precision-Recall curve saved at {plot_paths['box1_pr']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting Precision-Recall curves: {e}\")\n",
    "\n",
    "\n",
    "    # ROC-AUC Curves\n",
    "    try:\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(10, 20))\n",
    "\n",
    "        roc_auc_weighted_test = roc_auc_score(test_labels, test_probs, multi_class='ovr', average='weighted')\n",
    "\n",
    "        # Overall ROC-AUC curve (including the random guess line)\n",
    "        axes[0].plot([0, 1], [0, 1], 'k--', label='Random Guess')  # Random guess line\n",
    "        for i, class_name in enumerate(classes):\n",
    "            binary_labels = np.where(np.array(test_labels) == i, 1, 0)\n",
    "            fpr, tpr, _ = roc_curve(binary_labels, test_probs[:, i])\n",
    "            roc_auc = roc_auc_score(binary_labels, test_probs[:, i])\n",
    "\n",
    "            # Plot each class curve in the top plot\n",
    "            axes[0].plot(fpr, tpr, label=f'{class_name} (AUC={roc_auc:.2f})', color=colors[class_name])\n",
    "\n",
    "        axes[0].set_title(f'Test - merged ROC curves and overall weighted ROC-AUC score = {roc_auc_weighted_test:.2f}')\n",
    "        axes[0].set_xlabel('False Positive Rate')\n",
    "        axes[0].set_ylabel('True Positive Rate')\n",
    "        axes[0].legend()\n",
    "\n",
    "        # Class-specific ROC-AUC curves in separate plots (including random guess line)\n",
    "        for i, class_name in enumerate(classes):\n",
    "            binary_labels = np.where(np.array(test_labels) == i, 1, 0)\n",
    "            fpr, tpr, _ = roc_curve(binary_labels, test_probs[:, i])\n",
    "            roc_auc = roc_auc_score(binary_labels, test_probs[:, i])\n",
    "\n",
    "            # Plot each class curve in individual plots\n",
    "            axes[i + 1].plot(fpr, tpr, label=f'{class_name} (AUC={roc_auc:.2f})', color=colors[class_name])\n",
    "            axes[i + 1].plot([0, 1], [0, 1], 'k--', label='Random Guess')  # Random guess line\n",
    "            axes[i + 1].set_title(f'Test - {class_name} ROC Curve')\n",
    "            axes[i + 1].set_xlabel('False Positive Rate')\n",
    "            axes[i + 1].set_ylabel('True Positive Rate')\n",
    "            axes[i + 1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.95)\n",
    "        fig.suptitle(f'Classification model {identifier} - Weighted ROC Curves (Test)', fontsize=16)\n",
    "        plt.savefig(plot_paths['box2_roc_auc'])\n",
    "        plt.close()\n",
    "        print(f\"»»» ROC-AUC curve saved at {plot_paths['box2_roc_auc']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting ROC curves: {e}\")\n",
    "\n",
    "    # Test Metrics\n",
    "    # Extract data for test phase from results\n",
    "    test_loss = results.get('test_loss', 0)  # Final test loss\n",
    "    sensitivity = results['sensitivity']\n",
    "    specificity = results['specificity']\n",
    "\n",
    "    # Define metrics for the table\n",
    "    value_metrics = {\n",
    "        \"MCC\": round(matthews_corrcoef(test_labels, test_preds), 5),\n",
    "        \"Balanced Accuracy\": round(balanced_accuracy_score(test_labels, test_preds), 5),\n",
    "        \"Weighted F1 Score\": round(f1_score(test_labels, test_preds, average='weighted'), 5),\n",
    "        \"Final Loss Value\": round(test_loss, 5)\n",
    "    }\n",
    "    \n",
    "    # Adding Sensitivity and Specificity per class\n",
    "    for i, class_name in enumerate(classes):\n",
    "        value_metrics[f\"Sensitivity ({class_name})\"] = round(sensitivity.get(i, 0), 5)\n",
    "        value_metrics[f\"Specificity ({class_name})\"] = round(specificity.get(i, 0), 5)\n",
    "\n",
    "    metrics_df = pd.DataFrame(value_metrics, index=[0]).T\n",
    "\n",
    "    # Plotting the metrics table and confusion matrix\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))  # Two subplots side by side\n",
    "    ax1.axis('off')  # Turn off axis for table\n",
    "    table = ax1.table(cellText=metrics_df.values, rowLabels=metrics_df.index, loc='center')\n",
    "    table.auto_set_column_width([0, 1, 2])  # Adjust columns for the metrics and values\n",
    "\n",
    "    fig.suptitle(f'Classification model {identifier} - Single value metrics and Confusion Matrix (Test)', fontsize=16, ha='center')\n",
    "\n",
    "    # Plot the confusion matrix on the second axis (ax2)\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    cm_display.plot(cmap='Blues', ax=ax2)\n",
    "\n",
    "    # Adjust the layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(plot_paths['box3_value_metrics_cmatrix'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"»»» Metrics table and confusion matrix saved at {plot_paths['box3_value_metrics_cmatrix']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10. Setting Finetune / Test / Plotting functions specific to segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Old, keep here though\\n# def segm_run_testing(test_model, test_criterion, test_dataloader, storing_path, identifier, device):\\n#     test_model.eval()\\n    \\n#     # Initialize variables for the overall test pass\\n#     all_preds = []\\n#     all_labels = []\\n#     correct_pixels = 0\\n#     total_pixels = 0\\n#     running_loss = 0.0\\n\\n#     # Initialize Hausdorff-specific variables\\n#     hausdorff_preds = []  # Store coordinates of predicted regions\\n#     hausdorff_labels = []  # Store coordinates of ground truth regions\\n\\n#     # Initialize a dictionary to store results\\n#     results = {\\n#         \\'test_loss\\': [],\\n#         \\'test_iou\\': [],\\n#         \\'test_pixel_accuracy\\': [],\\n#         \\'test_dice\\': [],\\n#         \\'test_labels\\': [],\\n#         \\'test_preds\\': [],\\n#         \\'test_inputs\\': [],\\n#         \\'test_hausdorff\\': []\\n#     }\\n\\n#     print(\"»»» Starting Segmentation Network Testing process \\n\")\\n\\n#     for batch_idx, (inputs, labels) in enumerate(test_dataloader):\\n#         # Move data to device\\n#         inputs, labels = inputs.to(device), labels.to(device)\\n                \\n#         # Forward pass\\n#         outputs = test_model(inputs)\\n\\n#         # Compute loss\\n#         loss = test_criterion(outputs, labels)\\n#         running_loss += loss.item() * inputs.size(0)\\n\\n#         # Threshold predictions (binarization or softmax)\\n#         binary_preds = (outputs > 0.5).float()\\n\\n#         # Update pixel accuracy counts\\n#         correct_pixels += (binary_preds == labels).sum().item()\\n#         total_pixels += labels.numel()\\n\\n#         # Store data for final evaluation visualization\\n#         results[\\'test_preds\\'].append(binary_preds.cpu().numpy())\\n#         results[\\'test_labels\\'].append(labels.cpu().numpy())\\n#         results[\\'test_inputs\\'].append(inputs.cpu().numpy())\\n\\n#         # Flatten and accumulate predictions and labels for global metrics\\n#         all_preds.extend(binary_preds.cpu().numpy().astype(int).flatten())\\n#         all_labels.extend(labels.cpu().numpy().astype(int).flatten())\\n\\n#         # Prepare coordinates for Hausdorff distance calculation\\n#         for pred, label in zip(binary_preds.cpu().numpy(), labels.cpu().numpy()):\\n#             hausdorff_preds.append(np.argwhere(pred[0]))  # Binary prediction to coordinates\\n#             hausdorff_labels.append(np.argwhere(label[0]))  # Binary label to coordinates\\n\\n#     # Calculate the overall metrics (IoU, Dice, etc.)\\n#     test_loss = running_loss / len(test_dataloader.dataset)\\n#     test_pixel_accuracy = correct_pixels / total_pixels\\n#     test_iou = jaccard_score(all_labels, all_preds, average=\\'binary\\')\\n#     test_dice = f1_score(all_labels, all_preds, average=\\'binary\\')\\n\\n#     # Store global metrics\\n#     results[\\'test_loss\\'].append(test_loss)\\n#     results[\\'test_iou\\'].append(test_iou)\\n#     results[\\'test_pixel_accuracy\\'].append(test_pixel_accuracy)\\n#     results[\\'test_dice\\'].append(test_dice)\\n\\n#     # Compute Hausdorff distance\\n#     hausdorff_vals = []\\n#     for pred_coords, label_coords in zip(hausdorff_preds, hausdorff_labels):\\n#         forward_hd = directed_hausdorff(pred_coords, label_coords)[0]\\n#         reverse_hd = directed_hausdorff(label_coords, pred_coords)[0]\\n#         hausdorff_vals.append(max(forward_hd, reverse_hd))\\n\\n#     # Store the Hausdorff distance for the final epoch\\n#     final_hausdorff = np.mean(hausdorff_vals)  # Average over all test samples\\n#     results[\\'test_hausdorff\\'].append(final_hausdorff)\\n\\n#     results_filename = f\"logs_{identifier}_test.pkl\"\\n#     results_path = os.path.join(storing_path, results_filename)\\n\\n#     # Save results using pickle\\n#     with open(results_path, \\'wb\\') as f:\\n#         pickle.dump(results, f)\\n\\n#     print(f\"\\n»»» Segmentation Network Testing process complete\\n»»» Logs saved at: {results_path}\")\\n\\n#     return results_path\\n'"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finetuning Functions\n",
    "def segm_train_one_epoch(finetune_model, finetune_criterion, train_dataloader, finetune_optimizer, device):\n",
    "    finetune_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        # Move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        finetune_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = finetune_model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = finetune_criterion(outputs, labels)\n",
    "\n",
    "        # Accumulate loss, and the inputs.size(0) ensures loss contribution is weighted by number of samples in batch\n",
    "        running_loss += loss.item() * inputs.size(0) \n",
    "\n",
    "        # Backward pass and optimizer step\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to a max norm\n",
    "        torch.nn.utils.clip_grad_norm_(finetune_model.parameters(), max_norm=0.5)  \n",
    "\n",
    "        finetune_optimizer.step()\n",
    "\n",
    "        # Detach predictions and accumulate for IoU/Dice\n",
    "        binary_preds = (outputs.detach() > 0.5).float()\n",
    "        correct_pixels += (binary_preds == labels).sum().item()\n",
    "        total_pixels += labels.numel()\n",
    "\n",
    "        # Store predictions and labels\n",
    "        all_preds.extend(binary_preds.cpu().numpy().astype(int).flatten())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).flatten())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataloader.dataset)\n",
    "    train_iou = jaccard_score(all_labels, all_preds, average='binary')\n",
    "    train_pixel_accuracy = correct_pixels / total_pixels\n",
    "\n",
    "    return train_loss, train_iou, train_pixel_accuracy\n",
    "\n",
    "def segm_validate_one_epoch(finetune_model, finetune_criterion, val_dataloader, device, is_last_epoch):\n",
    "    finetune_model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    final_epoch_preds = []\n",
    "    final_epoch_labels = []\n",
    "    final_epoch_inputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(val_dataloader):\n",
    "            # Move data to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "            # Forward pass\n",
    "            outputs = finetune_model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = finetune_criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Threshold predictions (binarization or softmax)\n",
    "            binary_preds = (outputs > 0.5).float()\n",
    "            correct_pixels += (binary_preds == labels).sum().item()\n",
    "            total_pixels += labels.numel()\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(binary_preds.cpu().numpy().astype(int).flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int).flatten())\n",
    "\n",
    "            # If it's the last epoch, store data for example views (images, masks, predictions)\n",
    "            if is_last_epoch:\n",
    "                final_epoch_preds.append(binary_preds.cpu().numpy())\n",
    "                final_epoch_labels.append(labels.cpu().numpy())\n",
    "                final_epoch_inputs.append(inputs.cpu().numpy())  # Store original inputs (images)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    if is_last_epoch:\n",
    "        # Safely concatenate final epoch data only if non-empty\n",
    "        final_epoch_preds = np.concatenate(final_epoch_preds, axis=0) if final_epoch_preds else np.array([])\n",
    "        final_epoch_labels = np.concatenate(final_epoch_labels, axis=0) if final_epoch_labels else np.array([])\n",
    "        final_epoch_inputs = np.concatenate(final_epoch_inputs, axis=0) if final_epoch_inputs else np.array([])\n",
    "\n",
    "    val_loss = running_loss / len(val_dataloader.dataset)\n",
    "    val_pixel_accuracy = correct_pixels / total_pixels\n",
    "    val_iou = jaccard_score(all_labels, all_preds, average='binary')\n",
    "    val_dice = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    return val_loss, val_iou, val_dice, val_pixel_accuracy, all_labels, all_preds, final_epoch_labels, final_epoch_preds, final_epoch_inputs\n",
    "\n",
    "def segm_run_finetuning(finetune_model, finetune_criterion, train_dataloader, val_dataloader, finetune_optimizer, finetune_scheduler, device, total_epoch_count, storing_path, identifier, checkpoint_interval):\n",
    "\n",
    "    results_path = None\n",
    "    results = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'train_iou': [],\n",
    "        'train_pixel_accuracy': [],\n",
    "        'val_loss': [],\n",
    "        'val_iou': [],\n",
    "        'val_dice': [],\n",
    "        'val_pixel_accuracy': [],\n",
    "        'val_labels': [],\n",
    "        'val_preds': [],\n",
    "        'final_epoch_labels': [],\n",
    "        'final_epoch_preds': [],\n",
    "        'final_epoch_inputs': []\n",
    "    }\n",
    "\n",
    "    print(\"»»» Starting Segmentation Network Finetuning process\")\n",
    "\n",
    "    for epoch in range(total_epoch_count):\n",
    "        # Training\n",
    "        avg_train_loss, train_iou, train_pixel_accuracy = segm_train_one_epoch(finetune_model, finetune_criterion, train_dataloader, finetune_optimizer, device)\n",
    "\n",
    "        # Validation\n",
    "        if (epoch + 1) != total_epoch_count:\n",
    "            avg_val_loss, val_iou, val_dice, val_pixel_accuracy, val_labels, val_preds, final_epoch_labels, final_epoch_preds, final_epoch_inputs = segm_validate_one_epoch(finetune_model, finetune_criterion, val_dataloader, device, is_last_epoch = False)\n",
    "        else:\n",
    "            avg_val_loss, val_iou, val_dice, val_pixel_accuracy, val_labels, val_preds, final_epoch_labels, final_epoch_preds, final_epoch_inputs = segm_validate_one_epoch(finetune_model, finetune_criterion, val_dataloader, device, is_last_epoch = True)\n",
    "\n",
    "            results['final_epoch_labels'].append(final_epoch_labels)\n",
    "            results['final_epoch_preds'].append(final_epoch_preds)\n",
    "            results['final_epoch_inputs'].append(final_epoch_inputs)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{total_epoch_count}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        results['epoch'].append(epoch + 1)\n",
    "        results['train_loss'].append(avg_train_loss)\n",
    "        results['train_iou'].append(train_iou)\n",
    "        results['train_pixel_accuracy'].append(train_pixel_accuracy)\n",
    "        results['val_loss'].append(avg_val_loss)\n",
    "        results['val_iou'].append(val_iou)\n",
    "        results['val_dice'].append(val_dice)\n",
    "        results['val_pixel_accuracy'].append(val_pixel_accuracy)\n",
    "        results['val_labels'].append(val_labels)\n",
    "        results['val_preds'].append(val_preds)\n",
    "   \n",
    "        if finetune_scheduler:\n",
    "            finetune_scheduler.step(avg_val_loss)\n",
    "\n",
    "        if (epoch + 1) % checkpoint_interval == 0 or (epoch + 1) == total_epoch_count:\n",
    "            print(f\"»»» Checkpoint reached at epoch {(epoch + 1)}: Saving model state and results\")\n",
    "\n",
    "            try:\n",
    "                results_path = results_save(identifier, \"ft\", storing_path, results, epoch, total_epoch_count)\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "                return None\n",
    "            \n",
    "            # Clear results to avoid duplication\n",
    "            results = {key: [] for key in results.keys()}\n",
    "\n",
    "    print(\"\\n»»» Segmentation Network Finetuning process complete\\n»»» Logs saved at: \", results_path)\n",
    "      \n",
    "    return results_path # Path of files with model and results\n",
    "\n",
    "#---\n",
    "#auxilliary functions\n",
    "def categorize_and_select_examples_iou_ft(inputs, labels, preds):\n",
    "    best_box = []\n",
    "    worst_box = []\n",
    "    normal_case_box = []\n",
    "    all_examples = []\n",
    "\n",
    "    # Categorize examples\n",
    "    for idx in range(len(inputs)):\n",
    "        label_image = labels[idx, 0]\n",
    "        pred_image = preds[idx, 0]\n",
    "\n",
    "        # Calculate Intersection and Union for IoU calculation\n",
    "        intersection = np.sum((label_image == 1) & (pred_image == 1))\n",
    "        union = np.sum((label_image == 1) | (pred_image == 1))\n",
    "\n",
    "        # Handle edge case where both mask and prediction are 0s\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "\n",
    "        # Identify normal cases (no tumor, both mask and prediction are all 0s)\n",
    "        if np.sum(label_image) == 0 and np.sum(pred_image) == 0:\n",
    "            normal_case_box.append((idx, iou))  # Consider as perfect IoU for no tumor\n",
    "        else:\n",
    "            all_examples.append((idx, iou))\n",
    "\n",
    "    # Sort all examples by IoU score\n",
    "    sorted_examples = sorted(all_examples, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Split into best and worst boxes (top 20% and bottom 20%)\n",
    "    top_n = min(20, len(sorted_examples))\n",
    "    bottom_n = min(20, len(sorted_examples))\n",
    "    best_box = sorted_examples[:top_n]\n",
    "    worst_box = sorted_examples[-bottom_n:]\n",
    "\n",
    "    # Collect indices of all examples\n",
    "    best_indices = {i for i, _ in best_box}\n",
    "    worst_indices = {i for i, _ in worst_box}\n",
    "    normal_indices = {i for i, _ in normal_case_box}\n",
    "\n",
    "    # Identify average cases\n",
    "    average_box = [\n",
    "        (i, iou) for i, iou in all_examples if i not in best_indices and i not in worst_indices\n",
    "    ]\n",
    "\n",
    "    # Select 3 examples randomly from each box, ensuring total selection of 9 examples\n",
    "    selected_best = random.sample(best_box + normal_case_box, min(3, len(best_box + normal_case_box)))\n",
    "    selected_worst = random.sample(worst_box, min(3, len(worst_box)))\n",
    "    selected_average = random.sample(average_box, min(3, len(average_box)))\n",
    "\n",
    "    # Return the combined selection\n",
    "    return selected_best, selected_average, selected_worst\n",
    "\n",
    "def categorize_and_select_examples_dice_ft(final_epoch_labels, final_epoch_preds):\n",
    "    best_box = []\n",
    "    worst_box = []\n",
    "    normal_case_box = []\n",
    "    all_examples = []\n",
    "\n",
    "    # Categorize examples\n",
    "    for i in range(len(final_epoch_labels)):\n",
    "        label = final_epoch_labels[i, 0]\n",
    "        pred = final_epoch_preds[i, 0]\n",
    "        label_flat = label.flatten()\n",
    "        pred_flat = pred.flatten()\n",
    "\n",
    "        # Identify normal cases\n",
    "        if np.sum(label_flat) == 0 and np.sum(pred_flat) == 0:\n",
    "            normal_case_box.append((i, 0.0))  # Dice score is 0 for no tumor\n",
    "        else:\n",
    "            dice_score = f1_score(label_flat, pred_flat, average='binary', zero_division=1)\n",
    "            all_examples.append((i, dice_score))\n",
    "\n",
    "    # Sort examples by Dice score\n",
    "    sorted_examples = sorted(all_examples, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Split into best and worst boxes (e.g., top 20% and bottom 20%)\n",
    "    top_n = min(20, len(sorted_examples))\n",
    "    bottom_n = min(20, len(sorted_examples))\n",
    "    best_box = sorted_examples[:top_n]\n",
    "    worst_box = sorted_examples[-bottom_n:]\n",
    "\n",
    "    # Collect indices of all examples\n",
    "    best_indices = {i for i, _ in best_box}\n",
    "    worst_indices = {i for i, _ in worst_box}\n",
    "    normal_indices = {i for i, _ in normal_case_box}\n",
    "\n",
    "    # Identify average cases\n",
    "    average_box = [\n",
    "        (i, dice) for i, dice in all_examples if i not in best_indices and i not in worst_indices\n",
    "    ]\n",
    "\n",
    "    # Select 3 examples randomly from each box\n",
    "    selected_best = random.sample(best_box + normal_case_box, min(3, len(best_box + normal_case_box)))\n",
    "    selected_worst = random.sample(worst_box, min(3, len(worst_box)))\n",
    "    selected_average = random.sample(average_box, min(3, len(average_box)))\n",
    "\n",
    "    # Return the combined selection\n",
    "    return selected_best, selected_average, selected_worst\n",
    "\n",
    "def categorize_and_select_examples_iou_test(labels, preds):\n",
    "    best_box = []\n",
    "    worst_box = []\n",
    "    normal_case_box = []\n",
    "    all_examples = []\n",
    "\n",
    "    # Categorize examples\n",
    "    for idx in range(len(labels)):\n",
    "        label_image = labels[idx, 0]\n",
    "        pred_image = preds[idx, 0]\n",
    "\n",
    "        # Calculate Intersection and Union for IoU calculation\n",
    "        intersection = np.sum((label_image == 1) & (pred_image == 1))\n",
    "        union = np.sum((label_image == 1) | (pred_image == 1))\n",
    "\n",
    "        # Handle edge case where both mask and prediction are 0s\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "\n",
    "        # Identify normal cases (no tumor, both mask and prediction are all 0s)\n",
    "        if np.sum(label_image) == 0 and np.sum(pred_image) == 0:\n",
    "            normal_case_box.append((idx, iou))  # Consider as perfect IoU for no tumor\n",
    "        else:\n",
    "            all_examples.append((idx, iou))\n",
    "\n",
    "    # Sort all examples by IoU score\n",
    "    sorted_examples = sorted(all_examples, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Split into best and worst boxes (top 20% and bottom 20%)\n",
    "    top_n = min(20, len(sorted_examples))\n",
    "    bottom_n = min(20, len(sorted_examples))\n",
    "    best_box = sorted_examples[:top_n]\n",
    "    worst_box = sorted_examples[-bottom_n:]\n",
    "\n",
    "    # Collect indices of all examples\n",
    "    best_indices = {i for i, _ in best_box}\n",
    "    worst_indices = {i for i, _ in worst_box}\n",
    "    normal_indices = {i for i, _ in normal_case_box}\n",
    "\n",
    "    # Identify average cases\n",
    "    average_box = [\n",
    "        (i, iou) for i, iou in all_examples if i not in best_indices and i not in worst_indices\n",
    "    ]\n",
    "\n",
    "    # Select 3 examples randomly from each box, ensuring total selection of 9 examples\n",
    "    selected_best = random.sample(best_box + normal_case_box, min(3, len(best_box + normal_case_box)))\n",
    "    selected_worst = random.sample(worst_box, min(3, len(worst_box)))\n",
    "    selected_average = random.sample(average_box, min(3, len(average_box)))\n",
    "\n",
    "    # Return the combined selection\n",
    "    return selected_best, selected_average, selected_worst\n",
    "\n",
    "def categorize_and_select_examples_dice_test(labels, preds):\n",
    "    best_box = []\n",
    "    worst_box = []\n",
    "    normal_case_box = []\n",
    "    all_examples = []\n",
    "\n",
    "    # Categorize examples\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i, 0]\n",
    "        pred = preds[i, 0]\n",
    "        label_flat = label.flatten()\n",
    "        pred_flat = pred.flatten()\n",
    "\n",
    "        # Identify normal cases\n",
    "        if np.sum(label_flat) == 0 and np.sum(pred_flat) == 0:\n",
    "            normal_case_box.append((i, 0.0))  # Dice score is 0 for no tumor\n",
    "        else:\n",
    "            dice_score = f1_score(label_flat, pred_flat, average='binary', zero_division=1)\n",
    "            all_examples.append((i, dice_score))\n",
    "\n",
    "    # Sort examples by Dice score\n",
    "    sorted_examples = sorted(all_examples, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Split into best and worst boxes (e.g., top 20% and bottom 20%)\n",
    "    top_n = min(20, len(sorted_examples))\n",
    "    bottom_n = min(20, len(sorted_examples))\n",
    "    best_box = sorted_examples[:top_n]\n",
    "    worst_box = sorted_examples[-bottom_n:]\n",
    "\n",
    "    # Collect indices of all examples\n",
    "    best_indices = {i for i, _ in best_box}\n",
    "    worst_indices = {i for i, _ in worst_box}\n",
    "    normal_indices = {i for i, _ in normal_case_box}\n",
    "\n",
    "    # Identify average cases\n",
    "    average_box = [\n",
    "        (i, dice) for i, dice in all_examples if i not in best_indices and i not in worst_indices\n",
    "    ]\n",
    "\n",
    "    # Select 3 examples randomly from each box\n",
    "    selected_best = random.sample(best_box + normal_case_box, min(3, len(best_box + normal_case_box)))\n",
    "    selected_worst = random.sample(worst_box, min(3, len(worst_box)))\n",
    "    selected_average = random.sample(average_box, min(3, len(average_box)))\n",
    "\n",
    "    # Return the combined selection\n",
    "    return selected_best, selected_average, selected_worst\n",
    "\n",
    "#---\n",
    "\n",
    "def segm_plot_finetuning_results(results_path):\n",
    "    if not os.path.exists(results_path):\n",
    "        print(f\"File not found: {results_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"»»» Data taken from path: {results_path}\")\n",
    "    with open(results_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    directory_path = os.path.dirname(results_path)\n",
    "    identifier = os.path.basename(results_path).split('_')[1]  # Adjust if needed\n",
    "\n",
    "    plot_paths = {\n",
    "        'box1_loss_iou_pxlacc': os.path.join(directory_path, f\"{identifier}_loss_iou_pxlacc_ft.png\"),\n",
    "        'box2_dice_prec_rec':os.path.join(directory_path, f\"{identifier}_dice_prec_rec_ft.png\"),\n",
    "        'box3_example_views_dice': os.path.join(directory_path, f\"{identifier}_example_views_dice_ft.png\"),\n",
    "        'box4_example_views_iou': os.path.join(directory_path, f\"{identifier}_example_views_iou_ft.png\")\n",
    "    }\n",
    "\n",
    "    epochs = np.array(results['epoch'])  # Epochs\n",
    "\n",
    "    # Extract loss data and last values\n",
    "    train_loss = np.array(results['train_loss'])  # Train Loss\n",
    "    val_loss = np.array(results['val_loss'])  # Validation Loss\n",
    "    train_loss_final = round(train_loss[-1], 5) if train_loss.size > 0 else None # Last value of Train Loss\n",
    "    val_loss_final = round(val_loss[-1], 5) if val_loss.size > 0 else None # Last value of Validation Loss\n",
    "\n",
    "    # Extract IoU data and last values\n",
    "    train_iou = np.array(results['train_iou'])\n",
    "    val_iou = np.array(results['val_iou'])\n",
    "    train_iou_final = round(train_iou[-1], 5) if train_iou.size > 0 else None  # Last value of Train IoU\n",
    "    val_iou_final = round(val_iou[-1], 5) if val_iou.size > 0 else None  # Last value of Validation IoU\n",
    "\n",
    "    # Extract Pixel Accuracy data and last values\n",
    "    train_pixel_accuracy = np.array(results['train_pixel_accuracy'])\n",
    "    val_pixel_accuracy = np.array(results['val_pixel_accuracy'])\n",
    "    train_pixel_accuracy_final = round(train_pixel_accuracy[-1], 5) if train_pixel_accuracy.size > 0 else None  # Last value of Train Pixel Accuracy\n",
    "    val_pixel_accuracy_final = round(val_pixel_accuracy[-1], 5) if val_pixel_accuracy.size > 0 else None  # Last value of Validation Pixel Accuracy\n",
    "    \n",
    "    #----\n",
    "    # Box 1\n",
    "\n",
    "    epoch_ticks = [1] + list(range(10, len(epochs) + 1, 10))\n",
    "    if epoch_ticks[-1] != len(epochs):\n",
    "        epoch_ticks.append(len(epochs))\n",
    "\n",
    "    # Create figure with subplots for the three graphs\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # Three plots side by side\n",
    "\n",
    "    # Plot Loss Graph on the left\n",
    "    axes[0].plot(epochs, train_loss, label=f'Train Loss ({train_loss_final})', color='orange')\n",
    "    axes[0].plot(epochs, val_loss, label=f'Val Loss ({val_loss_final})', color='blue')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend(loc='upper right')\n",
    "    axes[0].set_title('Loss')\n",
    "\n",
    "    axes[0].set_xticks(epoch_ticks)\n",
    "\n",
    "    # IoU Plot in the middle\n",
    "    axes[1].plot(epochs, train_iou, label=f'Train IoU ({train_iou_final})', color='orange')\n",
    "    axes[1].plot(epochs, val_iou, label=f'Val IoU ({val_iou_final})', color='blue')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('IoU')\n",
    "    axes[1].legend(loc='upper right')\n",
    "    axes[1].set_title('IoU')\n",
    "\n",
    "    axes[1].set_xticks(epoch_ticks)\n",
    "\n",
    "    # Pixel Accuracy Plot\n",
    "    axes[2].plot(epochs, train_pixel_accuracy, label=f'Train Pixel Acc ({train_pixel_accuracy_final})', color='orange')\n",
    "    axes[2].plot(epochs, val_pixel_accuracy, label=f'Val Pixel Acc ({val_pixel_accuracy_final})', color='blue')\n",
    "    axes[2].set_xlabel('Epochs')\n",
    "    axes[2].set_ylabel('Pixel Accuracy')\n",
    "    axes[2].legend(loc='lower right')\n",
    "    axes[2].set_title('Pixel Accuracy')\n",
    "\n",
    "    axes[2].set_xticks(epoch_ticks)\n",
    "\n",
    "    # Add overall title for the figure\n",
    "    fig.suptitle(f'Segmentation model {identifier} - Loss / IoU / Pixel Accuracy Graphs (Train / Val)', fontsize=16)\n",
    "\n",
    "    # Save the combined figure\n",
    "    plt.savefig(plot_paths['box1_loss_iou_pxlacc'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"»»» Loss / Intersection over union / Pixel accuracy image saved at {plot_paths['box1_loss_iou_pxlacc']}\")\n",
    "    \n",
    "    #---\n",
    "    # Box 2\n",
    "\n",
    "    val_labels = results['val_labels']\n",
    "    val_preds = results['val_preds']\n",
    "\n",
    "    # Initialize metric arrays\n",
    "    val_dice = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "\n",
    "    # Compute metrics for each epoch\n",
    "    for labels, preds in zip(val_labels, val_preds):\n",
    "        labels = np.array(labels)\n",
    "        preds = np.array(preds)\n",
    "\n",
    "        val_dice.append(f1_score(labels, preds, average='binary'))\n",
    "        val_precision.append(precision_score(labels, preds, average='binary'))\n",
    "        val_recall.append(recall_score(labels, preds, average='binary'))\n",
    "\n",
    "    # Convert metrics to numpy arrays for easier handling\n",
    "    val_dice = np.array(val_dice)\n",
    "    val_precision = np.array(val_precision)\n",
    "    val_recall = np.array(val_recall)\n",
    "\n",
    "    # Get last values for legends\n",
    "    val_dice_final = round(val_dice[-1], 5) if val_dice.size > 0 else None\n",
    "    val_precision_final = round(val_precision[-1], 5) if val_precision.size > 0 else None\n",
    "    val_recall_final = round(val_recall[-1], 5) if val_recall.size > 0 else None\n",
    "\n",
    "    # Create figure for validation metrics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Dice Graph\n",
    "    axes[0].plot(epochs, val_dice, label=f'Dice ({val_dice_final})', color='blue')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Dice Score')\n",
    "    axes[0].legend(loc='lower right')\n",
    "    axes[0].set_title('Dice Score')\n",
    "\n",
    "    axes[0].set_xticks(epoch_ticks)\n",
    "\n",
    "    # Precision Graph\n",
    "    axes[1].plot(epochs, val_precision, label=f'Precision ({val_precision_final})', color='green')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Precision')\n",
    "    axes[1].legend(loc='lower right')\n",
    "    axes[1].set_title('Precision')\n",
    "\n",
    "    axes[1].set_xticks(epoch_ticks)\n",
    "\n",
    "    # Recall Graph\n",
    "    axes[2].plot(epochs, val_recall, label=f'Recall ({val_recall_final})', color='purple')\n",
    "    axes[2].set_xlabel('Epochs')\n",
    "    axes[2].set_ylabel('Recall')\n",
    "    axes[2].legend(loc='lower right')\n",
    "    axes[2].set_title('Recall')\n",
    "\n",
    "    axes[2].set_xticks(epoch_ticks)\n",
    "\n",
    "    # Add overall title for the figure\n",
    "    fig.suptitle(f'Segmentation model {identifier} - Dice / Precision / Recall Graphs (Validation)', fontsize=16)\n",
    "\n",
    "    # Save the combined figure\n",
    "    plt.savefig(plot_paths['box2_dice_prec_rec'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"»»» Dice / Precision / Recall image saved at {plot_paths['box2_dice_prec_rec']}\")\n",
    "\n",
    "    #---\n",
    "    # Box 3\n",
    "   \n",
    "    # Extract the final epoch inputs, labels, and predictions\n",
    "    final_epoch_inputs = np.array(results['final_epoch_inputs'])  # [1, N, 1, H, W]\n",
    "    final_epoch_labels = np.array(results['final_epoch_labels'])  # [1, N, 1, H, W]\n",
    "    final_epoch_preds = np.array(results['final_epoch_preds'])    # [1, N, 1, H, W]\n",
    "\n",
    "    # Flatten the batch dimension\n",
    "    final_epoch_inputs = final_epoch_inputs[0]\n",
    "    final_epoch_labels = final_epoch_labels[0]\n",
    "    final_epoch_preds = final_epoch_preds[0]\n",
    "\n",
    "    # Example selection logic for Dice\n",
    "    selected_best_dice, selected_average_dice, selected_worst_dice = categorize_and_select_examples_dice_ft(\n",
    "        final_epoch_labels, final_epoch_preds\n",
    "    )\n",
    "    selected_examples_dice = selected_best_dice + selected_average_dice + selected_worst_dice\n",
    "\n",
    "    # Visualization for Dice scores\n",
    "    fig, ax = plt.subplots(len(selected_examples_dice), 4, figsize=(12, len(selected_examples_dice) * 3), dpi=150)\n",
    "    plt.suptitle(\n",
    "        f\"Segmentation model {identifier} - Random example visualizations based on Dice score (Validation)\", # Here it is still 3 high values, 3 average, 3 low in top to bottom order\n",
    "        fontsize=14,\n",
    "        y=0.99\n",
    "    )\n",
    "    fig.subplots_adjust(top=0.7)\n",
    "\n",
    "    for row, (idx, dice) in enumerate(selected_examples_dice):\n",
    "        input_image = final_epoch_inputs[idx, 0]\n",
    "        label_image = final_epoch_labels[idx, 0]\n",
    "        pred_image = final_epoch_preds[idx, 0]\n",
    "\n",
    "        input_image = (input_image * 0.5 + 0.5) * 255\n",
    "        input_image = input_image.astype(np.uint8)\n",
    "\n",
    "        overlay = np.zeros((label_image.shape[0], label_image.shape[1], 3), dtype=np.uint8)\n",
    "        overlay[(pred_image == 1) & (label_image == 1)] = [144, 238, 144]  # Light green\n",
    "        overlay[(pred_image == 1) & (label_image == 0)] = [255, 99, 71]    # Red\n",
    "        overlay[(label_image == 1) & (pred_image == 0)] = [100, 149, 237]  # Blue\n",
    "\n",
    "        ax[row, 0].imshow(input_image, cmap='gray')\n",
    "        ax[row, 0].set_title(\"Input\" if row == 0 else \"\")\n",
    "        ax[row, 0].axis('off')\n",
    "\n",
    "        ax[row, 1].imshow(label_image, cmap='gray')\n",
    "        ax[row, 1].set_title(\"Ground Truth\" if row == 0 else \"\")\n",
    "        ax[row, 1].axis('off')\n",
    "\n",
    "        ax[row, 2].imshow(pred_image, cmap='gray')\n",
    "        ax[row, 2].set_title(\"Prediction\" if row == 0 else \"\")\n",
    "        ax[row, 2].axis('off')\n",
    "\n",
    "        overlay_title = f\"Overlay (Dice: {dice:.2f})\"\n",
    "        if np.sum(label_image) == 0:\n",
    "            overlay_title = f\"Overlay (Dice: {dice:.2f}) (no tumor)\"\n",
    "\n",
    "        ax[row, 3].imshow(overlay)\n",
    "        ax[row, 3].set_title(overlay_title)\n",
    "        ax[row, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout(h_pad=1.5, w_pad=1.0)\n",
    "    plt.savefig(plot_paths['box3_example_views_dice'])\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"»»» Example views with overlay based on Dice image saved at: {plot_paths['box3_example_views_dice']}\")\n",
    "\n",
    "    #---\n",
    "    # Box 4\n",
    "    \n",
    "    selected_best_iou, selected_average_iou, selected_worst_iou = categorize_and_select_examples_iou_ft(\n",
    "        final_epoch_inputs, final_epoch_labels, final_epoch_preds\n",
    "    )\n",
    "    selected_examples_iou = selected_best_iou + selected_average_iou + selected_worst_iou\n",
    "\n",
    "    # Visualization for IoU scores\n",
    "    fig, ax = plt.subplots(len(selected_examples_iou), 4, figsize=(12, len(selected_examples_iou) * 3), dpi=150)\n",
    "    plt.suptitle(\n",
    "        f\"Segmentation model {identifier} - Random example visualizations based on IoU score (Validation)\", # Here it is still 3 high values, 3 average, 3 low in top to bottom order\n",
    "        fontsize=14,\n",
    "        y=0.99\n",
    "    )\n",
    "    fig.subplots_adjust(top=0.7)\n",
    "\n",
    "    for row, (idx, iou) in enumerate(selected_examples_iou):\n",
    "        input_image = final_epoch_inputs[idx, 0]\n",
    "        label_image = final_epoch_labels[idx, 0]\n",
    "        pred_image = final_epoch_preds[idx, 0]\n",
    "\n",
    "        input_image = (input_image * 0.5 + 0.5) * 255\n",
    "        input_image = input_image.astype(np.uint8)\n",
    "\n",
    "        overlay = np.zeros((label_image.shape[0], label_image.shape[1], 3), dtype=np.uint8)\n",
    "        overlay[(pred_image == 1) & (label_image == 1)] = [144, 238, 144]  # Light green\n",
    "        overlay[(pred_image == 1) & (label_image == 0)] = [255, 99, 71]    # Red\n",
    "        overlay[(label_image == 1) & (pred_image == 0)] = [100, 149, 237]  # Blue\n",
    "\n",
    "        ax[row, 0].imshow(input_image, cmap='gray')\n",
    "        ax[row, 0].set_title(\"Input\" if row == 0 else \"\")\n",
    "        ax[row, 0].axis('off')\n",
    "\n",
    "        ax[row, 1].imshow(label_image, cmap='gray')\n",
    "        ax[row, 1].set_title(\"Ground Truth\" if row == 0 else \"\")\n",
    "        ax[row, 1].axis('off')\n",
    "\n",
    "        ax[row, 2].imshow(pred_image, cmap='gray')\n",
    "        ax[row, 2].set_title(\"Prediction\" if row == 0 else \"\")\n",
    "        ax[row, 2].axis('off')\n",
    "\n",
    "        overlay_title = f\"Overlay (IoU: {iou:.2f})\"\n",
    "        if np.sum(label_image) == 0:\n",
    "            overlay_title = f\"Overlay (IoU: {iou:.2f}) (no tumor)\"\n",
    "\n",
    "        ax[row, 3].imshow(overlay)\n",
    "        ax[row, 3].set_title(overlay_title)\n",
    "        ax[row, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout(h_pad=1.5, w_pad=1.0)\n",
    "    plt.savefig(plot_paths['box4_example_views_iou'])\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"»»» Example views with overlay based on IoU image saved at: {plot_paths['box4_example_views_iou']}\")\n",
    "\n",
    "# Testing Functions\n",
    "def segm_run_testing(test_model, test_criterion, test_dataloader, storing_path, identifier, device):\n",
    "    test_model.eval()\n",
    "    \n",
    "    # Initialize variables for the overall test pass\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_inputs = []\n",
    "    all_preds_flat = []\n",
    "    all_labels_flat = []\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    hausdorff_preds = []  # Store coordinates of predicted regions\n",
    "    hausdorff_labels = []  # Store coordinates of ground truth regions\n",
    "\n",
    "    # Initialize a dictionary to store results\n",
    "    results = {\n",
    "        'test_loss': [],\n",
    "        'test_iou': [],\n",
    "        'test_pixel_accuracy': [],\n",
    "        'test_dice': [],\n",
    "        'test_labels': [],\n",
    "        'test_preds': [],\n",
    "        'test_inputs': [],\n",
    "        'test_hausdorff': []\n",
    "    }\n",
    "\n",
    "    print(\"»»» Starting Segmentation Network Testing process \\n\")\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(test_dataloader):\n",
    "        # Move data to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = test_model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = test_criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Threshold predictions (binarization or softmax)\n",
    "        binary_preds = (outputs > 0.5).float()\n",
    "\n",
    "        # Update pixel accuracy counts\n",
    "        correct_pixels += (binary_preds == labels).sum().item()\n",
    "        total_pixels += labels.numel()\n",
    "\n",
    "        # Store data for final evaluation visualization\n",
    "        all_preds.append(binary_preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_inputs.append(inputs.cpu().numpy())\n",
    "\n",
    "        # Flatten and accumulate predictions and labels for global metrics\n",
    "        all_preds_flat.extend(binary_preds.cpu().numpy().astype(int).flatten())\n",
    "        all_labels_flat.extend(labels.cpu().numpy().astype(int).flatten())\n",
    "\n",
    "        # Prepare coordinates for Hausdorff distance calculation\n",
    "        for pred, label in zip(binary_preds.cpu().numpy(), labels.cpu().numpy()):\n",
    "            pred_coords = np.argwhere(pred[0])\n",
    "            label_coords = np.argwhere(label[0])\n",
    "            if pred_coords.size > 0 and label_coords.size > 0:  # Only add non-empty coordinates\n",
    "                hausdorff_preds.append(pred_coords)\n",
    "                hausdorff_labels.append(label_coords)\n",
    "\n",
    "    # Concatenate all batches into cohesive NumPy arrays\n",
    "    if all_preds:\n",
    "        results['test_preds'] = np.concatenate(all_preds, axis=0)\n",
    "        results['test_labels'] = np.concatenate(all_labels, axis=0)\n",
    "        results['test_inputs'] = np.concatenate(all_inputs, axis=0)\n",
    "    else:\n",
    "        results['test_preds'] = np.array([])\n",
    "        results['test_labels'] = np.array([])\n",
    "        results['test_inputs'] = np.array([])\n",
    "\n",
    "    # Calculate global metrics (IoU, Dice, etc.)\n",
    "    test_loss = running_loss / len(test_dataloader.dataset) if len(test_dataloader.dataset) > 0 else np.nan\n",
    "    test_pixel_accuracy = correct_pixels / total_pixels if total_pixels > 0 else np.nan\n",
    "\n",
    "    if len(all_preds_flat) > 0 and len(all_labels_flat) > 0:\n",
    "        test_iou = jaccard_score(all_labels_flat, all_preds_flat, average='binary')\n",
    "        test_dice = f1_score(all_labels_flat, all_preds_flat, average='binary')\n",
    "    else:\n",
    "        test_iou = np.nan\n",
    "        test_dice = np.nan\n",
    "\n",
    "    results['test_loss'].append(test_loss)\n",
    "    results['test_iou'].append(test_iou)\n",
    "    results['test_pixel_accuracy'].append(test_pixel_accuracy)\n",
    "    results['test_dice'].append(test_dice)\n",
    "\n",
    "    # Compute Hausdorff distance\n",
    "    hausdorff_vals = []\n",
    "    for pred_coords, label_coords in zip(hausdorff_preds, hausdorff_labels):\n",
    "        forward_hd = directed_hausdorff(pred_coords, label_coords)[0]\n",
    "        reverse_hd = directed_hausdorff(label_coords, pred_coords)[0]\n",
    "        hausdorff_vals.append(max(forward_hd, reverse_hd))\n",
    "\n",
    "    if hausdorff_vals:\n",
    "        final_hausdorff = np.mean(hausdorff_vals)\n",
    "    else:\n",
    "        final_hausdorff = np.nan  # No valid data for Hausdorff calculation\n",
    "    results['test_hausdorff'].append(final_hausdorff)\n",
    "\n",
    "    # Save results to file\n",
    "    results_filename = f\"logs_{identifier}_test.pkl\"\n",
    "    results_path = os.path.join(storing_path, results_filename)\n",
    "\n",
    "    with open(results_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    print(f\"\\n»»» Segmentation Network Testing process complete\\n»»» Logs saved at: {results_path}\")\n",
    "\n",
    "    return results_path\n",
    "\n",
    "def segm_plot_testing_results(results_path):\n",
    "    if not os.path.exists(results_path):\n",
    "        print(f\"File not found: {results_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"»»» Data taken from path: {results_path}\")\n",
    "    with open(results_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    # Extract directory and identifier from results_path\n",
    "    directory_path = os.path.dirname(results_path)\n",
    "    identifier = os.path.basename(results_path).split('_')[1]\n",
    "\n",
    "    # Define plot paths for segmentation results\n",
    "    plot_paths = {\n",
    "        'box1_value_metrics': os.path.join(directory_path, f\"{identifier}_value_metrics_test.png\"),\n",
    "        'box2_example_views_dice': os.path.join(directory_path, f\"{identifier}_example_views_dice_test.png\"),\n",
    "        'box3_example_views_iou': os.path.join(directory_path, f\"{identifier}_example_views_iou_test.png\")\n",
    "    } \n",
    "\n",
    "    #----\n",
    "    # Box 1\n",
    "\n",
    "    value_metrics = {\n",
    "        \"Test Loss\": results['test_loss'][0],\n",
    "        \"Test IoU\": results['test_iou'][0],\n",
    "        \"Pixel Accuracy\": results['test_pixel_accuracy'][0],\n",
    "        \"Dice Similarity Coefficient\": results['test_dice'][0],\n",
    "        \"Hausdorff Distance\": results['test_hausdorff'][0]\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "\n",
    "    fig.suptitle(f'Segmentation model {identifier} - Single value metrics (Test)', fontsize=16, ha='center')\n",
    "\n",
    "    table_data = [[key, f\"{value:.4f}\"] for key, value in value_metrics.items()]\n",
    "    table = ax.table(cellText=table_data, loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width([0, 1])\n",
    "\n",
    "    plt.savefig(plot_paths['box1_value_metrics'], bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"»»» Metrics table image saved at {plot_paths['box1_value_metrics']}\")\n",
    "\n",
    "    #----\n",
    "    # Box 2\n",
    "    \n",
    "    selected_best_dice, selected_average_dice, selected_worst_dice = categorize_and_select_examples_dice_test(\n",
    "        results['test_labels'], results['test_preds']\n",
    "    )\n",
    "    selected_examples_dice = selected_best_dice + selected_average_dice + selected_worst_dice\n",
    "\n",
    "    fig, ax = plt.subplots(len(selected_examples_dice), 4, figsize=(12, len(selected_examples_dice) * 3), dpi=150)\n",
    "    plt.suptitle(\n",
    "        f\"Segmentation model {identifier} - Random example visualizations based on Dice score (Test)\",\n",
    "        fontsize=14,\n",
    "        y=0.99\n",
    "    )\n",
    "    fig.subplots_adjust(top=0.7)\n",
    "\n",
    "    for row, (idx, dice) in enumerate(selected_examples_dice):\n",
    "        input_image = results['test_inputs'][idx, 0]\n",
    "        label_image = results['test_labels'][idx, 0]\n",
    "        pred_image = results['test_preds'][idx, 0]\n",
    "\n",
    "        input_image = (input_image * 0.5 + 0.5) * 255\n",
    "        input_image = input_image.astype(np.uint8)\n",
    "\n",
    "        overlay = np.zeros((label_image.shape[0], label_image.shape[1], 3), dtype=np.uint8)\n",
    "        overlay[(pred_image == 1) & (label_image == 1)] = [144, 238, 144]  # Light green\n",
    "        overlay[(pred_image == 1) & (label_image == 0)] = [255, 99, 71]    # Red\n",
    "        overlay[(label_image == 1) & (pred_image == 0)] = [100, 149, 237]  # Blue\n",
    "\n",
    "        ax[row, 0].imshow(input_image, cmap='gray')\n",
    "        ax[row, 0].set_title(\"Input\" if row == 0 else \"\")\n",
    "        ax[row, 0].axis('off')\n",
    "\n",
    "        ax[row, 1].imshow(label_image, cmap='gray')\n",
    "        ax[row, 1].set_title(\"Ground Truth\" if row == 0 else \"\")\n",
    "        ax[row, 1].axis('off')\n",
    "\n",
    "        ax[row, 2].imshow(pred_image, cmap='gray')\n",
    "        ax[row, 2].set_title(\"Prediction\" if row == 0 else \"\")\n",
    "        ax[row, 2].axis('off')\n",
    "\n",
    "        overlay_title = f\"Overlay (Dice: {dice:.2f})\"\n",
    "        if np.sum(label_image) == 0:\n",
    "            overlay_title = f\"Overlay (Dice: {dice:.2f}) (no tumor)\"\n",
    "\n",
    "        ax[row, 3].imshow(overlay)\n",
    "        ax[row, 3].set_title(overlay_title)\n",
    "        ax[row, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout(h_pad=1.5, w_pad=1.0)\n",
    "    plt.savefig(plot_paths['box2_example_views_dice'])\n",
    "    plt.close()\n",
    "    print(f\"»»» Example views with overlay based on Dice image saved at: {plot_paths['box2_example_views_dice']}\")\n",
    "\n",
    "    #----\n",
    "    # Box 3\n",
    "    \n",
    "    selected_best_iou, selected_average_iou, selected_worst_iou = categorize_and_select_examples_iou_test(\n",
    "        results['test_labels'], results['test_preds']\n",
    "    )\n",
    "    selected_examples_iou = selected_best_iou + selected_average_iou + selected_worst_iou\n",
    "\n",
    "    fig, ax = plt.subplots(len(selected_examples_iou), 4, figsize=(12, len(selected_examples_iou) * 3), dpi=150)\n",
    "    plt.suptitle(\n",
    "        f\"Segmentation model {identifier} - Random example visualizations based on IoU score (Test)\",\n",
    "        fontsize=14,\n",
    "        y=0.99\n",
    "    )\n",
    "    fig.subplots_adjust(top=0.7)\n",
    "\n",
    "    for row, (idx, iou) in enumerate(selected_examples_iou):\n",
    "        input_image = results['test_inputs'][idx, 0]\n",
    "        label_image = results['test_labels'][idx, 0]\n",
    "        pred_image = results['test_preds'][idx, 0]\n",
    "\n",
    "        input_image = (input_image * 0.5 + 0.5) * 255\n",
    "        input_image = input_image.astype(np.uint8)\n",
    "\n",
    "        overlay = np.zeros((label_image.shape[0], label_image.shape[1], 3), dtype=np.uint8)\n",
    "        overlay[(pred_image == 1) & (label_image == 1)] = [144, 238, 144]  # Light green\n",
    "        overlay[(pred_image == 1) & (label_image == 0)] = [255, 99, 71]    # Red\n",
    "        overlay[(label_image == 1) & (pred_image == 0)] = [100, 149, 237]  # Blue\n",
    "\n",
    "        ax[row, 0].imshow(input_image, cmap='gray')\n",
    "        ax[row, 0].set_title(\"Input\" if row == 0 else \"\")\n",
    "        ax[row, 0].axis('off')\n",
    "\n",
    "        ax[row, 1].imshow(label_image, cmap='gray')\n",
    "        ax[row, 1].set_title(\"Ground Truth\" if row == 0 else \"\")\n",
    "        ax[row, 1].axis('off')\n",
    "\n",
    "        ax[row, 2].imshow(pred_image, cmap='gray')\n",
    "        ax[row, 2].set_title(\"Prediction\" if row == 0 else \"\")\n",
    "        ax[row, 2].axis('off')\n",
    "\n",
    "        overlay_title = f\"Overlay (IoU: {iou:.2f})\"\n",
    "        if np.sum(label_image) == 0:\n",
    "            overlay_title = f\"Overlay (IoU: {iou:.2f}) (no tumor)\"\n",
    "\n",
    "        ax[row, 3].imshow(overlay)\n",
    "        ax[row, 3].set_title(overlay_title)\n",
    "        ax[row, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout(h_pad=1.5, w_pad=1.0)\n",
    "    plt.savefig(plot_paths['box3_example_views_iou'])\n",
    "    plt.close()\n",
    "    print(f\"»»» Example views with overlay based on IoU image saved at: {plot_paths['box3_example_views_iou']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pretrain Pretext Task - Contrastive Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0. Setting pretrain storing paths / backbone builder function / general pretrain parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storing_path51 = \"../results/pretrainPhase/simclr/ultrasound\"\n",
    "storing_path52 = \"../results/pretrainPhase/simclr/mammography\"\n",
    "storing_path53 = \"../results/pretrainPhase/simclr/multimodal\"\n",
    "storing_path54 = \"../results/pretrainPhase/moco/ultrasound\"\n",
    "storing_path55 = \"../results/pretrainPhase/moco/mammography\"\n",
    "storing_path56= \"../results/pretrainPhase/moco/multimodal\"\n",
    "storing_path57 = \"../results/pretrainPhase/byol/ultrasound\"\n",
    "storing_path58 = \"../results/pretrainPhase/byol/mammography\"\n",
    "storing_path59 = \"../results/pretrainPhase/byol/multimodal\"\n",
    "\n",
    "def initialize_resnet_backbone(is_byol):\n",
    "    resnet = torchvision.models.resnet18(weights=None)\n",
    "    backbone = nn.Sequential(\n",
    "        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False), *list(resnet.children())[1:-2], nn.AdaptiveAvgPool2d((1, 1))\n",
    "    )\n",
    "\n",
    "    if is_byol:\n",
    "        backbone.add_module('flatten', nn.Flatten())\n",
    "        backbone.output_dim = 512\n",
    "    return backbone\n",
    "\n",
    "total_epoch_count = 100\n",
    "checkpoint_interval = 5 \n",
    "lr_pretrain = 0.014\n",
    "temperature = 0.07\n",
    "weight_decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "#model = initialize_resnet_backbone(False).to(device)\n",
    "#summary(model, input_size=(1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "## Code snippet to show how to load the pretrained model if needed (only run if needed)\n",
    "# \n",
    "# (SIMCLR ONLY) \n",
    "## 1. create a new backbone and simclr model using said backbone\n",
    "#backbone_simclr_us_pretrain_loaded = initialize_resnet_backbone(False)\n",
    "#model_simclr_us_pretrain_loaded = SimCLR_v2(backbone_simclr_us_pretrain_loaded, projection_dim=128)\n",
    "#model_simclr_us_pretrain_loaded.to(device)\n",
    "\n",
    "## 2. get path to the saved model\n",
    "#model_filename = \"M1_savedModel.pth\"\n",
    "#model_path = os.path.join(storing_path51, model_filename)\n",
    "\n",
    "## Load the model weights into the model\n",
    "#model_simclr_us_pretrain_loaded.load_state_dict(torch.load(model_path))\n",
    "\n",
    "#------------\n",
    "\n",
    "# (MOCO ONLY) \n",
    "## 1. create a new backbone and simclr model using said backbone\n",
    "#backbone_simclr_us_pretrain_loaded = initialize_resnet_backbone(False)\n",
    "#model_simclr_us_pretrain_loaded = SimCLR_v2(backbone_simclr_us_pretrain_loaded, projection_dim=128)\n",
    "#model_simclr_us_pretrain_loaded.to(device)\n",
    "\n",
    "## 2. get path to the saved model\n",
    "#model_filename = \"M1_savedModel.pth\"\n",
    "#model_path = os.path.join(storing_path51, model_filename)\n",
    "\n",
    "## Load the model weights into the model\n",
    "#model_simclr_us_pretrain_loaded.load_state_dict(torch.load(model_path))\n",
    "\n",
    "## 1. create a new backbone and moco model using said backbone\n",
    "#backbone_moco_us_pretrain_loaded = initialize_resnet_backbone(False)\n",
    "#model_moco_us_pretrain_loaded = MoCo_v2(backbone_moco_us_pretrain_loaded)\n",
    "#model_moco_us_pretrain_loaded.to(device)\n",
    "\n",
    "## 2. get path to the saved model\n",
    "#model_filename = \"M4_savedModel.pth\"\n",
    "#model_path = os.path.join(storing_path54, model_filename)\n",
    "\n",
    "## Load the model weights into the model\n",
    "#model_moco_us_pretrain_loaded.load_state_dict(torch.load(model_path))\n",
    "\n",
    "#------------\n",
    "\n",
    "# (BYOL ONLY)\n",
    "## 1. Create a new backbone and BYOL model using said backbone\n",
    "#backbone_byol_us_pretrain_loaded = initialize_resnet_backbone(True)  #just the byol case uses true here\n",
    "#model_byol_us_pretrain_loaded = BYOL(backbone_byol_us_pretrain_loaded, projection_dim=128)\n",
    "#model_byol_us_pretrain_loaded.to(device)\n",
    "\n",
    "## 2. Get the path to the saved model\n",
    "#model_filename = \"M7_savedModel.pth\"\n",
    "#model_path = os.path.join(storing_path57, model_filename)\n",
    "\n",
    "## 3. Load the model weights into the BYOL model\n",
    "#model_byol_us_pretrain_loaded.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Using <font color='green'>SimCLR</font> method and <font color='purple'>Ultrasound</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a 4 step process\n",
    "# 1. Prepare model\n",
    "# Initialize the SimCLR model with the ResNet backbone\n",
    "backbone_simclr_us_pretrain = initialize_resnet_backbone(False)\n",
    "model_simclr_us_pretrain = SimCLR_v2(backbone_simclr_us_pretrain, projection_dim=128)\n",
    "model_simclr_us_pretrain.to(device)\n",
    "\n",
    "# Contrastive loss with temperature\n",
    "criterion_simclr_us_pretrain = SimCLRNTXentLoss(temperature)\n",
    "criterion_simclr_us_pretrain.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer_simclr_us_pretrain = optim.Adam(model_simclr_us_pretrain.parameters(), lr = lr_pretrain, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler\n",
    "scheduler_simclr_us_pretrain = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_simclr_us_pretrain, mode = 'min', patience = 3, factor = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pretrain loop\n",
    "pretraining_results_path_simclr_us = run_pretraining_simclr(\n",
    "    model_simclr_us_pretrain,                  # SimCLR model\n",
    "    criterion_simclr_us_pretrain,              # NTXentLoss (contrastive loss function)\n",
    "    us_pretrain_train_dataloader,              # Dataloader for training data\n",
    "    us_pretrain_val_dataloader,                # Dataloader for validation data\n",
    "    optimizer_simclr_us_pretrain,              # Adam optimizer\n",
    "    scheduler_simclr_us_pretrain,              # Learning rate scheduler\n",
    "    device,                                    # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                         # Number of epochs to train\n",
    "    storing_path51,                            # Path to store logs\n",
    "    \"M1\",                                      # Model identifier\n",
    "    checkpoint_interval                        # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the pretrained model\n",
    "model_filename = \"M1_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path51, model_filename)\n",
    "torch.save(model_simclr_us_pretrain.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of pretrain loop\n",
    "# Plot and save pretraining results\n",
    "plot_pretrain_results(pretraining_results_path_simclr_us)\n",
    "\n",
    "# Pre-Finetune t-distributed Stochastic Neighbor Embedding (t-SNE) Visualization\n",
    "tsne_pipeline(model_simclr_us_pretrain, tSNE_us_dataloader, device, pretraining_results_path_simclr_us, is_byol=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Using <font color='green'>SimCLR</font> method and <font color='pink'>Mammography</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Initialize the SimCLR model with the ResNet backbone\n",
    "backbone_simclr_mammo_pretrain = initialize_resnet_backbone(False)\n",
    "model_simclr_mammo_pretrain = SimCLR_v2(backbone_simclr_mammo_pretrain, projection_dim=128)\n",
    "model_simclr_mammo_pretrain.to(device)\n",
    "\n",
    "# Contrastive loss with temperature\n",
    "criterion_simclr_mammo_pretrain = SimCLRNTXentLoss(temperature)\n",
    "criterion_simclr_mammo_pretrain.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer_simclr_mammo_pretrain = optim.Adam(model_simclr_mammo_pretrain.parameters(), lr = lr_pretrain, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler\n",
    "scheduler_simclr_mammo_pretrain = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_simclr_mammo_pretrain, mode = 'min', patience = 3, factor = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pretrain loop\n",
    "pretraining_results_path_simclr_mammo = run_pretraining_simclr(\n",
    "    model_simclr_mammo_pretrain,                   # SimCLR model\n",
    "    criterion_simclr_mammo_pretrain,               # NTXentLoss (contrastive loss function)\n",
    "    mg_pretrain_train_dataloader,                  # Dataloader for training data\n",
    "    mg_pretrain_val_dataloader,                    # Dataloader for validation data\n",
    "    optimizer_simclr_mammo_pretrain,               # Adam optimizer\n",
    "    scheduler_simclr_mammo_pretrain,               # Learning rate scheduler\n",
    "    device,                                        # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                             # Number of epochs to train\n",
    "    storing_path52,                                # Path to store logs\n",
    "    \"M2\",                                          # Model identifier\n",
    "    checkpoint_interval                            # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the pretrained model\n",
    "model_filename = \"M2_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path52, model_filename)\n",
    "torch.save(model_simclr_mammo_pretrain.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of pretrain loop\n",
    "# Plot and save pretraining results\n",
    "plot_pretrain_results(pretraining_results_path_simclr_mammo)\n",
    "\n",
    "# Pre-Finetune t-distributed Stochastic Neighbor Embedding (t-SNE) Visualization\n",
    "tsne_pipeline(model_simclr_mammo_pretrain, tSNE_mg_dataloader, device, pretraining_results_path_simclr_mammo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Using <font color='green'>SimCLR</font> method and <font color='orange'>Multimodal</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Initialize the SimCLR model with the ResNet backbone\n",
    "backbone_simclr_multi_pretrain = initialize_resnet_backbone(False)\n",
    "model_simclr_multi_pretrain = SimCLR_v2(backbone_simclr_multi_pretrain, projection_dim=128)\n",
    "model_simclr_multi_pretrain.to(device)\n",
    "\n",
    "# Contrastive loss with temperature\n",
    "criterion_simclr_multi_pretrain = SimCLRNTXentLoss(temperature)\n",
    "criterion_simclr_multi_pretrain.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer_simclr_multi_pretrain = optim.Adam(model_simclr_multi_pretrain.parameters(), lr = lr_pretrain, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler\n",
    "scheduler_simclr_multi_pretrain = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_simclr_multi_pretrain, mode = 'min', patience = 3, factor = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pretrain loop\n",
    "pretraining_results_path_simclr_multi = run_pretraining_simclr(\n",
    "    model_simclr_multi_pretrain,                      # SimCLR model\n",
    "    criterion_simclr_multi_pretrain,                  # NTXentLoss (contrastive loss function)\n",
    "    multi_pretrain_train_dataloader,                  # Dataloader for training data\n",
    "    multi_pretrain_val_dataloader,                    # Dataloader for validation data\n",
    "    optimizer_simclr_multi_pretrain,                  # Adam optimizer\n",
    "    scheduler_simclr_multi_pretrain,                  # Learning rate scheduler\n",
    "    device,                                           # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                # Number of epochs to train\n",
    "    storing_path53,                                   # Path to store logs\n",
    "    \"M3\",                                             # Model identifier\n",
    "    checkpoint_interval                               # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the pretrained model\n",
    "model_filename = \"M3_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path53, model_filename)\n",
    "torch.save(model_simclr_multi_pretrain.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of pretrain loop\n",
    "# Plot and save pretraining results\n",
    "plot_pretrain_results(pretraining_results_path_simclr_multi)\n",
    "\n",
    "# Pre-Finetune t-distributed Stochastic Neighbor Embedding (t-SNE) Visualization\n",
    "tsne_pipeline(model_simclr_multi_pretrain, tSNE_multi_dataloader, device, pretraining_results_path_simclr_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Using <font color='turquoise'>MoCo</font> method and <font color='purple'>Ultrasound</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new (checking)\n",
    "# 1. Prepare model\n",
    "# Initialize the MoCo model with the ResNet backbone\n",
    "backbone_moco_us_pretrain = initialize_resnet_backbone(False)\n",
    "model_moco_us_pretrain = MoCo_v2(backbone_moco_us_pretrain)\n",
    "model_moco_us_pretrain.to(device)\n",
    "\n",
    "# Contrastive loss with queue_size, temperature\n",
    "criterion_moco_us_pretrain = MoCoNTXentLoss(queue_size = 16384, temperature = temperature)  # Contrastive loss with temperature\n",
    "criterion_moco_us_pretrain.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer_moco_us_pretrain = optim.Adam(model_moco_us_pretrain.parameters(), lr = lr_pretrain, weight_decay = weight_decay)  \n",
    "\n",
    "#Moco_v2 works better with cosineAnnealingLR scheduler\n",
    "scheduler_moco_us_pretrain = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_moco_us_pretrain, \n",
    "    T_max = total_epoch_count,  # total number of epochs for one cycle\n",
    "    eta_min = 0,  # Minimum learning rate (you can adjust this if needed)\n",
    "    last_epoch = -1  # Start from scratch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pretrain loop\n",
    "pretraining_results_path_moco_us = run_pretraining_moco(\n",
    "    model_moco_us_pretrain,             # MoCo model\n",
    "    criterion_moco_us_pretrain,         # Adapted NTXentLoss (contrastive loss function)\n",
    "    us_pretrain_train_dataloader,       # Dataloader for training data\n",
    "    us_pretrain_val_dataloader,         # Dataloader for validation data\n",
    "    optimizer_moco_us_pretrain,         # Adam optimizer\n",
    "    scheduler_moco_us_pretrain,         # Learning rate scheduler\n",
    "    device,                             # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                  # Number of epochs to train\n",
    "    storing_path54,                     # Path to store logs\n",
    "    \"M4\",                               # Model identifier\n",
    "    checkpoint_interval                 # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the pretrained model\n",
    "model_filename = \"M4_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path54, model_filename)\n",
    "torch.save(model_moco_us_pretrain.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of pretrain loop\n",
    "# Plot and save pretraining results\n",
    "plot_pretrain_results(pretraining_results_path_moco_us)\n",
    "\n",
    "# Pre-Finetune t-distributed Stochastic Neighbor Embedding (t-SNE) Visualization\n",
    "tsne_pipeline(model_moco_us_pretrain, tSNE_us_dataloader, device, pretraining_results_path_moco_us, is_byol = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Using <font color='turquoise'>MoCo</font> method and <font color='pink'>Mammography</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new (checking)\n",
    "# 1. Prepare model\n",
    "# Initialize the MoCo model with the ResNet backbone\n",
    "backbone_moco_mammo_pretrain = initialize_resnet_backbone(False)\n",
    "model_moco_mammo_pretrain = MoCo_v2(backbone_moco_mammo_pretrain)\n",
    "model_moco_mammo_pretrain.to(device)\n",
    "\n",
    "# Contrastive loss with queue_size, temperature\n",
    "criterion_moco_mammo_pretrain = MoCoNTXentLoss(queue_size = 16384, temperature = temperature)  # Contrastive loss with temperature\n",
    "criterion_moco_mammo_pretrain.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer_moco_mammo_pretrain = optim.Adam(model_moco_mammo_pretrain.parameters(), lr = lr_pretrain, weight_decay = weight_decay)  \n",
    "\n",
    "#Moco_v2 works better with cosineAnnealingLR scheduler\n",
    "scheduler_moco_mammo_pretrain = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_moco_mammo_pretrain, \n",
    "    T_max = total_epoch_count,  # total number of epochs for one cycle\n",
    "    eta_min = 0,  # minimum learning rate (you can adjust this if needed)\n",
    "    last_epoch = -1  # start from scratch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pretrain loop\n",
    "pretraining_results_path_moco_mammo = run_pretraining_moco(\n",
    "    model_moco_mammo_pretrain,             # MoCo model\n",
    "    criterion_moco_mammo_pretrain,         # Adapted NTXentLoss (contrastive loss function)\n",
    "    mg_pretrain_train_dataloader,          # Dataloader for training data\n",
    "    mg_pretrain_val_dataloader,            # Dataloader for validation data\n",
    "    optimizer_moco_mammo_pretrain,         # Adam optimizer\n",
    "    scheduler_moco_mammo_pretrain,         # Learning rate scheduler\n",
    "    device,                                # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                     # Number of epochs to train\n",
    "    storing_path55,                        # Path to store logs\n",
    "    \"M5\",                                  # Model identifier\n",
    "    checkpoint_interval                    # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the pretrained model\n",
    "model_filename = \"M5_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path55, model_filename)\n",
    "torch.save(model_moco_mammo_pretrain.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of pretrain loop\n",
    "# Plot and save pretraining results\n",
    "plot_pretrain_results(pretraining_results_path_moco_mammo)\n",
    "\n",
    "# Pre-Finetune t-distributed Stochastic Neighbor Embedding (t-SNE) Visualization\n",
    "tsne_pipeline(model_moco_mammo_pretrain, tSNE_mg_dataloader, device, pretraining_results_path_moco_mammo, is_byol = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Using <font color='turquoise'>MoCo</font> method and <font color='orange'>Multimodal</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new (checking)\n",
    "# 1. Prepare model\n",
    "# Initialize the MoCo model with the ResNet backbone\n",
    "backbone_moco_multi_pretrain = initialize_resnet_backbone(False)\n",
    "model_moco_multi_pretrain = MoCo_v2(backbone_moco_multi_pretrain)\n",
    "model_moco_multi_pretrain.to(device)\n",
    "\n",
    "# Contrastive loss with queue_size, temperature\n",
    "criterion_moco_multi_pretrain = MoCoNTXentLoss(queue_size = 16384, temperature = temperature)  # Contrastive loss with temperature\n",
    "criterion_moco_multi_pretrain.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer_moco_multi_pretrain = optim.Adam(model_moco_multi_pretrain.parameters(), lr = lr_pretrain, weight_decay = weight_decay)  \n",
    "\n",
    "#Moco_v2 works better with cosineAnnealingLR scheduler\n",
    "scheduler_moco_multi_pretrain = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_moco_multi_pretrain, \n",
    "    T_max = total_epoch_count,  # total number of epochs for one cycle\n",
    "    eta_min = 0,  # minimum learning rate (you can adjust this if needed)\n",
    "    last_epoch = -1  # start from scratch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pretrain loop\n",
    "pretraining_results_path_moco_multi= run_pretraining_moco(\n",
    "    model_moco_multi_pretrain,             # MoCo model\n",
    "    criterion_moco_multi_pretrain,         # Adapted NTXentLoss (contrastive loss function)\n",
    "    multi_pretrain_train_dataloader,       # Dataloader for training data\n",
    "    multi_pretrain_val_dataloader,         # Dataloader for validation data\n",
    "    optimizer_moco_multi_pretrain,         # Adam optimizer\n",
    "    scheduler_moco_multi_pretrain,         # Learning rate scheduler\n",
    "    device,                                # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                     # Number of epochs to train\n",
    "    storing_path56,                        # Path to store logs\n",
    "    \"M6\",                                  # Model identifier\n",
    "    checkpoint_interval                    # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the pretrained model\n",
    "model_filename = \"M6_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path56, model_filename)\n",
    "torch.save(model_moco_multi_pretrain.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of pretrain loop\n",
    "# Plot and save pretraining results\n",
    "plot_pretrain_results(pretraining_results_path_moco_multi)\n",
    "\n",
    "# Pre-Finetune t-distributed Stochastic Neighbor Embedding (t-SNE) Visualization\n",
    "tsne_pipeline(model_moco_multi_pretrain, tSNE_multi_dataloader, device, pretraining_results_path_moco_multi, is_byol = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7. Using <font color='yellow'>BYOL</font> method and <font color='purple'>Ultrasound</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new (checking)\n",
    "# 1. Prepare model\n",
    "# Initialize the MoCo model with the ResNet backbone\n",
    "backbone_byol_us_pretrain = initialize_resnet_backbone(True)\n",
    "model_byol_us_pretrain = BYOL(backbone_byol_us_pretrain)\n",
    "model_byol_us_pretrain.to(device)\n",
    "\n",
    "# Initialize loss function\n",
    "criterion_byol_us_pretrain = BYOLLoss()\n",
    "criterion_byol_us_pretrain.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer_byol_us_pretrain = optim.Adam(model_byol_us_pretrain.parameters(), lr = lr_pretrain, weight_decay = weight_decay)\n",
    "\n",
    "# Original BYOL work didnt use scheduler, we follow suit\n",
    "\n",
    "# All BYOL cases involve model_byol.online_encoder and is_byol = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pretrain loop\n",
    "pretraining_results_path_byol_us = run_pretraining_byol(\n",
    "    model_byol_us_pretrain,                  # BYOL model\n",
    "    criterion_byol_us_pretrain,              # Negative cosine similarity\n",
    "    us_pretrain_train_dataloader,            # Dataloader for training data\n",
    "    us_pretrain_val_dataloader,              # Dataloader for validation data\n",
    "    optimizer_byol_us_pretrain,              # Adam optimizer\n",
    "    device,                                  # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                       # Number of epochs to train\n",
    "    storing_path57,                          # Path to store logs\n",
    "    \"M7\",                                    # Model identifier\n",
    "    checkpoint_interval                      # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the pretrained model\n",
    "model_filename = \"M7_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path57, model_filename)\n",
    "torch.save(model_byol_us_pretrain.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of pretrain loop\n",
    "# Plot and save pretraining results\n",
    "plot_pretrain_results(pretraining_results_path_byol_us)\n",
    "\n",
    "# Pre-Finetune t-distributed Stochastic Neighbor Embedding (t-SNE) Visualization\n",
    "tsne_pipeline(model_byol_us_pretrain.online_encoder, tSNE_us_dataloader, device, pretraining_results_path_byol_us, is_byol = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8. Using <font color='yellow'>BYOL</font> method and <font color='pink'>Mammography</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new (checking)\n",
    "# 1. Prepare model\n",
    "# Initialize the MoCo model with the ResNet backbone\n",
    "backbone_byol_mammo_pretrain = initialize_resnet_backbone(True)\n",
    "model_byol_mammo_pretrain = BYOL(backbone_byol_mammo_pretrain)\n",
    "model_byol_mammo_pretrain.to(device)\n",
    "\n",
    "# Initialize loss function\n",
    "criterion_byol_mammo_pretrain = BYOLLoss()\n",
    "criterion_byol_mammo_pretrain.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer_byol_mammo_pretrain = optim.Adam(model_byol_mammo_pretrain.parameters(), lr = lr_pretrain, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pretrain loop\n",
    "pretraining_results_path_byol_mammo = run_pretraining_byol(\n",
    "    model_byol_mammo_pretrain,                  # BYOL model\n",
    "    criterion_byol_mammo_pretrain,              # Negative cosine similarity\n",
    "    mg_pretrain_train_dataloader,               # Dataloader for training data\n",
    "    mg_pretrain_val_dataloader,                 # Dataloader for validation data\n",
    "    optimizer_byol_mammo_pretrain,              # Adam optimizer\n",
    "    device,                                     # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                          # Number of epochs to train\n",
    "    storing_path58,                             # Path to store logs\n",
    "    \"M8\",                                       # Model identifier\n",
    "    checkpoint_interval                         # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the pretrained model\n",
    "model_filename = \"M8_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path58, model_filename)\n",
    "torch.save(model_byol_mammo_pretrain.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of pretrain loop\n",
    "# Plot and save pretraining results\n",
    "plot_pretrain_results(pretraining_results_path_byol_mammo)\n",
    "\n",
    "# Pre-Finetune t-distributed Stochastic Neighbor Embedding (t-SNE) Visualization\n",
    "tsne_pipeline(model_byol_mammo_pretrain.online_encoder, tSNE_mg_dataloader, device, pretraining_results_path_byol_mammo, is_byol = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9. Using <font color='yellow'>BYOL</font> method and <font color='orange'>Multimodal</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new (checking)\n",
    "# 1. Prepare model\n",
    "# Initialize the MoCo model with the ResNet backbone\n",
    "backbone_byol_multi_pretrain = initialize_resnet_backbone(True)\n",
    "model_byol_multi_pretrain = BYOL(backbone_byol_multi_pretrain)\n",
    "model_byol_multi_pretrain.to(device)\n",
    "\n",
    "# Initialize loss function\n",
    "criterion_byol_multi_pretrain = BYOLLoss()\n",
    "criterion_byol_multi_pretrain.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer_byol_multi_pretrain = optim.Adam(model_byol_multi_pretrain.parameters(), lr = lr_pretrain, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pretrain loop\n",
    "pretraining_results_path_byol_multi = run_pretraining_byol(\n",
    "    model_byol_multi_pretrain,                     # BYOL model\n",
    "    criterion_byol_multi_pretrain,                 # Negative cosine similarity\n",
    "    multi_pretrain_train_dataloader,               # Dataloader for training data\n",
    "    multi_pretrain_val_dataloader,                 # Dataloader for validation data\n",
    "    optimizer_byol_multi_pretrain,                 # Adam optimizer\n",
    "    device,                                        # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                             # Number of epochs to train\n",
    "    storing_path59,                                # Path to store logs\n",
    "    \"M9\",                                          # Model identifier\n",
    "    checkpoint_interval                            # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the pretrained model\n",
    "model_filename = \"M9_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path59, model_filename)\n",
    "torch.save(model_byol_multi_pretrain.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of pretrain loop\n",
    "# Plot and save pretraining results\n",
    "plot_pretrain_results(pretraining_results_path_byol_multi)\n",
    "\n",
    "# Pre-Finetune t-distributed Stochastic Neighbor Embedding (t-SNE) Visualization\n",
    "tsne_pipeline(model_byol_multi_pretrain.online_encoder, tSNE_multi_dataloader, device, pretraining_results_path_byol_multi, is_byol=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Finetuning / Testing for Downstream Task - Multiclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0. Setting classification models architecture and storing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storing_path61 = \"../results/classification/simclr/ultrasound\"\n",
    "storing_path62 = \"../results/classification/simclr/mammography\"\n",
    "storing_path63 = \"../results/classification/simclr/multimodal\"\n",
    "storing_path64 = \"../results/classification/moco/ultrasound\"\n",
    "storing_path65 = \"../results/classification/moco/mammography\"\n",
    "storing_path66 = \"../results/classification/moco/multimodal\"\n",
    "storing_path67 = \"../results/classification/byol/ultrasound\"\n",
    "storing_path68 = \"../results/classification/byol/mammography\"\n",
    "storing_path69 = \"../results/classification/byol/multimodal\"\n",
    "storing_path610 = \"../results/classification/supervised/ultrasound\"\n",
    "storing_path611 = \"../results/classification/supervised/mammography\"\n",
    "storing_path612 = \"../results/classification/supervised/multimodal\"\n",
    "\n",
    "class classif_model_finetune(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super(classif_model_finetune, self).__init__()\n",
    "        self.base_model = base_model  # Use the backbone of concluded Pretrain phase\n",
    "        self.fc = nn.Linear(512, num_classes)  # Add a new FC layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x).flatten(start_dim = 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Supervised classification model with randomly initialized weights\n",
    "class classif_model_supervised(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(classif_model_supervised, self).__init__()\n",
    "        self.base_model = models.resnet18(weights=None)  # Random init (No Pretraining)\n",
    "\n",
    "        # Modify first convolutional layer to accept 1-channel grayscale images\n",
    "        self.base_model.conv1 = nn.Conv2d(\n",
    "            in_channels = 1,\n",
    "            out_channels = 64,  \n",
    "            kernel_size = 7, \n",
    "            stride = 2, \n",
    "            padding = 3, \n",
    "            bias = False\n",
    "        )\n",
    "\n",
    "        # Remove the default fully connected layer (fc)\n",
    "        in_features = self.base_model.fc.in_features  # Get input size of last layer\n",
    "        self.base_model.fc = nn.Identity()  # Remove it\n",
    "\n",
    "        # Replace final classification layer\n",
    "        self.fc = nn.Linear(in_features, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x).flatten(start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "lr_finetune = 0.001\n",
    "weight_decay = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. With models pretrained with <font color='green'>SimCLR</font> method and employing the <font color='purple'>Ultrasound</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a 6 step process\n",
    "# 1. Prepare model\n",
    "# Initialize finetuning model with the associated ResNet of pretraining phase\n",
    "# Always working with a copy ensuring no inadverted changes to original pretrained models\n",
    "finetune_backbone = deepcopy(model_simclr_us_pretrain.backbone) \n",
    "model_simclr_us_classif = classif_model_finetune(finetune_backbone, num_classes)\n",
    "model_simclr_us_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_simclr_us_classif = nn.CrossEntropyLoss()\n",
    "criterion_simclr_us_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_simclr_us_classif = torch.optim.Adam(model_simclr_us_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_simclr_us_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_simclr_us_classif, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_simclr_us = classif_run_finetuning(\n",
    "    model_simclr_us_classif,                           # model resulting from SimCLR pretraining         \n",
    "    criterion_simclr_us_classif,                       # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_us_finetune_train_dataloader,              # Dataloader for training data\n",
    "    classif_us_finetune_val_dataloader,                # Dataloader for validation data\n",
    "    optimizer_simclr_us_classif,                       # Adam optimizer\n",
    "    scheduler_simclr_us_classif,                       # Learning rate scheduler\n",
    "    device,                                            # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                 # Number of epochs to finetune\n",
    "    storing_path61,                                    # Path to store logs      \n",
    "    \"M1_classification\",                               # Model identifier\n",
    "    checkpoint_interval                                # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M1_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path61, model_filename)\n",
    "torch.save(model_simclr_us_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_simclr_us, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_simclr_us = classif_run_testing(\n",
    "    model_simclr_us_classif,\n",
    "    criterion_simclr_us_classif,\n",
    "    classif_us_finetune_test_dataloader,\n",
    "    storing_path61,\n",
    "    \"M1_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_simclr_us, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. With models pretrained with <font color='green'>SimCLR</font> method and employing the <font color='pink'>Mammography</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Initialize finetuning model with the associated ResNet of pretraining phase\n",
    "# Always working with a copy to ensure no inadverted changes happen to original pretrain models since they are to be used twice (once per task of which we have 2)\n",
    "finetune_backbone = deepcopy(model_simclr_mammo_pretrain.backbone) \n",
    "model_simclr_mammo_classif = classif_model_finetune(finetune_backbone, num_classes)\n",
    "model_simclr_mammo_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_simclr_mammo_classif = nn.CrossEntropyLoss()\n",
    "criterion_simclr_mammo_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_simclr_mammo_classif = torch.optim.Adam(model_simclr_mammo_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_simclr_mammo_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_simclr_mammo_classif, mode='min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_simclr_mammo = classif_run_finetuning(\n",
    "    model_simclr_mammo_classif,                           # model resulting from SimCLR pretraining         \n",
    "    criterion_simclr_mammo_classif,                       # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_mg_finetune_train_dataloader,                 # Dataloader for training data\n",
    "    classif_mg_finetune_val_dataloader,                   # Dataloader for validation data\n",
    "    optimizer_simclr_mammo_classif,                       # Adam optimizer\n",
    "    scheduler_simclr_mammo_classif,                       # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path62,                                       # Path to store logs      \n",
    "    \"M2_classification\",                                  # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M2_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path62, model_filename)\n",
    "torch.save(model_simclr_mammo_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_simclr_mammo, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_simclr_mammo = classif_run_testing(\n",
    "    model_simclr_mammo_classif,\n",
    "    criterion_simclr_mammo_classif,\n",
    "    classif_mg_finetune_test_dataloader,\n",
    "    storing_path62,\n",
    "    \"M2_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_simclr_mammo, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. With models pretrained with <font color='green'>SimCLR</font> method and employing the <font color='orange'>Multimodal</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Initialize finetuning model with the associated ResNet of pretraining phase\n",
    "# Always working with a copy to ensure no inadverted changes happen to original pretrain models since they are to be used twice (once per task of which we have 2)\n",
    "finetune_backbone = deepcopy(model_simclr_multi_pretrain.backbone) \n",
    "model_simclr_multi_classif = classif_model_finetune(finetune_backbone, num_classes)\n",
    "model_simclr_multi_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_simclr_multi_classif = nn.CrossEntropyLoss()\n",
    "criterion_simclr_multi_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_simclr_multi_classif = torch.optim.Adam(model_simclr_multi_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_simclr_multi_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_simclr_multi_classif, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_simclr_multi = classif_run_finetuning(\n",
    "    model_simclr_multi_classif,                           # model resulting from SimCLR pretraining         \n",
    "    criterion_simclr_multi_classif,                       # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_multi_finetune_train_dataloader,              # Dataloader for training data\n",
    "    classif_multi_finetune_val_dataloader,                # Dataloader for validation data\n",
    "    optimizer_simclr_multi_classif,                       # Adam optimizer\n",
    "    scheduler_simclr_multi_classif,                       # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path63,                                       # Path to store logs      \n",
    "    \"M3_classification\",                                  # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M3_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path63, model_filename)\n",
    "torch.save(model_simclr_multi_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_simclr_multi, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_simclr_multi = classif_run_testing(\n",
    "    model_simclr_multi_classif,\n",
    "    criterion_simclr_multi_classif,\n",
    "    classif_multi_finetune_test_dataloader,\n",
    "    storing_path63,\n",
    "    \"M3_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_simclr_multi, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. With models pretrained with <font color='turquoise'>MoCo</font> method and employing the <font color='purple'>Ultrasound</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Initialize finetuning model with the associated ResNet of pretraining phase\n",
    "# Always working with a copy to ensure no inadverted changes happen to original pretrain models since they are to be used twice (once per task of which we have 2)\n",
    "finetune_backbone = deepcopy(model_moco_us_pretrain.backbone) \n",
    "model_moco_us_classif = classif_model_finetune(finetune_backbone, num_classes)\n",
    "model_moco_us_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_moco_us_classif = nn.CrossEntropyLoss()\n",
    "criterion_moco_us_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_moco_us_classif = torch.optim.Adam(model_moco_us_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_moco_us_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_moco_us_classif, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_moco_us = classif_run_finetuning(\n",
    "    model_moco_us_classif,                                # model resulting from SimCLR pretraining         \n",
    "    criterion_moco_us_classif,                            # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_us_finetune_train_dataloader,                 # Dataloader for training data\n",
    "    classif_us_finetune_val_dataloader,                   # Dataloader for validation data\n",
    "    optimizer_moco_us_classif,                            # Adam optimizer\n",
    "    scheduler_moco_us_classif,                            # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path64,                                       # Path to store logs      \n",
    "    \"M4_classification\",                                  # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M4_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path64, model_filename)\n",
    "torch.save(model_moco_us_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_moco_us, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_moco_us = classif_run_testing(\n",
    "    model_moco_us_classif,\n",
    "    criterion_moco_us_classif,\n",
    "    classif_us_finetune_test_dataloader,\n",
    "    storing_path64,\n",
    "    \"M4_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_moco_us, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. With models pretrained with <font color='turquoise'>MoCo</font> method and employing the <font color='pink'>Mammography</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Initialize finetuning model with the associated ResNet of pretraining phase\n",
    "# Always working with a copy to ensure no inadverted changes happen to original pretrain models since they are to be used twice (once per task of which we have 2)\n",
    "finetune_backbone = deepcopy(model_moco_mammo_pretrain.backbone) \n",
    "model_moco_mammo_classif = classif_model_finetune(finetune_backbone, num_classes)\n",
    "model_moco_mammo_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_moco_mammo_classif = nn.CrossEntropyLoss()\n",
    "criterion_moco_mammo_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_moco_mammo_classif = torch.optim.Adam(model_moco_mammo_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_moco_mammo_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_moco_mammo_classif, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_moco_mammo = classif_run_finetuning(\n",
    "    model_moco_mammo_classif,                           # model resulting from SimCLR pretraining         \n",
    "    criterion_moco_mammo_classif,                       # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_mg_finetune_train_dataloader,               # Dataloader for training data\n",
    "    classif_mg_finetune_val_dataloader,                 # Dataloader for validation data\n",
    "    optimizer_moco_mammo_classif,                       # Adam optimizer\n",
    "    scheduler_moco_mammo_classif,                       # Learning rate scheduler\n",
    "    device,                                             # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                  # Number of epochs to finetune\n",
    "    storing_path65,                                     # Path to store logs      \n",
    "    \"M5_classification\",                                # Model identifier\n",
    "    checkpoint_interval                                 # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M5_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path65, model_filename)\n",
    "torch.save(model_moco_mammo_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_moco_mammo, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_moco_mammo = classif_run_testing(\n",
    "    model_moco_mammo_classif,\n",
    "    criterion_moco_mammo_classif,\n",
    "    classif_mg_finetune_test_dataloader,\n",
    "    storing_path65,\n",
    "    \"M5_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_moco_mammo, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6. With models pretrained with <font color='turquoise'>MoCo</font> method and employing the <font color='orange'>Multimodal</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Initialize finetuning model with the associated ResNet of pretraining phase\n",
    "# Always working with a copy to ensure no inadverted changes happen to original pretrain models since they are to be used twice (once per task of which we have 2)\n",
    "finetune_backbone = deepcopy(model_moco_multi_pretrain.backbone) \n",
    "model_moco_multi_classif = classif_model_finetune(finetune_backbone, num_classes)\n",
    "model_moco_multi_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_moco_multi_classif = nn.CrossEntropyLoss()\n",
    "criterion_moco_multi_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_moco_multi_classif = torch.optim.Adam(model_moco_multi_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_moco_multi_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_moco_multi_classif, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_moco_multi = classif_run_finetuning(\n",
    "    model_moco_multi_classif,                             # model resulting from SimCLR pretraining         \n",
    "    criterion_moco_multi_classif,                         # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_multi_finetune_train_dataloader,              # Dataloader for training data\n",
    "    classif_multi_finetune_val_dataloader,                # Dataloader for validation data\n",
    "    optimizer_moco_multi_classif,                       # Adam optimizer\n",
    "    scheduler_moco_multi_classif,                       # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path66,                                       # Path to store logs      \n",
    "    \"M6_classification\",                                  # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M6_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path66, model_filename)\n",
    "torch.save(model_moco_multi_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_moco_multi, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_moco_multi = classif_run_testing(\n",
    "    model_moco_multi_classif,\n",
    "    criterion_moco_multi_classif,\n",
    "    classif_multi_finetune_test_dataloader,\n",
    "    storing_path66,\n",
    "    \"M6_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_moco_multi, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7. With models pretrained with <font color='yellow'>BYOL</font> method and employing the <font color='purple'>Ultrasound</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Initialize finetuning model with the associated ResNet of pretraining phase\n",
    "# Always working with a copy to ensure no inadverted changes happen to original pretrain models since they are to be used twice (once per task of which we have 2)\n",
    "finetune_backbone = deepcopy(model_byol_us_pretrain.online_encoder[0]) #the backbone for byol is accessed with .online_encoder[0]\n",
    "model_byol_us_classif = classif_model_finetune(finetune_backbone, num_classes)\n",
    "model_byol_us_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_byol_us_classif = nn.CrossEntropyLoss()\n",
    "criterion_byol_us_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_byol_us_classif = torch.optim.Adam(model_byol_us_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_byol_us_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_byol_us_classif, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_byol_us = classif_run_finetuning(\n",
    "    model_byol_us_classif,                             # model resulting from SimCLR pretraining         \n",
    "    criterion_byol_us_classif,                         # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_us_finetune_train_dataloader,              # Dataloader for training data\n",
    "    classif_us_finetune_val_dataloader,                # Dataloader for validation data\n",
    "    optimizer_byol_us_classif,                         # Adam optimizer\n",
    "    scheduler_byol_us_classif,                         # Learning rate scheduler\n",
    "    device,                                            # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                 # Number of epochs to finetune\n",
    "    storing_path67,                                    # Path to store logs      \n",
    "    \"M7_classification\",                               # Model identifier\n",
    "    checkpoint_interval                                # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M7_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path67, model_filename)\n",
    "torch.save(model_byol_us_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_byol_us, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_byol_us = classif_run_testing(\n",
    "    model_byol_us_classif,\n",
    "    criterion_byol_us_classif,\n",
    "    classif_us_finetune_test_dataloader,\n",
    "    storing_path67,\n",
    "    \"M7_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_byol_us, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8. With models pretrained with <font color='yellow'>BYOL</font> method and employing the <font color='pink'>Mammography</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Initialize finetuning model with the associated ResNet of pretraining phase\n",
    "# Always working with a copy to ensure no inadverted changes happen to original pretrain models since they are to be used twice (once per task of which we have 2)\n",
    "finetune_backbone = deepcopy(model_byol_mammo_pretrain.online_encoder[0]) \n",
    "model_byol_mammo_classif = classif_model_finetune(finetune_backbone, num_classes)\n",
    "model_byol_mammo_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_byol_mammo_classif = nn.CrossEntropyLoss()\n",
    "criterion_byol_mammo_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_byol_mammo_classif = torch.optim.Adam(model_byol_mammo_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_byol_mammo_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_byol_mammo_classif, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_byol_mammo = classif_run_finetuning(\n",
    "    model_byol_mammo_classif,                           # model resulting from SimCLR pretraining         \n",
    "    criterion_byol_mammo_classif,                       # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_mg_finetune_train_dataloader,               # Dataloader for training data\n",
    "    classif_mg_finetune_val_dataloader,                 # Dataloader for validation data\n",
    "    optimizer_byol_mammo_classif,                       # Adam optimizer\n",
    "    scheduler_byol_mammo_classif,                       # Learning rate scheduler\n",
    "    device,                                             # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                  # Number of epochs to finetune\n",
    "    storing_path68,                                     # Path to store logs      \n",
    "    \"M8_classification\",                                # Model identifier\n",
    "    checkpoint_interval                                 # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M8_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path68, model_filename)\n",
    "torch.save(model_byol_mammo_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_byol_mammo, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_byol_mammo = classif_run_testing(\n",
    "    model_byol_mammo_classif,\n",
    "    criterion_byol_mammo_classif,\n",
    "    classif_mg_finetune_test_dataloader,\n",
    "    storing_path68,\n",
    "    \"M8_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_byol_mammo, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9. With models pretrained with <font color='yellow'>BYOL</font> method and employing the <font color='orange'>Multimodal</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Initialize finetuning model with the associated ResNet of pretraining phase\n",
    "# Always working with a copy to ensure no inadverted changes happen to original pretrain models since they are to be used twice (once per task of which we have 2)\n",
    "finetune_backbone = deepcopy(model_byol_multi_pretrain.online_encoder[0]) \n",
    "model_byol_multi_classif = classif_model_finetune(finetune_backbone, num_classes)\n",
    "model_byol_multi_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_byol_multi_classif = nn.CrossEntropyLoss()\n",
    "criterion_byol_multi_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_byol_multi_classif = torch.optim.Adam(model_byol_multi_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_byol_multi_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_byol_multi_classif, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_byol_multi = classif_run_finetuning(\n",
    "    model_byol_multi_classif,                             # model resulting from SimCLR pretraining         \n",
    "    criterion_byol_multi_classif,                         # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_multi_finetune_train_dataloader,              # Dataloader for training data\n",
    "    classif_multi_finetune_val_dataloader,                # Dataloader for validation data\n",
    "    optimizer_byol_multi_classif,                         # Adam optimizer\n",
    "    scheduler_byol_multi_classif,                         # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path69,                                       # Path to store logs      \n",
    "    \"M9_classification\",                                  # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M9_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path69, model_filename)\n",
    "torch.save(model_byol_multi_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_byol_multi, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_byol_multi = classif_run_testing(\n",
    "    model_byol_multi_classif,\n",
    "    criterion_byol_multi_classif,\n",
    "    classif_multi_finetune_test_dataloader,\n",
    "    storing_path69,\n",
    "    \"M9_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_byol_multi, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.10. With models that aren't <font color='red'>Pretrained</font> and employing the <font color='purple'>Ultrasound</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "model_supervised_us_classif = classif_model_supervised(num_classes)  # No pretraining\n",
    "model_supervised_us_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_supervised_us_classif = nn.CrossEntropyLoss()\n",
    "criterion_supervised_us_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_supervised_us_classif = torch.optim.Adam(model_supervised_us_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_supervised_us_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_supervised_us_classif, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_supervised_us = classif_run_finetuning(\n",
    "    model_supervised_us_classif,                       # supervised counterpart model       \n",
    "    criterion_supervised_us_classif,                   # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_us_finetune_train_dataloader,              # Dataloader for training data\n",
    "    classif_us_finetune_val_dataloader,                # Dataloader for validation data\n",
    "    optimizer_supervised_us_classif,                   # Adam optimizer\n",
    "    scheduler_supervised_us_classif,                   # Learning rate scheduler\n",
    "    device,                                            # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                 # Number of epochs to finetune\n",
    "    storing_path610,                                   # Path to store logs      \n",
    "    \"M10_classification\",                              # Model identifier\n",
    "    checkpoint_interval                                # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M10_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path610, model_filename)\n",
    "torch.save(model_supervised_us_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_supervised_us, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_supervised_us = classif_run_testing(\n",
    "    model_supervised_us_classif,\n",
    "    criterion_supervised_us_classif,\n",
    "    classif_us_finetune_test_dataloader,\n",
    "    storing_path610,\n",
    "    \"M10_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_supervised_us, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.11. With models that aren't <font color='red'>Pretrained</font> and employing the <font color='pink'>Mammography</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "model_supervised_mammo_classif = classif_model_supervised(num_classes)  # No pretraining\n",
    "model_supervised_mammo_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_supervised_mammo_classif = nn.CrossEntropyLoss()\n",
    "criterion_supervised_mammo_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_supervised_mammo_classif = torch.optim.Adam(model_supervised_mammo_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_supervised_mammo_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_supervised_mammo_classif, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_supervised_mammo = classif_run_finetuning(\n",
    "    model_supervised_mammo_classif,                    # supervised counterpart model       \n",
    "    criterion_supervised_mammo_classif,                # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_mg_finetune_train_dataloader,              # Dataloader for training data\n",
    "    classif_mg_finetune_val_dataloader,                # Dataloader for validation data\n",
    "    optimizer_supervised_mammo_classif,                # Adam optimizer\n",
    "    scheduler_supervised_mammo_classif,                # Learning rate scheduler\n",
    "    device,                                            # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                 # Number of epochs to finetune\n",
    "    storing_path611,                                   # Path to store logs      \n",
    "    \"M11_classification\",                              # Model identifier\n",
    "    checkpoint_interval                                # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M11_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path611, model_filename)\n",
    "torch.save(model_supervised_mammo_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_supervised_mammo, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_supervised_mammo = classif_run_testing(\n",
    "    model_supervised_mammo_classif,\n",
    "    criterion_supervised_mammo_classif,\n",
    "    classif_mg_finetune_test_dataloader,\n",
    "    storing_path611,\n",
    "    \"M11_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_supervised_mammo, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.12. With models that aren't <font color='red'>Pretrained</font> and employing the <font color='orange'>Multimodal</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "model_supervised_multi_classif = classif_model_supervised(num_classes)  # No pretraining\n",
    "model_supervised_multi_classif.to(device)\n",
    "\n",
    "# Standard cross entropy loss function used in classification\n",
    "criterion_supervised_multi_classif = nn.CrossEntropyLoss()\n",
    "criterion_supervised_multi_classif.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_supervised_multi_classif = torch.optim.Adam(model_supervised_multi_classif.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_supervised_multi_classif = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_supervised_multi_classif, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "classif_ft_results_path_supervised_multi = classif_run_finetuning(\n",
    "    model_supervised_multi_classif,                    # supervised counterpart model       \n",
    "    criterion_supervised_multi_classif,                # CrossEntropyLoss (the standard choice for classification)\n",
    "    classif_multi_finetune_train_dataloader,           # Dataloader for training data\n",
    "    classif_multi_finetune_val_dataloader,             # Dataloader for validation data\n",
    "    optimizer_supervised_multi_classif,                # Adam optimizer\n",
    "    scheduler_supervised_multi_classif,                # Learning rate scheduler\n",
    "    device,                                            # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                 # Number of epochs to finetune\n",
    "    storing_path612,                                   # Path to store logs      \n",
    "    \"M12_classification\",                              # Model identifier\n",
    "    checkpoint_interval                                # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned classification model\n",
    "model_filename = \"M12_classification_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path612, model_filename)\n",
    "torch.save(model_supervised_multi_classif.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "classif_plot_finetuning_results(classif_ft_results_path_supervised_multi, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned classification model\n",
    "classif_test_results_path_supervised_multi = classif_run_testing(\n",
    "    model_supervised_multi_classif,\n",
    "    criterion_supervised_multi_classif,\n",
    "    classif_multi_finetune_test_dataloader,\n",
    "    storing_path612,\n",
    "    \"M12_classification\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned classification model\n",
    "classif_plot_testing_results(classif_test_results_path_supervised_multi, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Finetuning / Testing for Downstream Task - Binary Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0. Setting segmentation models architecture and storing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storing_path71 = \"../results/segmentation/simclr/ultrasound\"\n",
    "storing_path72 = \"../results/segmentation/simclr/mammography\"\n",
    "storing_path73 = \"../results/segmentation/simclr/multimodal\"\n",
    "storing_path74 = \"../results/segmentation/moco/ultrasound\"\n",
    "storing_path75 = \"../results/segmentation/moco/mammography\"\n",
    "storing_path76 = \"../results/segmentation/moco/multimodal\"\n",
    "storing_path77 = \"../results/segmentation/byol/ultrasound\"\n",
    "storing_path78 = \"../results/segmentation/byol/mammography\"\n",
    "storing_path79 = \"../results/segmentation/byol/multimodal\"\n",
    "storing_path710 = \"../results/segmentation/supervised/ultrasound\"\n",
    "storing_path711 = \"../results/segmentation/supervised/mammography\"\n",
    "storing_path712 = \"../results/segmentation/supervised/multimodal\"\n",
    "\n",
    "def extract_encoder_layers(pretrained_backbone):\n",
    "    # Initial layers: Conv2d, BatchNorm2d, ReLU, MaxPool2d\n",
    "    initial_layers = pretrained_backbone[:4]\n",
    "    \n",
    "    # Extract hierarchical layers\n",
    "    layer1 = pretrained_backbone[4]\n",
    "    layer2 = pretrained_backbone[5]\n",
    "    layer3 = pretrained_backbone[6]\n",
    "    layer4 = pretrained_backbone[7]\n",
    "\n",
    "    return initial_layers, layer1, layer2, layer3, layer4\n",
    "\n",
    "class segm_model_finetune(nn.Module):\n",
    "    def __init__(self, base_model_backbone, num_classes=1):\n",
    "        super(segm_model_finetune, self).__init__()\n",
    "\n",
    "        initial, l1, l2, l3, l4 = extract_encoder_layers(base_model_backbone)\n",
    "        self.encoder = nn.ModuleDict({\n",
    "            \"initial\": initial,\n",
    "            \"layer1\": l1,\n",
    "            \"layer2\": l2,\n",
    "            \"layer3\": l3,\n",
    "            \"layer4\": l4\n",
    "        })\n",
    "\n",
    "        # Simplified Decoder layers\n",
    "        self.decoder1 = self._conv_block(512 + 256, 256, upsample=True)  # Skip connection: 512 from encoder + 256 from previous decoder\n",
    "        self.decoder2 = self._conv_block(256 + 128, 128, upsample=True)  # Skip connection: 256 from encoder + 128 from previous decoder\n",
    "        self.decoder3 = self._conv_block(128 + 64, 64, upsample=True)    # Skip connection: 128 from encoder + 64 from previous decoder\n",
    "        self.decoder4 = self._conv_block(64 + 64, 32, upsample=True)     # Skip connection: 64 from encoder + 64 from previous decoder\n",
    "\n",
    "        # Segmentation head (1x1 convolution for pixel-wise classification)\n",
    "        self.segmentation_head = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels, upsample=False):\n",
    "        layers = []\n",
    "        if upsample:\n",
    "            layers.append(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False))\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) and m not in self.encoder.modules():\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder: Capture intermediate outputs\n",
    "        x = self.encoder[\"initial\"](x)\n",
    "        encoder1 = self.encoder[\"layer1\"](x)  # Early layer\n",
    "        encoder2 = self.encoder[\"layer2\"](encoder1)  # Downsampled\n",
    "        encoder3 = self.encoder[\"layer3\"](encoder2)  # Further downsampled\n",
    "        encoder4 = self.encoder[\"layer4\"](encoder3)  # Final encoder stage\n",
    "\n",
    "        # Resize encoder outputs to match decoder spatial dimensions\n",
    "        encoder3_resized = F.interpolate(encoder3, size=encoder4.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = self.decoder1(torch.cat([encoder4, encoder3_resized], dim=1))\n",
    "\n",
    "        encoder2_resized = F.interpolate(encoder2, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = self.decoder2(torch.cat([x, encoder2_resized], dim=1))\n",
    "\n",
    "        encoder1_resized = F.interpolate(encoder1, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = self.decoder3(torch.cat([x, encoder1_resized], dim=1))\n",
    "\n",
    "        # Resize encoder1 output again to match the current x\n",
    "        encoder1_resized = F.interpolate(encoder1, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = self.decoder4(torch.cat([x, encoder1_resized], dim=1))\n",
    "\n",
    "        # Segmentation head\n",
    "        output = self.segmentation_head(x)\n",
    "\n",
    "        # Resize output to match target mask size\n",
    "        output = F.interpolate(output, size=(64, 64), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return output\n",
    "\n",
    "class segm_model_supervised(nn.Module):\n",
    "    def __init__(self, base_model_backbone, num_classes=1):\n",
    "        super(segm_model_supervised, self).__init__()\n",
    "\n",
    "        # Using the pre-trained ResNet backbone for feature extraction\n",
    "        self.encoder = base_model_backbone  # ResNet backbone\n",
    "\n",
    "        # Simplified Decoder layers (using the same architecture as in your previous model)\n",
    "        self.decoder1 = self._conv_block(512 + 256, 256, upsample=True)  # Skip connection: 512 from encoder + 256 from previous decoder\n",
    "        self.decoder2 = self._conv_block(256 + 128, 128, upsample=True)  # Skip connection: 256 from encoder + 128 from previous decoder\n",
    "        self.decoder3 = self._conv_block(128 + 64, 64, upsample=True)    # Skip connection: 128 from encoder + 64 from previous decoder\n",
    "        self.decoder4 = self._conv_block(64 + 64, 32, upsample=True)     # Skip connection: 64 from encoder + 64 from previous decoder\n",
    "\n",
    "        # Segmentation head (1x1 convolution for pixel-wise classification)\n",
    "        self.segmentation_head = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels, upsample=False):\n",
    "        layers = []\n",
    "        if upsample:\n",
    "            layers.append(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False))\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) and m not in self.encoder.modules():\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder: Pass the input through the ResNet backbone\n",
    "        x = self.encoder.conv1(x)  # Conv1 layer\n",
    "        x = self.encoder.bn1(x)    # BatchNorm1 layer\n",
    "        x = self.encoder.relu(x)   # ReLU activation\n",
    "        x = self.encoder.maxpool(x)  # MaxPool layer\n",
    "\n",
    "        # Now pass the input through the ResNet layers\n",
    "        encoder1 = self.encoder.layer1(x)\n",
    "        encoder2 = self.encoder.layer2(encoder1)\n",
    "        encoder3 = self.encoder.layer3(encoder2)\n",
    "        encoder4 = self.encoder.layer4(encoder3)\n",
    "\n",
    "        # Resize encoder outputs to match decoder spatial dimensions\n",
    "        encoder3_resized = F.interpolate(encoder3, size=encoder4.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = self.decoder1(torch.cat([encoder4, encoder3_resized], dim=1))\n",
    "\n",
    "        encoder2_resized = F.interpolate(encoder2, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = self.decoder2(torch.cat([x, encoder2_resized], dim=1))\n",
    "\n",
    "        encoder1_resized = F.interpolate(encoder1, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = self.decoder3(torch.cat([x, encoder1_resized], dim=1))\n",
    "\n",
    "        # Resize encoder1 output again to match the current x\n",
    "        encoder1_resized = F.interpolate(encoder1, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = self.decoder4(torch.cat([x, encoder1_resized], dim=1))\n",
    "\n",
    "        # Segmentation head (final output map)\n",
    "        output = self.segmentation_head(x)\n",
    "\n",
    "        # Resize output to match target mask size\n",
    "        output = F.interpolate(output, size=(64, 64), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Loss function used for segmentation nets\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # BCE loss directly applied to raw logits\n",
    "        bce_loss = self.bce_loss(preds, targets)\n",
    "\n",
    "        # Compute Dice loss\n",
    "        # Apply sigmoid to logits for Dice loss calculation\n",
    "        preds = torch.sigmoid(preds)\n",
    "        intersection = torch.sum(preds * targets)\n",
    "        smooth = 1e-6\n",
    "        dice_loss = 1 - (2. * intersection + smooth) / (torch.sum(preds) + torch.sum(targets) + smooth)\n",
    "        \n",
    "        # Combined loss\n",
    "        return bce_loss + dice_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. With models pretrained with <font color='green'>SimCLR</font> method and employing the <font color='purple'>Ultrasound</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a 6 step process\n",
    "# 1. Prepare model\n",
    "# Always working with a copy ensuring no inadverted changes to original pretrained models\n",
    "finetune_backbone = deepcopy(model_simclr_us_pretrain.backbone) \n",
    "model_simclr_us_segm = segm_model_finetune(finetune_backbone, num_classes = 1)\n",
    "model_simclr_us_segm.to(device)\n",
    "\n",
    "# Combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_simclr_us_segm = DiceBCELoss()\n",
    "criterion_simclr_us_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_simclr_us_segm = torch.optim.Adam(model_simclr_us_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_simclr_us_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_simclr_us_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_simclr_us = segm_run_finetuning(\n",
    "    model_simclr_us_segm,                           # model resulting from pretraining         \n",
    "    criterion_simclr_us_segm,                       # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_us_finetune_train_dataloader,              # Dataloader for training data\n",
    "    segm_us_finetune_val_dataloader,                # Dataloader for validation data\n",
    "    optimizer_simclr_us_segm,                       # Adam optimizer\n",
    "    scheduler_simclr_us_segm,                       # Learning rate scheduler\n",
    "    device,                                         # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                              # Number of epochs to finetune\n",
    "    storing_path71,                                 # Path to store logs      \n",
    "    \"M1_segmentation\",                              # Model identifier\n",
    "    checkpoint_interval                             # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M1_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path71, model_filename)\n",
    "torch.save(model_simclr_us_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_simclr_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_simclr_us = segm_run_testing(\n",
    "    model_simclr_us_segm,\n",
    "    criterion_simclr_us_segm,\n",
    "    segm_us_finetune_test_dataloader,\n",
    "    storing_path71,\n",
    "    \"M1_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_simclr_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. With models pretrained with <font color='green'>SimCLR</font> method and employing the <font color='pink'>Mammography</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Always working with a copy ensuring no inadverted changes to original pretrained models\n",
    "finetune_backbone = deepcopy(model_simclr_mammo_pretrain.backbone) \n",
    "model_simclr_mammo_segm = segm_model_finetune(finetune_backbone, num_classes = 1)\n",
    "model_simclr_mammo_segm.to(device)\n",
    "\n",
    "# Combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_simclr_mammo_segm = DiceBCELoss()\n",
    "criterion_simclr_mammo_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_simclr_mammo_segm = torch.optim.Adam(model_simclr_mammo_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_simclr_mammo_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_simclr_mammo_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_simclr_mammo = segm_run_finetuning(\n",
    "    model_simclr_mammo_segm,                           # model resulting from pretraining         \n",
    "    criterion_simclr_mammo_segm,                       # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_mg_finetune_train_dataloader,                 # Dataloader for training data\n",
    "    segm_mg_finetune_val_dataloader,                   # Dataloader for validation data\n",
    "    optimizer_simclr_mammo_segm,                       # Adam optimizer\n",
    "    scheduler_simclr_mammo_segm,                       # Learning rate scheduler\n",
    "    device,                                            # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                 # Number of epochs to finetune\n",
    "    storing_path72,                                    # Path to store logs      \n",
    "    \"M2_segmentation\",                                 # Model identifier\n",
    "    checkpoint_interval                                # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M2_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path72, model_filename)\n",
    "torch.save(model_simclr_mammo_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_simclr_mammo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_simclr_mammo = segm_run_testing(\n",
    "    model_simclr_mammo_segm,\n",
    "    criterion_simclr_mammo_segm,\n",
    "    segm_mg_finetune_test_dataloader,\n",
    "    storing_path72,\n",
    "    \"M2_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_simclr_mammo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. With models pretrained with <font color='green'>SimCLR</font> method and employing the <font color='orange'>Multimodal</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Always working with a copy ensuring no inadverted changes to original pretrained models\n",
    "finetune_backbone = deepcopy(model_simclr_multi_pretrain.backbone) \n",
    "model_simclr_multi_segm = segm_model_finetune(finetune_backbone, num_classes = 1)\n",
    "model_simclr_multi_segm.to(device)\n",
    "\n",
    "# combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_simclr_multi_segm = DiceBCELoss()\n",
    "criterion_simclr_multi_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_simclr_multi_segm = torch.optim.Adam(model_simclr_multi_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_simclr_multi_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_simclr_multi_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_simclr_multi = segm_run_finetuning(\n",
    "    model_simclr_multi_segm,                              # model resulting from pretraining         \n",
    "    criterion_simclr_multi_segm,                          # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_multi_finetune_train_dataloader,                 # Dataloader for training data\n",
    "    segm_multi_finetune_val_dataloader,                   # Dataloader for validation data\n",
    "    optimizer_simclr_multi_segm,                          # Adam optimizer\n",
    "    scheduler_simclr_multi_segm,                          # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path73,                                       # Path to store logs      \n",
    "    \"M3_segmentation\",                                    # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M3_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path73, model_filename)\n",
    "torch.save(model_simclr_multi_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_simclr_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_simclr_multi = segm_run_testing(\n",
    "    model_simclr_multi_segm,\n",
    "    criterion_simclr_multi_segm,\n",
    "    segm_multi_finetune_test_dataloader,\n",
    "    storing_path73,\n",
    "    \"M3_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_simclr_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. With models pretrained with <font color='turquoise'>MoCo</font> method and employing the <font color='purple'>Ultrasound</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTAVAS AQUI A CORRER ESTA PARTE (7.4) CORRE ATE AO FIM E SE TUDO TIVER OK VOLTA A CONTINUAR A ESCREVER RELATORIO E ESPERA RESPOSTA DO PROF SOBRE METER ISTO A CORRER NUMA MAQUINA DA FCUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Always working with a copy ensuring no inadverted changes to original pretrained models\n",
    "finetune_backbone = deepcopy(model_moco_us_pretrain.backbone) \n",
    "model_moco_us_segm = segm_model_finetune(finetune_backbone, num_classes = 1)\n",
    "model_moco_us_segm.to(device)\n",
    "\n",
    "# Combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_moco_us_segm = DiceBCELoss()\n",
    "criterion_moco_us_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_moco_us_segm = torch.optim.Adam(model_moco_us_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_moco_us_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_moco_us_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_moco_us = segm_run_finetuning(\n",
    "    model_moco_us_segm,                                   # model resulting from pretraining         \n",
    "    criterion_moco_us_segm,                               # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_us_finetune_train_dataloader,                    # Dataloader for training data\n",
    "    segm_us_finetune_val_dataloader,                      # Dataloader for validation data\n",
    "    optimizer_moco_us_segm,                               # Adam optimizer\n",
    "    scheduler_moco_us_segm,                               # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path74,                                       # Path to store logs      \n",
    "    \"M4_segmentation\",                                    # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M4_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path74, model_filename)\n",
    "torch.save(model_moco_us_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_moco_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_moco_us = segm_run_testing(\n",
    "    model_moco_us_segm,\n",
    "    criterion_moco_us_segm,\n",
    "    segm_us_finetune_test_dataloader,\n",
    "    storing_path74,\n",
    "    \"M4_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_moco_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. With models pretrained with <font color='turquoise'>MoCo</font> method and employing the <font color='pink'>Mammography</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Always working with a copy ensuring no inadverted changes to original pretrained models\n",
    "finetune_backbone = deepcopy(model_moco_mammo_pretrain.backbone) \n",
    "model_moco_mammo_segm = segm_model_finetune(finetune_backbone, num_classes = 1)\n",
    "model_moco_mammo_segm.to(device)\n",
    "\n",
    "# Combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_moco_mammo_segm = DiceBCELoss()\n",
    "criterion_moco_mammo_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_moco_mammo_segm = torch.optim.Adam(model_moco_mammo_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_moco_mammo_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_moco_mammo_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_moco_mammo = segm_run_finetuning(\n",
    "    model_moco_mammo_segm,                                # model resulting from pretraining         \n",
    "    criterion_moco_mammo_segm,                            # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_mg_finetune_train_dataloader,                    # Dataloader for training data\n",
    "    segm_mg_finetune_val_dataloader,                      # Dataloader for validation data\n",
    "    optimizer_moco_mammo_segm,                            # Adam optimizer\n",
    "    scheduler_moco_mammo_segm,                            # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path75,                                       # Path to store logs      \n",
    "    \"M5_segmentation\",                                    # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M5_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path75, model_filename)\n",
    "torch.save(model_moco_mammo_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_moco_mammo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_moco_mammo = segm_run_testing(\n",
    "    model_moco_mammo_segm,\n",
    "    criterion_moco_mammo_segm,\n",
    "    segm_mg_finetune_test_dataloader,\n",
    "    storing_path75,\n",
    "    \"M5_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_moco_mammo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6. With models pretrained with <font color='turquoise'>MoCo</font> method and employing the <font color='orange'>Multimodal</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Always working with a copy ensuring no inadverted changes to original pretrained models\n",
    "finetune_backbone = deepcopy(model_moco_multi_pretrain.backbone) \n",
    "model_moco_multi_segm = segm_model_finetune(finetune_backbone, num_classes = 1)\n",
    "model_moco_multi_segm.to(device)\n",
    "\n",
    "# Combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_moco_multi_segm = DiceBCELoss()\n",
    "criterion_moco_multi_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_moco_multi_segm = torch.optim.Adam(model_moco_multi_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_moco_multi_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_moco_multi_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_moco_multi = segm_run_finetuning(\n",
    "    model_moco_multi_segm,                                # model resulting from pretraining         \n",
    "    criterion_moco_multi_segm,                            # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_multi_finetune_train_dataloader,                 # Dataloader for training data\n",
    "    segm_multi_finetune_val_dataloader,                   # Dataloader for validation data\n",
    "    optimizer_moco_multi_segm,                            # Adam optimizer\n",
    "    scheduler_moco_multi_segm,                            # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path76,                                       # Path to store logs      \n",
    "    \"M6_segmentation\",                                    # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M6_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path76, model_filename)\n",
    "torch.save(model_moco_multi_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_moco_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_moco_multi = segm_run_testing(\n",
    "    model_moco_multi_segm,\n",
    "    criterion_moco_multi_segm,\n",
    "    segm_multi_finetune_test_dataloader,\n",
    "    storing_path76,\n",
    "    \"M6_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_moco_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7. With models pretrained with <font color='yellow'>BYOL</font> method and employing the <font color='purple'>Ultrasound</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Always working with a copy ensuring no inadverted changes to original pretrained models\n",
    "finetune_backbone = deepcopy(model_byol_us_pretrain.online_encoder[0]) \n",
    "\n",
    "# The backbone for BYOL is accessed with .online_encoder[0]\n",
    "model_byol_us_segm = segm_model_finetune(finetune_backbone, num_classes = 1)\n",
    "model_byol_us_segm.to(device)\n",
    "\n",
    "# Combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_byol_us_segm = DiceBCELoss()\n",
    "criterion_byol_us_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_byol_us_segm = torch.optim.Adam(model_byol_us_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_byol_us_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_byol_us_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_byol_us = segm_run_finetuning(\n",
    "    model_byol_us_segm,                                   # model resulting from pretraining         \n",
    "    criterion_byol_us_segm,                               # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_us_finetune_train_dataloader,                    # Dataloader for training data\n",
    "    segm_us_finetune_val_dataloader,                      # Dataloader for validation data\n",
    "    optimizer_byol_us_segm,                               # Adam optimizer\n",
    "    scheduler_byol_us_segm,                               # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path77,                                       # Path to store logs      \n",
    "    \"M7_segmentation\",                                    # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M7_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path77, model_filename)\n",
    "torch.save(model_byol_us_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_byol_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_byol_us = segm_run_testing(\n",
    "    model_byol_us_segm,\n",
    "    criterion_byol_us_segm,\n",
    "    segm_us_finetune_test_dataloader,\n",
    "    storing_path77,\n",
    "    \"M7_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_byol_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8. With models pretrained with <font color='yellow'>BYOL</font> method and employing the <font color='pink'>Mammography</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Always working with a copy ensuring no inadverted changes to original pretrained models\n",
    "finetune_backbone = deepcopy(model_byol_mammo_pretrain.online_encoder[0])\n",
    "\n",
    "# The backbone for BYOL is accessed with .online_encoder[0]\n",
    "model_byol_mammo_segm = segm_model_finetune(finetune_backbone, num_classes = 1)\n",
    "model_byol_mammo_segm.to(device)\n",
    "\n",
    "# Combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_byol_mammo_segm = DiceBCELoss()\n",
    "criterion_byol_mammo_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_byol_mammo_segm = torch.optim.Adam(model_byol_mammo_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_byol_mammo_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_byol_mammo_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_byol_mammo = segm_run_finetuning(\n",
    "    model_byol_mammo_segm,                                # model resulting from pretraining         \n",
    "    criterion_byol_mammo_segm,                            # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_mg_finetune_train_dataloader,                    # Dataloader for training data\n",
    "    segm_mg_finetune_val_dataloader,                      # Dataloader for validation data\n",
    "    optimizer_byol_mammo_segm,                            # Adam optimizer\n",
    "    scheduler_byol_mammo_segm,                            # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path78,                                       # Path to store logs      \n",
    "    \"M8_segmentation\",                                    # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M8_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path78, model_filename)\n",
    "torch.save(model_byol_mammo_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_byol_mammo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_byol_mammo = segm_run_testing(\n",
    "    model_byol_mammo_segm,\n",
    "    criterion_byol_mammo_segm,\n",
    "    segm_mg_finetune_test_dataloader,\n",
    "    storing_path78,\n",
    "    \"M8_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_byol_mammo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.9. With models pretrained with <font color='yellow'>BYOL</font> method and employing the <font color='orange'>Multimodal</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model\n",
    "# Always working with a copy ensuring no inadverted changes to original pretrained models\n",
    "finetune_backbone = deepcopy(model_byol_multi_pretrain.online_encoder[0])\n",
    "\n",
    "# The backbone for BYOL is accessed with .online_encoder[0]\n",
    "model_byol_multi_segm = segm_model_finetune(finetune_backbone, num_classes = 1)\n",
    "model_byol_multi_segm.to(device)\n",
    "\n",
    "# Combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_byol_multi_segm = DiceBCELoss()\n",
    "criterion_byol_multi_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_byol_multi_segm = torch.optim.Adam(model_byol_multi_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_byol_multi_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_byol_multi_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_byol_multi = segm_run_finetuning(\n",
    "    model_byol_multi_segm,                                # model resulting from pretraining         \n",
    "    criterion_byol_multi_segm,                            # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_multi_finetune_train_dataloader,                 # Dataloader for training data\n",
    "    segm_multi_finetune_val_dataloader,                   # Dataloader for validation data\n",
    "    optimizer_byol_multi_segm,                            # Adam optimizer\n",
    "    scheduler_byol_multi_segm,                            # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path79,                                       # Path to store logs      \n",
    "    \"M9_segmentation\",                                    # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M9_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path79, model_filename)\n",
    "torch.save(model_byol_multi_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_byol_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_byol_multi = segm_run_testing(\n",
    "    model_byol_multi_segm,\n",
    "    criterion_byol_multi_segm,\n",
    "    segm_multi_finetune_test_dataloader,\n",
    "    storing_path79,\n",
    "    \"M9_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_byol_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.10. With models that aren't <font color='red'>Pretrained</font> and employing the <font color='purple'>Ultrasound</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model (Supervised - no pretraining)\n",
    "supervised_us_backbone = torchvision.models.resnet18(weights = None)\n",
    "\n",
    "# Change the first convolutional layer to accept 1 input channel\n",
    "supervised_us_backbone.conv1 = nn.Conv2d(1, 64, kernel_size = (7, 7), stride = (2, 2), padding = (3, 3), bias = False)\n",
    "\n",
    "model_supervised_us_segm = segm_model_supervised(base_model_backbone = supervised_us_backbone, num_classes = 1)\n",
    "model_supervised_us_segm.to(device)\n",
    "\n",
    "# Combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_supervised_us_segm = DiceBCELoss()\n",
    "criterion_supervised_us_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_supervised_us_segm = torch.optim.Adam(model_supervised_us_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_supervised_us_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_supervised_us_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_supervised_us = segm_run_finetuning(\n",
    "    model_supervised_us_segm,                             # model resulting from pretraining         \n",
    "    criterion_supervised_us_segm,                         # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_us_finetune_train_dataloader,                    # Dataloader for training data\n",
    "    segm_us_finetune_val_dataloader,                      # Dataloader for validation data\n",
    "    optimizer_supervised_us_segm,                         # Adam optimizer\n",
    "    scheduler_supervised_us_segm,                         # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path710,                                      # Path to store logs      \n",
    "    \"M10_segmentation\",                                   # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M10_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path710, model_filename)\n",
    "torch.save(model_supervised_us_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_supervised_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_supervised_us = segm_run_testing(\n",
    "    model_supervised_us_segm,\n",
    "    criterion_supervised_us_segm,\n",
    "    segm_us_finetune_test_dataloader,\n",
    "    storing_path710,\n",
    "    \"M10_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_supervised_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.11. With models that aren't <font color='red'>Pretrained</font> and employing the <font color='pink'>Mammography</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model (Supervised - no pretraining)\n",
    "supervised_mammo_backbone = torchvision.models.resnet18(weights = None)\n",
    "\n",
    "# Change the first convolutional layer to accept 1 input channel\n",
    "supervised_mammo_backbone.conv1 = nn.Conv2d(1, 64, kernel_size = (7, 7), stride = (2, 2), padding = (3, 3), bias = False)\n",
    "\n",
    "model_supervised_mammo_segm = segm_model_supervised(base_model_backbone = supervised_mammo_backbone, num_classes = 1)\n",
    "model_supervised_mammo_segm.to(device)\n",
    "\n",
    "# Combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_supervised_mammo_segm = DiceBCELoss()\n",
    "criterion_supervised_mammo_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_supervised_mammo_segm = torch.optim.Adam(model_supervised_mammo_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_supervised_mammo_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_supervised_mammo_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_supervised_mammo = segm_run_finetuning(\n",
    "    model_supervised_mammo_segm,                          # model resulting from pretraining         \n",
    "    criterion_supervised_mammo_segm,                      # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_mg_finetune_train_dataloader,                    # Dataloader for training data\n",
    "    segm_mg_finetune_val_dataloader,                      # Dataloader for validation data\n",
    "    optimizer_supervised_mammo_segm,                      # Adam optimizer\n",
    "    scheduler_supervised_mammo_segm,                      # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path711,                                      # Path to store logs      \n",
    "    \"M11_segmentation\",                                   # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M11_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path711, model_filename)\n",
    "torch.save(model_supervised_mammo_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_supervised_mammo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_supervised_mammo = segm_run_testing(\n",
    "    model_supervised_mammo_segm,\n",
    "    criterion_supervised_mammo_segm,\n",
    "    segm_mg_finetune_test_dataloader,\n",
    "    storing_path711,\n",
    "    \"M11_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_supervised_mammo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.12. With models that aren't <font color='red'>Pretrained</font> and employing the <font color='orange'>Multimodal</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare model (Supervised - no pretraining)\n",
    "supervised_multi_backbone = torchvision.models.resnet18(weights = None)\n",
    "\n",
    "# Change the first convolutional layer to accept 1 input channel\n",
    "supervised_multi_backbone.conv1 = nn.Conv2d(1, 64, kernel_size = (7, 7), stride = (2, 2), padding = (3, 3), bias = False)\n",
    "\n",
    "model_supervised_multi_segm = segm_model_supervised(base_model_backbone = supervised_multi_backbone, num_classes = 1)\n",
    "model_supervised_multi_segm.to(device)\n",
    "\n",
    "# Combined dice and binary cross entropy loss function used for segmentation here\n",
    "criterion_supervised_multi_segm = DiceBCELoss()\n",
    "criterion_supervised_multi_segm.to(device)\n",
    "\n",
    "# Adam optimizer same as pretrain cases but with finetune learning rate\n",
    "optimizer_supervised_multi_segm = torch.optim.Adam(model_supervised_multi_segm.parameters(), lr = lr_finetune, weight_decay = weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau scheduler (good for classification and segmentation)\n",
    "scheduler_supervised_multi_segm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_supervised_multi_segm, mode = 'min', patience = 3, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finetune loop\n",
    "segm_ft_results_path_supervised_multi = segm_run_finetuning(\n",
    "    model_supervised_multi_segm,                          # model resulting from pretraining         \n",
    "    criterion_supervised_multi_segm,                      # DiceBCELoss (chosen loss function for segmentation)\n",
    "    segm_multi_finetune_train_dataloader,                 # Dataloader for training data\n",
    "    segm_multi_finetune_val_dataloader,                   # Dataloader for validation data\n",
    "    optimizer_supervised_multi_segm,                      # Adam optimizer\n",
    "    scheduler_supervised_multi_segm,                      # Learning rate scheduler\n",
    "    device,                                               # Device to run training on (GPU/CPU)\n",
    "    total_epoch_count,                                    # Number of epochs to finetune\n",
    "    storing_path712,                                      # Path to store logs      \n",
    "    \"M12_segmentation\",                                   # Model identifier\n",
    "    checkpoint_interval                                   # Save checkpoints every N epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the finetuned segmentation model\n",
    "model_filename = \"M12_segmentation_savedModel.pth\"\n",
    "model_path = os.path.join(storing_path712, model_filename)\n",
    "torch.save(model_supervised_multi_segm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving plots based on results of finetune loop\n",
    "segm_plot_finetuning_results(segm_ft_results_path_supervised_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final test run of finetuned segmentation model\n",
    "segm_test_results_path_supervised_multi = segm_run_testing(\n",
    "    model_supervised_multi_segm,\n",
    "    criterion_supervised_multi_segm,\n",
    "    segm_multi_finetune_test_dataloader,\n",
    "    storing_path712,\n",
    "    \"M12_segmentation\",\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Saving plots based on results of final test of finetuned segmentation model\n",
    "segm_plot_testing_results(segm_test_results_path_supervised_multi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
